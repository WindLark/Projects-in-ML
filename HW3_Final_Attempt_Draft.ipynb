{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "collapsed_sections": [
        "Fm04hi-8pqE7",
        "ghEEe2IGijzj",
        "P7_WMmLuQqzQ",
        "2G5quPs0Q1BI",
        "nLOnFGLMQ2BZ",
        "hZ1y0vqxSV6T",
        "D23j0ukJkply",
        "8XA9FEMRTsbe",
        "Tepo7Jtsrfyd"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "[Colab notebook link](https://colab.research.google.com/drive/1Ld3bW0ASDQEegpxRwoFmGiJziGcHS82U?usp=sharing)"
      ],
      "metadata": {
        "id": "5m_kZdUr5CEm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Exploration of Pytorch and Resources Used (Task 1)\n",
        "\n",
        "My objective for this homework was to learn the Pytorch framework for a machine learning classification problem. Pytorch is an open source machine learning library used by companies such as Amazon and Salesforce for a variety of ML problems such as NLP. I reviewed a variety of datasets and problems before deciding that I wanted to do a multi-classification problem with a dataset with a noticeable number of features. Ultimately, I selected a dataset with 52913 instances, 7 features, and 5 possible labels. This dataset will be described in more depth in “Base Requirements”. \n",
        "\n",
        "##Learning Pytorch\n",
        "https://pytorch.org/tutorials/beginner/basics/intro.html\n",
        "\n",
        "https://pytorch.org/tutorials/recipes/recipes/defining_a_neural_network.html\n",
        "\n",
        "https://pytorch.org/tutorials/beginner/introyt/trainingyt.html\n",
        "\n",
        "Because this was my first time creating a solo project with Pytorch, I relied on the tutorials above as part of my research on how to implement a neural network. My goal was to understand nn.module, which Pytorch uses for the majority of its functions related to NN. Appropriate attribution to a specific tutorial is provided as needed in the code.\n",
        "\n",
        "\n",
        "##Learning Pytorch Dataloader\n",
        "\n",
        "https://pytorch.org/docs/stable/data.html\n",
        "\n",
        "https://pythonguides.com/pytorch-dataloader/\n",
        "\n",
        "https://androidkt.com/load-pandas-dataframe-using-dataset-and-dataloader-in-pytorch/\n",
        "\n",
        "I initially had a lot of difficulty understanding the dataset functions of Pytorch due to picking large datasets that were difficult to process in a reasonable time with my initial level of knowledge. Eventually, I selected a dataset that I could preprocess via pandas and convert to a Pytorch dataset and pass to a DataLoader. This provided to be a much more efficient method that took seconds to set up each time and let me better understand how Dataset/Dataloader worked. A Dataset convert desired objects storing data (e.g. pandas dataframe, numpy array) it to a set of Pytorch tensors (usually an x and y) with additional functions defined to let it provide details such as Dataset size or being able to return desired elements of the dataset. This Dataset can be then taken by a Dataloader and used to provide arbitrary amounts of data (e.g. for batches, or providing the entire dataset to calculate length).\n",
        "\n",
        "\n",
        "##Research and Review of Activation Functions, Backpropagation, Loss Functions \n",
        "\n",
        "https://pytorch.org/docs/stable/nn.html#non-linear-activations-weighted-sum-nonlinearity\n",
        "\n",
        "https://stats.stackexchange.com/questions/362461/is-it-better-to-avoid-relu-as-activation-function-if-input-data-has-plenty-of-ne\n",
        "\n",
        "https://www.quora.com/What-are-the-advantages-of-using-Leaky-Rectified-Linear-Units-Leaky-ReLU-over-normal-ReLU-in-deep-learning\n",
        "\n",
        "https://ml-cheatsheet.readthedocs.io/en/latest/activation_functions.html\n",
        "\n",
        "The above resources were used to investigate and review the concepts of ReLU activation function and related activation functions like Leaky ReLU and ELU that we would go on to try as part of Task 2 and 3 with modified hyperparameters. ReLU (Rectified Linear Unit) takes gradient values and calculates max(0,z), effectively returning 0 for non positive values and 1 for positive values. Normally, this solves the vanishing gradient problem, but for large amounts of negative values it is possible the problem will persist via “dead” ReLU neurons in the network. Leaky ReLU allows for fractional small negative inputs and ELU and a smoothing function alpha to allow for negative inputs.\n",
        "\n",
        "https://machinelearningmastery.com/how-to-choose-loss-functions-when-training-deep-learning-neural-networks/\n",
        "\n",
        "This article was used to determine the most appropriate loss function for our problem. We chose Cross Entropy Loss as it is well suited to handling mult-classification problems where each class can be denoted by an integer label.\n",
        "\n",
        "https://medium.com/@mugeshk/backpropagation-algorithm-using-pytorch-ee1287888aca\n",
        "\n",
        "An additional resource used to study how backpropagation were specifically implemented in Pytorch.\n",
        "\n"
      ],
      "metadata": {
        "id": "_vLoIGO44sg_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Data Set and Base Requirements\n",
        "\n",
        "Dataset used https://archive.ics.uci.edu/ml/datasets/Basketball+dataset\n",
        "\n",
        "This time we will use accelerometer data to classify whether a basketball athlete is performing one of five actions in a basketball game: dribble, hold, pass, pickup, or shoot. We expect these classifications will be useful for sports analysts and coaches who are likely interested in whether their athletes are moving the correct way to perform the acting they intend (i.e. am athlete who intends to shoot should not be trying to pass!)\n",
        "\n",
        "The dataset consists of actions with the following 7 features:\n",
        "\n",
        "\n",
        "*   1:   Time - The time since the action has started, in seconds.\n",
        "\n",
        "*   2,3,4:   X,Y,Z - Acceleration of the athlete performing the action, as measured by an accelerometer in m/s^2.\n",
        "\n",
        "*   5:  R - Acceleration of the athlete as measured by a gyroscope in m/s^2.\n",
        "\n",
        "*   6,7: Theta, Phi - Angle of acceleration as measured by a gyroscope in  degrees.\n",
        "\n",
        "Additionally, \"dribble, hold, pass, pickup, or shoot\" will be assigned as the labels to this dataset, using numbers 0-4.\n",
        "\n",
        "The data was split among text files and organized by athlete initial and action taken. This setup was leveraged to quickly create a pandas dataframe that properly represented our data as seen in \"Loading Data and Peforming Exploratory Data Analysis\".\n",
        "\n"
      ],
      "metadata": {
        "id": "Fm04hi-8pqE7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install torch torchvision torchaudio torchdata"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5fkfOKylaIRE",
        "outputId": "0147f1a5-0e04-46fd-edc2-9ef376484687"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (1.12.1+cu113)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (0.13.1+cu113)\n",
            "Requirement already satisfied: torchaudio in /usr/local/lib/python3.7/dist-packages (0.12.1+cu113)\n",
            "Collecting torchdata\n",
            "  Downloading torchdata-0.4.1-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.4 MB 10.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch) (4.1.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchvision) (1.21.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from torchvision) (2.23.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision) (7.1.2)\n",
            "Collecting portalocker>=2.0.0\n",
            "  Downloading portalocker-2.5.1-py2.py3-none-any.whl (15 kB)\n",
            "Collecting urllib3>=1.25\n",
            "  Downloading urllib3-1.26.12-py2.py3-none-any.whl (140 kB)\n",
            "\u001b[K     |████████████████████████████████| 140 kB 4.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision) (2022.9.24)\n",
            "  Downloading urllib3-1.25.11-py2.py3-none-any.whl (127 kB)\n",
            "\u001b[K     |████████████████████████████████| 127 kB 62.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision) (3.0.4)\n",
            "Installing collected packages: urllib3, portalocker, torchdata\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 1.24.3\n",
            "    Uninstalling urllib3-1.24.3:\n",
            "      Successfully uninstalled urllib3-1.24.3\n",
            "Successfully installed portalocker-2.5.1 torchdata-0.4.1 urllib3-1.25.11\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YMWVA4RuPIN5"
      },
      "outputs": [],
      "source": [
        "#import requirements\n",
        "import numpy as np\n",
        "import operator\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#pytorch specific utils\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader , random_split, Dataset\n",
        "from torchtext import datasets\n",
        "from torchtext.transforms import ToTensor\n",
        "from torchtext.data import get_tokenizer"
      ],
      "metadata": {
        "id": "wjI9DBXAY8hR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "RANDOM_STATE=233"
      ],
      "metadata": {
        "id": "CQKLdMHVLbKx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Loading Data and Performing Exploratory Data Analysis (Task 2)\n",
        "\n",
        "1. Exploratory Data Analysis (Can include data cleaning, visualization etc.)\n",
        "\n",
        "Additional comments on dataset, cleaning, and visualization have been provided where needed."
      ],
      "metadata": {
        "id": "ghEEe2IGijzj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#quick mv command for misplaced files\n",
        "#mv  *.txt Athlete_D"
      ],
      "metadata": {
        "id": "RZhwOYyGyjEO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from os import listdir\n",
        "from os.path import isfile, join"
      ],
      "metadata": {
        "id": "SZwTswaYzdS5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_dir = \"Athlete_D\"\n",
        "ath_D_array = [f for f in listdir(test_dir) if isfile(join(test_dir, f))]\n",
        "print(ath_D_array)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aHBaJUyGzhcx",
        "outputId": "870cdb48-d62b-4500-cd6b-6815945825cd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['D_pass2.txt', 'D_pickup5.txt', 'D_pickup3.txt', 'D_pass5.txt', 'D_shoot2.txt', 'D_hold3.txt', 'D_dribble3.txt', 'D_hold2.txt', 'D_pickup2.txt', 'D_pickup1.txt', 'D_pickup4.txt', 'D_shoot3.txt', 'D_pass1.txt', 'D_dribble2.txt', 'D_pass3.txt', 'D_pass4.txt', 'D_shoot4.txt', 'D_pass6.txt', 'D_dribble1.txt', 'D_shoot1.txt', 'D_shoot5.txt', 'D_hold1.txt']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Athletes were categorized by initial, so we sort them accordingly \n",
        "#into folders before starting processing\n",
        "#File names are marked as INITIAL_ACTION(NUMBER).\n",
        "#We will use the ACTION \n",
        "athlete_folders = [\"Athlete_D\",\"Athlete_Jc\", \"Athlete_L\", \"Athlete_X\"]\n",
        "actions = [\"dribble\",\"hold\",\"pass\",\"pickup\",\"shoot\"] #label will correspond to index number"
      ],
      "metadata": {
        "id": "dso52Ui3z4jM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#test with reading one file\n",
        "newcol= \"Action\"\n",
        "df = pd.read_table(\"Athlete_D/D_dribble1.txt\", delimiter=\",\", skiprows=3)\n",
        "df[newcol]=0"
      ],
      "metadata": {
        "id": "kg8U4NKIi6gk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "Q9rK-SGvsDt5",
        "outputId": "169f15d5-ac36-4a20-91a8-3f40ec8ff65f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "     Time (s)   X (m/s2)   Y (m/s2)   Z (m/s2)   R (m/s2)   Theta (deg)  \\\n",
              "0    0.000000   3.428497   9.112331  -2.047042   9.948847    101.873802   \n",
              "1    0.009703   3.447650   9.285912  -2.331953  10.176072    103.247643   \n",
              "2    0.019734   3.506308   9.212888  -2.244564  10.109875    102.827530   \n",
              "3    0.029704   3.703830   8.972271  -1.990779   9.908744    101.590256   \n",
              "4    0.039805   3.902549   8.799889  -1.545457   9.749685     99.120628   \n",
              "..        ...        ...        ...        ...        ...           ...   \n",
              "984  9.879907   3.989937   8.725668  -0.760159   9.624694     94.529945   \n",
              "985  9.889970   4.096479   8.585608  -0.766145   9.543624     94.604553   \n",
              "986  9.900604   4.034230   8.754398  -0.822408   9.674236     94.876610   \n",
              "987  9.910431   3.951630   9.159019  -1.513135  10.089231     98.625496   \n",
              "988  9.919714   3.681085   9.283517  -2.084153  10.201851    101.788033   \n",
              "\n",
              "      Phi (deg)  Action  \n",
              "0     69.381287       0  \n",
              "1     69.631172       0  \n",
              "2     69.163811       0  \n",
              "3     67.568748       0  \n",
              "4     66.083809       0  \n",
              "..          ...     ...  \n",
              "984   65.427071       0  \n",
              "985   64.492638       0  \n",
              "986   65.258652       0  \n",
              "987   66.662369       0  \n",
              "988   68.370766       0  \n",
              "\n",
              "[989 rows x 8 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-86a731f3-6b16-47f0-8ef8-7b83c3ba2378\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Time (s)</th>\n",
              "      <th>X (m/s2)</th>\n",
              "      <th>Y (m/s2)</th>\n",
              "      <th>Z (m/s2)</th>\n",
              "      <th>R (m/s2)</th>\n",
              "      <th>Theta (deg)</th>\n",
              "      <th>Phi (deg)</th>\n",
              "      <th>Action</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>3.428497</td>\n",
              "      <td>9.112331</td>\n",
              "      <td>-2.047042</td>\n",
              "      <td>9.948847</td>\n",
              "      <td>101.873802</td>\n",
              "      <td>69.381287</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.009703</td>\n",
              "      <td>3.447650</td>\n",
              "      <td>9.285912</td>\n",
              "      <td>-2.331953</td>\n",
              "      <td>10.176072</td>\n",
              "      <td>103.247643</td>\n",
              "      <td>69.631172</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.019734</td>\n",
              "      <td>3.506308</td>\n",
              "      <td>9.212888</td>\n",
              "      <td>-2.244564</td>\n",
              "      <td>10.109875</td>\n",
              "      <td>102.827530</td>\n",
              "      <td>69.163811</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.029704</td>\n",
              "      <td>3.703830</td>\n",
              "      <td>8.972271</td>\n",
              "      <td>-1.990779</td>\n",
              "      <td>9.908744</td>\n",
              "      <td>101.590256</td>\n",
              "      <td>67.568748</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.039805</td>\n",
              "      <td>3.902549</td>\n",
              "      <td>8.799889</td>\n",
              "      <td>-1.545457</td>\n",
              "      <td>9.749685</td>\n",
              "      <td>99.120628</td>\n",
              "      <td>66.083809</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>984</th>\n",
              "      <td>9.879907</td>\n",
              "      <td>3.989937</td>\n",
              "      <td>8.725668</td>\n",
              "      <td>-0.760159</td>\n",
              "      <td>9.624694</td>\n",
              "      <td>94.529945</td>\n",
              "      <td>65.427071</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>985</th>\n",
              "      <td>9.889970</td>\n",
              "      <td>4.096479</td>\n",
              "      <td>8.585608</td>\n",
              "      <td>-0.766145</td>\n",
              "      <td>9.543624</td>\n",
              "      <td>94.604553</td>\n",
              "      <td>64.492638</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>986</th>\n",
              "      <td>9.900604</td>\n",
              "      <td>4.034230</td>\n",
              "      <td>8.754398</td>\n",
              "      <td>-0.822408</td>\n",
              "      <td>9.674236</td>\n",
              "      <td>94.876610</td>\n",
              "      <td>65.258652</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>987</th>\n",
              "      <td>9.910431</td>\n",
              "      <td>3.951630</td>\n",
              "      <td>9.159019</td>\n",
              "      <td>-1.513135</td>\n",
              "      <td>10.089231</td>\n",
              "      <td>98.625496</td>\n",
              "      <td>66.662369</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>988</th>\n",
              "      <td>9.919714</td>\n",
              "      <td>3.681085</td>\n",
              "      <td>9.283517</td>\n",
              "      <td>-2.084153</td>\n",
              "      <td>10.201851</td>\n",
              "      <td>101.788033</td>\n",
              "      <td>68.370766</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>989 rows × 8 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-86a731f3-6b16-47f0-8ef8-7b83c3ba2378')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-86a731f3-6b16-47f0-8ef8-7b83c3ba2378 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-86a731f3-6b16-47f0-8ef8-7b83c3ba2378');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dfcols = [\"Time (s)\", \"X (m/s2)\", \"Y (m/s2)\", \"Z (m/s2)\", \"R (m/s2)\", \"Theta (deg)\", \"Phi (deg)\", \"Action\"]\n",
        "all_df = pd.DataFrame(columns = dfcols)"
      ],
      "metadata": {
        "id": "K_bnfhuJ7MOs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49
        },
        "id": "hMTx82qV7QQo",
        "outputId": "bf6a6d65-3fb8-4da1-c3df-937d20eced25"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Empty DataFrame\n",
              "Columns: [Time (s), X (m/s2), Y (m/s2), Z (m/s2), R (m/s2), Theta (deg), Phi (deg), Action]\n",
              "Index: []"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-cd558ecd-d659-4447-ab62-4693eb690a9e\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Time (s)</th>\n",
              "      <th>X (m/s2)</th>\n",
              "      <th>Y (m/s2)</th>\n",
              "      <th>Z (m/s2)</th>\n",
              "      <th>R (m/s2)</th>\n",
              "      <th>Theta (deg)</th>\n",
              "      <th>Phi (deg)</th>\n",
              "      <th>Action</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-cd558ecd-d659-4447-ab62-4693eb690a9e')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-cd558ecd-d659-4447-ab62-4693eb690a9e button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-cd558ecd-d659-4447-ab62-4693eb690a9e');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "myframes = []\n",
        "for foldername in athlete_folders:\n",
        "  textfiles = [f for f in listdir(foldername) if isfile(join(foldername, f))]\n",
        "  for textfile in textfiles:\n",
        "    fullpath = foldername + \"/\" + textfile\n",
        "    df_temp = pd.read_table(fullpath, delimiter=\",\", skiprows=3)\n",
        "    if \"dribble\" in textfile:\n",
        "        df_temp[\"Action\"] = 0\n",
        "    elif \"hold\" in textfile:\n",
        "        df_temp[\"Action\"] = 1\n",
        "    elif \"pass\" in textfile:\n",
        "        df_temp[\"Action\"] = 2\n",
        "    elif \"pickup\" in textfile:\n",
        "        df_temp[\"Action\"] = 3\n",
        "    elif \"shoot\" in textfile:\n",
        "        df_temp[\"Action\"] = 4\n",
        "    else:\n",
        "        df_temp[\"Action\"] = 5 #this label represents INVALID data.\n",
        "    myframes.append(df_temp)"
      ],
      "metadata": {
        "id": "2ENfG2b_4BO2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(myframes)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wxrsckqi7y74",
        "outputId": "7e3c2f98-cd8a-4ec1-ef10-202f34493ff3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "79"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "all_df = pd.concat(myframes)"
      ],
      "metadata": {
        "id": "28YwTEJq7Je0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#52913 rows × 8 columns\n",
        "all_df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "P1F_h7Iu9jF6",
        "outputId": "1b6e4e84-4f16-49c5-b2a3-447ab64aba49"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "     Time (s)   X (m/s2)   Y (m/s2)   Z (m/s2)   R (m/s2)   Theta (deg)  \\\n",
              "0    0.000000   4.630386   8.900445  -3.051410  10.486634    106.916687   \n",
              "1    0.010067   4.814740   8.911219  -2.783259  10.504193    105.364960   \n",
              "2    0.020014   4.978743   8.688558  -2.178724  10.248207    102.274506   \n",
              "3    0.030174   5.116409   8.512584  -1.629254  10.064601     99.316017   \n",
              "4    0.040226   5.410896   8.554482  -1.308431  10.206319     97.365486   \n",
              "..        ...        ...        ...        ...        ...           ...   \n",
              "569  5.689598   0.373774   0.997706   0.107629   1.070845     84.231529   \n",
              "570  5.699913   0.378777   0.987700   0.113731   1.063935     83.863571   \n",
              "571  5.709731   0.380851   0.961219   0.117758   1.040604     83.502335   \n",
              "572  5.719699   0.373408   0.975009   0.070533   1.046446     86.135223   \n",
              "573  5.730040   0.362547   1.009421   0.034412   1.073105     88.162338   \n",
              "\n",
              "      Phi (deg)  Action  \n",
              "0     62.514652       2  \n",
              "1     61.617607       2  \n",
              "2     60.186302       2  \n",
              "3     58.992424       2  \n",
              "4     57.685757       2  \n",
              "..          ...     ...  \n",
              "569   69.462387       3  \n",
              "570   69.018478       3  \n",
              "571   68.385696       3  \n",
              "572   69.044220       3  \n",
              "573   70.243568       3  \n",
              "\n",
              "[52913 rows x 8 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-86af1d0a-6e0f-4bed-9da3-11034ccb67e0\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Time (s)</th>\n",
              "      <th>X (m/s2)</th>\n",
              "      <th>Y (m/s2)</th>\n",
              "      <th>Z (m/s2)</th>\n",
              "      <th>R (m/s2)</th>\n",
              "      <th>Theta (deg)</th>\n",
              "      <th>Phi (deg)</th>\n",
              "      <th>Action</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>4.630386</td>\n",
              "      <td>8.900445</td>\n",
              "      <td>-3.051410</td>\n",
              "      <td>10.486634</td>\n",
              "      <td>106.916687</td>\n",
              "      <td>62.514652</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.010067</td>\n",
              "      <td>4.814740</td>\n",
              "      <td>8.911219</td>\n",
              "      <td>-2.783259</td>\n",
              "      <td>10.504193</td>\n",
              "      <td>105.364960</td>\n",
              "      <td>61.617607</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.020014</td>\n",
              "      <td>4.978743</td>\n",
              "      <td>8.688558</td>\n",
              "      <td>-2.178724</td>\n",
              "      <td>10.248207</td>\n",
              "      <td>102.274506</td>\n",
              "      <td>60.186302</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.030174</td>\n",
              "      <td>5.116409</td>\n",
              "      <td>8.512584</td>\n",
              "      <td>-1.629254</td>\n",
              "      <td>10.064601</td>\n",
              "      <td>99.316017</td>\n",
              "      <td>58.992424</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.040226</td>\n",
              "      <td>5.410896</td>\n",
              "      <td>8.554482</td>\n",
              "      <td>-1.308431</td>\n",
              "      <td>10.206319</td>\n",
              "      <td>97.365486</td>\n",
              "      <td>57.685757</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>569</th>\n",
              "      <td>5.689598</td>\n",
              "      <td>0.373774</td>\n",
              "      <td>0.997706</td>\n",
              "      <td>0.107629</td>\n",
              "      <td>1.070845</td>\n",
              "      <td>84.231529</td>\n",
              "      <td>69.462387</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>570</th>\n",
              "      <td>5.699913</td>\n",
              "      <td>0.378777</td>\n",
              "      <td>0.987700</td>\n",
              "      <td>0.113731</td>\n",
              "      <td>1.063935</td>\n",
              "      <td>83.863571</td>\n",
              "      <td>69.018478</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>571</th>\n",
              "      <td>5.709731</td>\n",
              "      <td>0.380851</td>\n",
              "      <td>0.961219</td>\n",
              "      <td>0.117758</td>\n",
              "      <td>1.040604</td>\n",
              "      <td>83.502335</td>\n",
              "      <td>68.385696</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>572</th>\n",
              "      <td>5.719699</td>\n",
              "      <td>0.373408</td>\n",
              "      <td>0.975009</td>\n",
              "      <td>0.070533</td>\n",
              "      <td>1.046446</td>\n",
              "      <td>86.135223</td>\n",
              "      <td>69.044220</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>573</th>\n",
              "      <td>5.730040</td>\n",
              "      <td>0.362547</td>\n",
              "      <td>1.009421</td>\n",
              "      <td>0.034412</td>\n",
              "      <td>1.073105</td>\n",
              "      <td>88.162338</td>\n",
              "      <td>70.243568</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>52913 rows × 8 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-86af1d0a-6e0f-4bed-9da3-11034ccb67e0')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-86af1d0a-6e0f-4bed-9da3-11034ccb67e0 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-86af1d0a-6e0f-4bed-9da3-11034ccb67e0');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "invalid_data = df.loc[df['Action'] == 5]"
      ],
      "metadata": {
        "id": "3bpiZaKh9r05"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#all of our data is valid actions!\n",
        "#we have 52913 examples to work with\n",
        "invalid_data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49
        },
        "id": "v3zC47C197kr",
        "outputId": "d7e9eeeb-2eb7-464e-8622-262e362fce8b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Empty DataFrame\n",
              "Columns: [Time (s),  X (m/s2),  Y (m/s2),  Z (m/s2),  R (m/s2),  Theta (deg),  Phi (deg), Action]\n",
              "Index: []"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-9964a335-012e-42e3-b53b-5b9f886ad6b2\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Time (s)</th>\n",
              "      <th>X (m/s2)</th>\n",
              "      <th>Y (m/s2)</th>\n",
              "      <th>Z (m/s2)</th>\n",
              "      <th>R (m/s2)</th>\n",
              "      <th>Theta (deg)</th>\n",
              "      <th>Phi (deg)</th>\n",
              "      <th>Action</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-9964a335-012e-42e3-b53b-5b9f886ad6b2')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-9964a335-012e-42e3-b53b-5b9f886ad6b2 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-9964a335-012e-42e3-b53b-5b9f886ad6b2');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test= all_df.loc[all_df['Action'] == 3]"
      ],
      "metadata": {
        "id": "3ZZBbNVM_z7u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "CZXAx0_UAIp0",
        "outputId": "ea49535e-64d9-481e-a9f9-3579ee4a27d2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "     Time (s)   X (m/s2)   Y (m/s2)   Z (m/s2)   R (m/s2)   Theta (deg)  \\\n",
              "0    0.000000   1.897405   6.508637   7.580042  10.169540     41.809349   \n",
              "1    0.009813   1.783680   6.707356   7.337031  10.099612     43.409012   \n",
              "2    0.019684   1.868674   7.147889   7.380127  10.442726     45.030994   \n",
              "3    0.029636   1.713051   6.873753   7.319075  10.185866     44.064941   \n",
              "4    0.039645   1.448492   6.623559   7.020997   9.760330     43.999977   \n",
              "..        ...        ...        ...        ...        ...           ...   \n",
              "569  5.689598   0.373774   0.997706   0.107629   1.070845     84.231529   \n",
              "570  5.699913   0.378777   0.987700   0.113731   1.063935     83.863571   \n",
              "571  5.709731   0.380851   0.961219   0.117758   1.040604     83.502335   \n",
              "572  5.719699   0.373408   0.975009   0.070533   1.046446     86.135223   \n",
              "573  5.730040   0.362547   1.009421   0.034412   1.073105     88.162338   \n",
              "\n",
              "      Phi (deg)  Action  \n",
              "0     73.747482       3  \n",
              "1     75.108047       3  \n",
              "2     75.349052       3  \n",
              "3     76.006027       3  \n",
              "4     77.664307       3  \n",
              "..          ...     ...  \n",
              "569   69.462387       3  \n",
              "570   69.018478       3  \n",
              "571   68.385696       3  \n",
              "572   69.044220       3  \n",
              "573   70.243568       3  \n",
              "\n",
              "[15388 rows x 8 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-be08c288-c994-4282-9b61-3c056574eef7\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Time (s)</th>\n",
              "      <th>X (m/s2)</th>\n",
              "      <th>Y (m/s2)</th>\n",
              "      <th>Z (m/s2)</th>\n",
              "      <th>R (m/s2)</th>\n",
              "      <th>Theta (deg)</th>\n",
              "      <th>Phi (deg)</th>\n",
              "      <th>Action</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.897405</td>\n",
              "      <td>6.508637</td>\n",
              "      <td>7.580042</td>\n",
              "      <td>10.169540</td>\n",
              "      <td>41.809349</td>\n",
              "      <td>73.747482</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.009813</td>\n",
              "      <td>1.783680</td>\n",
              "      <td>6.707356</td>\n",
              "      <td>7.337031</td>\n",
              "      <td>10.099612</td>\n",
              "      <td>43.409012</td>\n",
              "      <td>75.108047</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.019684</td>\n",
              "      <td>1.868674</td>\n",
              "      <td>7.147889</td>\n",
              "      <td>7.380127</td>\n",
              "      <td>10.442726</td>\n",
              "      <td>45.030994</td>\n",
              "      <td>75.349052</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.029636</td>\n",
              "      <td>1.713051</td>\n",
              "      <td>6.873753</td>\n",
              "      <td>7.319075</td>\n",
              "      <td>10.185866</td>\n",
              "      <td>44.064941</td>\n",
              "      <td>76.006027</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.039645</td>\n",
              "      <td>1.448492</td>\n",
              "      <td>6.623559</td>\n",
              "      <td>7.020997</td>\n",
              "      <td>9.760330</td>\n",
              "      <td>43.999977</td>\n",
              "      <td>77.664307</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>569</th>\n",
              "      <td>5.689598</td>\n",
              "      <td>0.373774</td>\n",
              "      <td>0.997706</td>\n",
              "      <td>0.107629</td>\n",
              "      <td>1.070845</td>\n",
              "      <td>84.231529</td>\n",
              "      <td>69.462387</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>570</th>\n",
              "      <td>5.699913</td>\n",
              "      <td>0.378777</td>\n",
              "      <td>0.987700</td>\n",
              "      <td>0.113731</td>\n",
              "      <td>1.063935</td>\n",
              "      <td>83.863571</td>\n",
              "      <td>69.018478</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>571</th>\n",
              "      <td>5.709731</td>\n",
              "      <td>0.380851</td>\n",
              "      <td>0.961219</td>\n",
              "      <td>0.117758</td>\n",
              "      <td>1.040604</td>\n",
              "      <td>83.502335</td>\n",
              "      <td>68.385696</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>572</th>\n",
              "      <td>5.719699</td>\n",
              "      <td>0.373408</td>\n",
              "      <td>0.975009</td>\n",
              "      <td>0.070533</td>\n",
              "      <td>1.046446</td>\n",
              "      <td>86.135223</td>\n",
              "      <td>69.044220</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>573</th>\n",
              "      <td>5.730040</td>\n",
              "      <td>0.362547</td>\n",
              "      <td>1.009421</td>\n",
              "      <td>0.034412</td>\n",
              "      <td>1.073105</td>\n",
              "      <td>88.162338</td>\n",
              "      <td>70.243568</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>15388 rows × 8 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-be08c288-c994-4282-9b61-3c056574eef7')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-be08c288-c994-4282-9b61-3c056574eef7 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-be08c288-c994-4282-9b61-3c056574eef7');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-veEhtcM_7Nn",
        "outputId": "a1fde565-a07e-4516-9faa-5daae0fd6c33"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "15388"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#is data balanced?\n",
        "numdata = []\n",
        "\n",
        "for i in range (0,5):\n",
        "  numdata.append(len(all_df.loc[all_df['Action'] == i]))"
      ],
      "metadata": {
        "id": "2Vstuw9Y-Dk5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "numdata"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sZF-V61pAbsh",
        "outputId": "2a50dae3-44a5-43f8-e93e-9f71c835564f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[12823, 6369, 11363, 15388, 6970]"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_nd = pd.DataFrame(numdata, columns=[''])\n",
        "ax = df_nd.plot(kind='bar', title = \"action distribution\")\n",
        "ax.set_xlabel(\"action label\")\n",
        "ax.set_ylabel(\"frequency\")\n",
        "ax.get_legend().remove()\n",
        "plt.show()\n",
        "#We seem to have less data on the \"hold\" and \"shoot\" actions."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 292
        },
        "id": "El5SgFhNA7Ll",
        "outputId": "c6087d6b-07d8-4c47-df48-83f13b51900d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Even if we only took 6369 examples from each dataset, \n",
        "#we would still be left with about 30,000 examples and plenty of data\n",
        "#for train/val/test split\n",
        "#so let's do that via random selection and rejoin the dataset.\n",
        "balanced_combiner = []\n",
        "for i in range (0,5):\n",
        "  chunk_df = all_df.loc[all_df['Action'] == i]\n",
        "  balanced_temp = chunk_df.sample(n=6369, random_state=RANDOM_STATE)\n",
        "  balanced_combiner.append(balanced_temp)\n",
        "balanced_all = pd.concat(balanced_combiner)\n",
        "balanced_all = balanced_all.reset_index(drop=True)"
      ],
      "metadata": {
        "id": "d7lUeD0-DhBH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "balanced_all"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "-C2079FKEPB4",
        "outputId": "31bd533d-6fc3-4aee-8dc1-7d6acfb24692"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "       Time (s)   X (m/s2)   Y (m/s2)   Z (m/s2)   R (m/s2)   Theta (deg)  \\\n",
              "0      9.286569   7.256825   8.359355   6.147113  12.662043     60.956364   \n",
              "1      6.519716   4.259285   9.551667   0.842759  10.492192     85.392899   \n",
              "2      1.930154   1.389834   9.433155   1.292869   9.622243     82.278236   \n",
              "3      5.190177   5.582081   8.097190   2.985569  10.278022     73.113251   \n",
              "4      1.839292   1.944092   9.502586   1.479617   9.811621     81.326569   \n",
              "...         ...        ...        ...        ...        ...           ...   \n",
              "31840  0.330058   5.899313   8.258799   0.590171  10.166512     86.672081   \n",
              "31841  2.910930   4.835090   8.590396  -0.167594   9.859061     90.974014   \n",
              "31842  0.900034   0.041246   0.559623   0.199151   0.595433     70.460068   \n",
              "31843  2.800076   2.767697   9.557653  -0.118513   9.951027     90.682388   \n",
              "31844  2.610846   2.853889  10.117896   0.617704  10.530814     86.637283   \n",
              "\n",
              "        Phi (deg)  Action  \n",
              "0       49.038479       0  \n",
              "1       65.966927       0  \n",
              "2       81.618622       0  \n",
              "3       55.418201       0  \n",
              "4       78.437668       0  \n",
              "...           ...     ...  \n",
              "31840   54.461536       4  \n",
              "31841   60.627087       4  \n",
              "31842   85.784775       4  \n",
              "31843   73.850082       4  \n",
              "31844   74.248184       4  \n",
              "\n",
              "[31845 rows x 8 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-3ef57301-bfc7-4cae-ae09-3d1d49bfaed0\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Time (s)</th>\n",
              "      <th>X (m/s2)</th>\n",
              "      <th>Y (m/s2)</th>\n",
              "      <th>Z (m/s2)</th>\n",
              "      <th>R (m/s2)</th>\n",
              "      <th>Theta (deg)</th>\n",
              "      <th>Phi (deg)</th>\n",
              "      <th>Action</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>9.286569</td>\n",
              "      <td>7.256825</td>\n",
              "      <td>8.359355</td>\n",
              "      <td>6.147113</td>\n",
              "      <td>12.662043</td>\n",
              "      <td>60.956364</td>\n",
              "      <td>49.038479</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>6.519716</td>\n",
              "      <td>4.259285</td>\n",
              "      <td>9.551667</td>\n",
              "      <td>0.842759</td>\n",
              "      <td>10.492192</td>\n",
              "      <td>85.392899</td>\n",
              "      <td>65.966927</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1.930154</td>\n",
              "      <td>1.389834</td>\n",
              "      <td>9.433155</td>\n",
              "      <td>1.292869</td>\n",
              "      <td>9.622243</td>\n",
              "      <td>82.278236</td>\n",
              "      <td>81.618622</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>5.190177</td>\n",
              "      <td>5.582081</td>\n",
              "      <td>8.097190</td>\n",
              "      <td>2.985569</td>\n",
              "      <td>10.278022</td>\n",
              "      <td>73.113251</td>\n",
              "      <td>55.418201</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1.839292</td>\n",
              "      <td>1.944092</td>\n",
              "      <td>9.502586</td>\n",
              "      <td>1.479617</td>\n",
              "      <td>9.811621</td>\n",
              "      <td>81.326569</td>\n",
              "      <td>78.437668</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31840</th>\n",
              "      <td>0.330058</td>\n",
              "      <td>5.899313</td>\n",
              "      <td>8.258799</td>\n",
              "      <td>0.590171</td>\n",
              "      <td>10.166512</td>\n",
              "      <td>86.672081</td>\n",
              "      <td>54.461536</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31841</th>\n",
              "      <td>2.910930</td>\n",
              "      <td>4.835090</td>\n",
              "      <td>8.590396</td>\n",
              "      <td>-0.167594</td>\n",
              "      <td>9.859061</td>\n",
              "      <td>90.974014</td>\n",
              "      <td>60.627087</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31842</th>\n",
              "      <td>0.900034</td>\n",
              "      <td>0.041246</td>\n",
              "      <td>0.559623</td>\n",
              "      <td>0.199151</td>\n",
              "      <td>0.595433</td>\n",
              "      <td>70.460068</td>\n",
              "      <td>85.784775</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31843</th>\n",
              "      <td>2.800076</td>\n",
              "      <td>2.767697</td>\n",
              "      <td>9.557653</td>\n",
              "      <td>-0.118513</td>\n",
              "      <td>9.951027</td>\n",
              "      <td>90.682388</td>\n",
              "      <td>73.850082</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31844</th>\n",
              "      <td>2.610846</td>\n",
              "      <td>2.853889</td>\n",
              "      <td>10.117896</td>\n",
              "      <td>0.617704</td>\n",
              "      <td>10.530814</td>\n",
              "      <td>86.637283</td>\n",
              "      <td>74.248184</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>31845 rows × 8 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-3ef57301-bfc7-4cae-ae09-3d1d49bfaed0')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-3ef57301-bfc7-4cae-ae09-3d1d49bfaed0 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-3ef57301-bfc7-4cae-ae09-3d1d49bfaed0');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Looks balanced!\n",
        "numdatanew = []\n",
        "for i in range (0,5):\n",
        "  numdatanew.append(len(balanced_all.loc[balanced_all['Action'] == i]))\n",
        "print(numdatanew)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TwR5eU9yEUwK",
        "outputId": "1157c745-c8f4-464e-ad99-325ef26eb4ed"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[6369, 6369, 6369, 6369, 6369]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_nd = pd.DataFrame(numdatanew, columns=[''])\n",
        "ax = df_nd.plot(kind='bar', title = \"action distribution\")\n",
        "ax.set_xlabel(\"action label\")\n",
        "ax.set_ylabel(\"frequency\")\n",
        "ax.get_legend().remove()\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 292
        },
        "id": "2n4dbDcnEk57",
        "outputId": "3aab0406-dadc-45a5-caf6-59d9790d82c0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Summary of all stats\n",
        "#Some of the stats - especailly those involve degrees\n",
        "#vary wildly, as indicated by std.\n",
        "#We will normalize these following a train/val/test split.\n",
        "balanced_all.describe()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 300
        },
        "id": "EuCk4SA4GDPn",
        "outputId": "fb779aae-44b7-45fd-c775-6fe5922f2aba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "           Time (s)      X (m/s2)      Y (m/s2)      Z (m/s2)      R (m/s2)  \\\n",
              "count  31845.000000  31845.000000  31845.000000  31845.000000  31845.000000   \n",
              "mean       3.681323      3.395105      6.532801      1.008307      8.653781   \n",
              "std        2.925320      4.027623      4.081234      2.681248      4.516600   \n",
              "min        0.000000    -39.195477    -15.529989    -33.500866      0.130054   \n",
              "25%        1.529681      0.528750      4.237737      0.064643      8.814483   \n",
              "50%        3.070108      3.300407      7.987057      0.766145      9.833926   \n",
              "75%        5.009738      5.309142      9.099164      1.884237     10.001303   \n",
              "max       20.670580     38.540661     39.221813     28.358122     54.321270   \n",
              "\n",
              "        Theta (deg)     Phi (deg)        Action  \n",
              "count  31845.000000  31845.000000  31845.000000  \n",
              "mean      81.248700     75.564008      2.000000  \n",
              "std       14.943924     51.551630      1.414236  \n",
              "min        9.837831      0.005464      0.000000  \n",
              "25%       75.878181     57.309948      1.000000  \n",
              "50%       81.005798     68.394600      2.000000  \n",
              "75%       88.917610     77.554070      3.000000  \n",
              "max      167.521606    359.945709      4.000000  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-e06b7af2-e7d3-48c2-afef-032f0fda4404\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Time (s)</th>\n",
              "      <th>X (m/s2)</th>\n",
              "      <th>Y (m/s2)</th>\n",
              "      <th>Z (m/s2)</th>\n",
              "      <th>R (m/s2)</th>\n",
              "      <th>Theta (deg)</th>\n",
              "      <th>Phi (deg)</th>\n",
              "      <th>Action</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>31845.000000</td>\n",
              "      <td>31845.000000</td>\n",
              "      <td>31845.000000</td>\n",
              "      <td>31845.000000</td>\n",
              "      <td>31845.000000</td>\n",
              "      <td>31845.000000</td>\n",
              "      <td>31845.000000</td>\n",
              "      <td>31845.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>3.681323</td>\n",
              "      <td>3.395105</td>\n",
              "      <td>6.532801</td>\n",
              "      <td>1.008307</td>\n",
              "      <td>8.653781</td>\n",
              "      <td>81.248700</td>\n",
              "      <td>75.564008</td>\n",
              "      <td>2.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>2.925320</td>\n",
              "      <td>4.027623</td>\n",
              "      <td>4.081234</td>\n",
              "      <td>2.681248</td>\n",
              "      <td>4.516600</td>\n",
              "      <td>14.943924</td>\n",
              "      <td>51.551630</td>\n",
              "      <td>1.414236</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>-39.195477</td>\n",
              "      <td>-15.529989</td>\n",
              "      <td>-33.500866</td>\n",
              "      <td>0.130054</td>\n",
              "      <td>9.837831</td>\n",
              "      <td>0.005464</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>1.529681</td>\n",
              "      <td>0.528750</td>\n",
              "      <td>4.237737</td>\n",
              "      <td>0.064643</td>\n",
              "      <td>8.814483</td>\n",
              "      <td>75.878181</td>\n",
              "      <td>57.309948</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>3.070108</td>\n",
              "      <td>3.300407</td>\n",
              "      <td>7.987057</td>\n",
              "      <td>0.766145</td>\n",
              "      <td>9.833926</td>\n",
              "      <td>81.005798</td>\n",
              "      <td>68.394600</td>\n",
              "      <td>2.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>5.009738</td>\n",
              "      <td>5.309142</td>\n",
              "      <td>9.099164</td>\n",
              "      <td>1.884237</td>\n",
              "      <td>10.001303</td>\n",
              "      <td>88.917610</td>\n",
              "      <td>77.554070</td>\n",
              "      <td>3.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>20.670580</td>\n",
              "      <td>38.540661</td>\n",
              "      <td>39.221813</td>\n",
              "      <td>28.358122</td>\n",
              "      <td>54.321270</td>\n",
              "      <td>167.521606</td>\n",
              "      <td>359.945709</td>\n",
              "      <td>4.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-e06b7af2-e7d3-48c2-afef-032f0fda4404')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-e06b7af2-e7d3-48c2-afef-032f0fda4404 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-e06b7af2-e7d3-48c2-afef-032f0fda4404');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Looking at data this way helps us get a better understanding of what the parameters reference as well. For example, \"Time = 0\" would be the representation of an athlete's velocty and angular momentum when starting an action. Actions last up to 20 seconds, meaning that an action's other features can vary depending on the time the action is recorded (for example, if an athlete jumped during an action, their vertical velocity would be positive at one point and negative at another.) "
      ],
      "metadata": {
        "id": "9aPT8VDbIXJL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "balanced_all.loc[balanced_all['Time (s)'] == 0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "LKgwhHNfHgOT",
        "outputId": "15355f87-d7af-48f5-b532-dee6d836761f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "       Time (s)   X (m/s2)   Y (m/s2)   Z (m/s2)   R (m/s2)   Theta (deg)  \\\n",
              "1097        0.0   1.829170   9.677363   0.592565   9.866528     86.556854   \n",
              "2345        0.0   2.612074   9.417592   0.434548   9.782781     87.454102   \n",
              "2478        0.0   3.479972   9.069236  -2.246958   9.970460    103.024139   \n",
              "5426        0.0   1.865083   9.044097   2.370260   9.533749     75.604263   \n",
              "6524        0.0   2.998738   9.139865   1.316811   9.708941     82.205025   \n",
              "6531        0.0   2.920926   9.548077   0.484826   9.996631     87.220123   \n",
              "7201        0.0   0.391346   0.826256   0.225021   0.941533     76.172829   \n",
              "8266        0.0   0.209889   0.836140   0.076756   0.865491     84.912048   \n",
              "8357        0.0   5.066131   8.058883  -1.243788   9.599910     97.444305   \n",
              "9219        0.0   6.501455   8.044518  -0.800860  10.374226     94.427475   \n",
              "9255        0.0   4.000711   9.130288   1.353921  10.059868     82.265297   \n",
              "9746        0.0   2.960430   8.360553   1.408988   8.980436     80.973267   \n",
              "10182       0.0   3.470395   9.206903   0.305261   9.843977     88.222977   \n",
              "11190       0.0   5.863400   7.539341  -0.681150   9.575233     94.079277   \n",
              "11714       0.0   0.294211   0.880559   0.122395   0.936442     82.489853   \n",
              "12676       0.0   2.971204   9.275137   0.166397   9.740837     89.021202   \n",
              "13749       0.0   8.256405   5.963956  -3.403358  10.738708    108.477020   \n",
              "14361       0.0   3.957615   8.396465   2.477999   9.607489     75.053108   \n",
              "15478       0.0   0.124591   0.896910   0.209157   0.929364     76.993950   \n",
              "15577       0.0   6.669049   6.271611  -2.852691   9.588908    107.307457   \n",
              "16749       0.0   4.684256   8.234857   0.736217   9.502481     85.556480   \n",
              "16854       0.0   0.359740   0.904354   0.357300   1.036790     69.841316   \n",
              "17143       0.0   4.277241   8.761581  -1.459266   9.858477     98.512283   \n",
              "17586       0.0   5.412093   8.191761   1.158794   9.886279     83.268753   \n",
              "17843       0.0   5.492299   7.697359   1.993173   9.663716     78.097130   \n",
              "19812       0.0  -0.374693   8.285135   3.974375   9.196712     64.395813   \n",
              "20124       0.0   0.374693   5.898116   7.805098   9.790185     37.132942   \n",
              "20133       0.0  -0.374693   8.285135   3.974375   9.196712     64.395813   \n",
              "22282       0.0   1.906982   8.396465   4.596867   9.760553     61.903126   \n",
              "23421       0.0   1.030704   6.658275   7.202956   9.862938     43.088005   \n",
              "23431       0.0  -0.165837   0.987700  -0.111046   1.007662     96.326942   \n",
              "25053       0.0   3.220201   6.949171   4.565742   8.916651     59.199738   \n",
              "25328       0.0   1.632846   9.157822   2.845509   9.727734     72.991409   \n",
              "25657       0.0   0.708684   9.187749   0.842759   9.253497     84.774567   \n",
              "25760       0.0   8.186973   4.508282  -2.455254   9.663301    104.719093   \n",
              "26293       0.0   1.045069   9.355343   0.440533   9.423836     87.320633   \n",
              "26432       0.0   6.373365   6.652289  -2.111686   9.451558    102.910103   \n",
              "27065       0.0   1.904588   9.714474   1.030704   9.952929     84.055916   \n",
              "27293       0.0   0.118612   0.885806   0.216357   0.919528     76.391212   \n",
              "27754       0.0   2.998738   8.510190  -0.070629   9.023344     90.448479   \n",
              "27858       0.0  -0.052838   0.856031  -0.056987   0.859551     93.801437   \n",
              "28323       0.0   0.001197   9.810242  -0.128090   9.811078     90.748055   \n",
              "28928       0.0   7.295133   6.556521  -1.624466   9.942123     99.403847   \n",
              "29853       0.0   2.220622   8.841787  -0.401029   9.125196     92.518814   \n",
              "30096       0.0   2.110489   9.163807  -0.760159   9.434371     94.621521   \n",
              "30761       0.0   6.846220   6.313510  -2.704251   9.697634    106.191986   \n",
              "31224       0.0   2.158373   8.790312   1.083376   9.116022     83.174660   \n",
              "31572       0.0   0.203544   0.967077   0.145092   0.998859     81.647797   \n",
              "\n",
              "        Phi (deg)  Action  \n",
              "1097    79.296494       0  \n",
              "2345    74.498055       0  \n",
              "2478    69.007607       0  \n",
              "5426    78.347748       0  \n",
              "6524    71.835632       1  \n",
              "6531    72.990196       1  \n",
              "7201    64.656006       1  \n",
              "8266    75.908676       1  \n",
              "8357    57.844917       1  \n",
              "9219    51.055393       1  \n",
              "9255    66.337921       1  \n",
              "9746    70.501198       1  \n",
              "10182   69.346863       1  \n",
              "11190   52.127537       1  \n",
              "11714   71.524567       1  \n",
              "12676   72.237617       1  \n",
              "13749   35.842201       2  \n",
              "14361   64.763481       2  \n",
              "15478   82.091560       2  \n",
              "15577   43.240868       2  \n",
              "16749   60.367367       2  \n",
              "16854   68.307976       2  \n",
              "17143   63.979229       2  \n",
              "17586   56.548248       2  \n",
              "17843   54.490993       2  \n",
              "19812   92.589417       3  \n",
              "20124   86.365028       3  \n",
              "20133   92.589417       3  \n",
              "22282   77.204208       3  \n",
              "23421   81.200432       3  \n",
              "23431   99.531181       3  \n",
              "25053   65.137291       3  \n",
              "25328   79.890366       3  \n",
              "25657   85.589310       4  \n",
              "25760   28.840052       4  \n",
              "26293   83.626015       4  \n",
              "26432   46.226715       4  \n",
              "27065   78.907478       4  \n",
              "27293   82.373306       4  \n",
              "27754   70.589058       4  \n",
              "27858   93.532097       4  \n",
              "28323   89.993011       4  \n",
              "28928   41.947708       4  \n",
              "29853   75.901718       4  \n",
              "30096   77.030533       4  \n",
              "30761   42.681915       4  \n",
              "31224   76.204514       4  \n",
              "31572   78.114258       4  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-2e264d73-610a-43d1-a848-620d3feb35e3\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Time (s)</th>\n",
              "      <th>X (m/s2)</th>\n",
              "      <th>Y (m/s2)</th>\n",
              "      <th>Z (m/s2)</th>\n",
              "      <th>R (m/s2)</th>\n",
              "      <th>Theta (deg)</th>\n",
              "      <th>Phi (deg)</th>\n",
              "      <th>Action</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1097</th>\n",
              "      <td>0.0</td>\n",
              "      <td>1.829170</td>\n",
              "      <td>9.677363</td>\n",
              "      <td>0.592565</td>\n",
              "      <td>9.866528</td>\n",
              "      <td>86.556854</td>\n",
              "      <td>79.296494</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2345</th>\n",
              "      <td>0.0</td>\n",
              "      <td>2.612074</td>\n",
              "      <td>9.417592</td>\n",
              "      <td>0.434548</td>\n",
              "      <td>9.782781</td>\n",
              "      <td>87.454102</td>\n",
              "      <td>74.498055</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2478</th>\n",
              "      <td>0.0</td>\n",
              "      <td>3.479972</td>\n",
              "      <td>9.069236</td>\n",
              "      <td>-2.246958</td>\n",
              "      <td>9.970460</td>\n",
              "      <td>103.024139</td>\n",
              "      <td>69.007607</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5426</th>\n",
              "      <td>0.0</td>\n",
              "      <td>1.865083</td>\n",
              "      <td>9.044097</td>\n",
              "      <td>2.370260</td>\n",
              "      <td>9.533749</td>\n",
              "      <td>75.604263</td>\n",
              "      <td>78.347748</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6524</th>\n",
              "      <td>0.0</td>\n",
              "      <td>2.998738</td>\n",
              "      <td>9.139865</td>\n",
              "      <td>1.316811</td>\n",
              "      <td>9.708941</td>\n",
              "      <td>82.205025</td>\n",
              "      <td>71.835632</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6531</th>\n",
              "      <td>0.0</td>\n",
              "      <td>2.920926</td>\n",
              "      <td>9.548077</td>\n",
              "      <td>0.484826</td>\n",
              "      <td>9.996631</td>\n",
              "      <td>87.220123</td>\n",
              "      <td>72.990196</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7201</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.391346</td>\n",
              "      <td>0.826256</td>\n",
              "      <td>0.225021</td>\n",
              "      <td>0.941533</td>\n",
              "      <td>76.172829</td>\n",
              "      <td>64.656006</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8266</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.209889</td>\n",
              "      <td>0.836140</td>\n",
              "      <td>0.076756</td>\n",
              "      <td>0.865491</td>\n",
              "      <td>84.912048</td>\n",
              "      <td>75.908676</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8357</th>\n",
              "      <td>0.0</td>\n",
              "      <td>5.066131</td>\n",
              "      <td>8.058883</td>\n",
              "      <td>-1.243788</td>\n",
              "      <td>9.599910</td>\n",
              "      <td>97.444305</td>\n",
              "      <td>57.844917</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9219</th>\n",
              "      <td>0.0</td>\n",
              "      <td>6.501455</td>\n",
              "      <td>8.044518</td>\n",
              "      <td>-0.800860</td>\n",
              "      <td>10.374226</td>\n",
              "      <td>94.427475</td>\n",
              "      <td>51.055393</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9255</th>\n",
              "      <td>0.0</td>\n",
              "      <td>4.000711</td>\n",
              "      <td>9.130288</td>\n",
              "      <td>1.353921</td>\n",
              "      <td>10.059868</td>\n",
              "      <td>82.265297</td>\n",
              "      <td>66.337921</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9746</th>\n",
              "      <td>0.0</td>\n",
              "      <td>2.960430</td>\n",
              "      <td>8.360553</td>\n",
              "      <td>1.408988</td>\n",
              "      <td>8.980436</td>\n",
              "      <td>80.973267</td>\n",
              "      <td>70.501198</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10182</th>\n",
              "      <td>0.0</td>\n",
              "      <td>3.470395</td>\n",
              "      <td>9.206903</td>\n",
              "      <td>0.305261</td>\n",
              "      <td>9.843977</td>\n",
              "      <td>88.222977</td>\n",
              "      <td>69.346863</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11190</th>\n",
              "      <td>0.0</td>\n",
              "      <td>5.863400</td>\n",
              "      <td>7.539341</td>\n",
              "      <td>-0.681150</td>\n",
              "      <td>9.575233</td>\n",
              "      <td>94.079277</td>\n",
              "      <td>52.127537</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11714</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.294211</td>\n",
              "      <td>0.880559</td>\n",
              "      <td>0.122395</td>\n",
              "      <td>0.936442</td>\n",
              "      <td>82.489853</td>\n",
              "      <td>71.524567</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12676</th>\n",
              "      <td>0.0</td>\n",
              "      <td>2.971204</td>\n",
              "      <td>9.275137</td>\n",
              "      <td>0.166397</td>\n",
              "      <td>9.740837</td>\n",
              "      <td>89.021202</td>\n",
              "      <td>72.237617</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13749</th>\n",
              "      <td>0.0</td>\n",
              "      <td>8.256405</td>\n",
              "      <td>5.963956</td>\n",
              "      <td>-3.403358</td>\n",
              "      <td>10.738708</td>\n",
              "      <td>108.477020</td>\n",
              "      <td>35.842201</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14361</th>\n",
              "      <td>0.0</td>\n",
              "      <td>3.957615</td>\n",
              "      <td>8.396465</td>\n",
              "      <td>2.477999</td>\n",
              "      <td>9.607489</td>\n",
              "      <td>75.053108</td>\n",
              "      <td>64.763481</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15478</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.124591</td>\n",
              "      <td>0.896910</td>\n",
              "      <td>0.209157</td>\n",
              "      <td>0.929364</td>\n",
              "      <td>76.993950</td>\n",
              "      <td>82.091560</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15577</th>\n",
              "      <td>0.0</td>\n",
              "      <td>6.669049</td>\n",
              "      <td>6.271611</td>\n",
              "      <td>-2.852691</td>\n",
              "      <td>9.588908</td>\n",
              "      <td>107.307457</td>\n",
              "      <td>43.240868</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16749</th>\n",
              "      <td>0.0</td>\n",
              "      <td>4.684256</td>\n",
              "      <td>8.234857</td>\n",
              "      <td>0.736217</td>\n",
              "      <td>9.502481</td>\n",
              "      <td>85.556480</td>\n",
              "      <td>60.367367</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16854</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.359740</td>\n",
              "      <td>0.904354</td>\n",
              "      <td>0.357300</td>\n",
              "      <td>1.036790</td>\n",
              "      <td>69.841316</td>\n",
              "      <td>68.307976</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17143</th>\n",
              "      <td>0.0</td>\n",
              "      <td>4.277241</td>\n",
              "      <td>8.761581</td>\n",
              "      <td>-1.459266</td>\n",
              "      <td>9.858477</td>\n",
              "      <td>98.512283</td>\n",
              "      <td>63.979229</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17586</th>\n",
              "      <td>0.0</td>\n",
              "      <td>5.412093</td>\n",
              "      <td>8.191761</td>\n",
              "      <td>1.158794</td>\n",
              "      <td>9.886279</td>\n",
              "      <td>83.268753</td>\n",
              "      <td>56.548248</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17843</th>\n",
              "      <td>0.0</td>\n",
              "      <td>5.492299</td>\n",
              "      <td>7.697359</td>\n",
              "      <td>1.993173</td>\n",
              "      <td>9.663716</td>\n",
              "      <td>78.097130</td>\n",
              "      <td>54.490993</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19812</th>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.374693</td>\n",
              "      <td>8.285135</td>\n",
              "      <td>3.974375</td>\n",
              "      <td>9.196712</td>\n",
              "      <td>64.395813</td>\n",
              "      <td>92.589417</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20124</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.374693</td>\n",
              "      <td>5.898116</td>\n",
              "      <td>7.805098</td>\n",
              "      <td>9.790185</td>\n",
              "      <td>37.132942</td>\n",
              "      <td>86.365028</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20133</th>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.374693</td>\n",
              "      <td>8.285135</td>\n",
              "      <td>3.974375</td>\n",
              "      <td>9.196712</td>\n",
              "      <td>64.395813</td>\n",
              "      <td>92.589417</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22282</th>\n",
              "      <td>0.0</td>\n",
              "      <td>1.906982</td>\n",
              "      <td>8.396465</td>\n",
              "      <td>4.596867</td>\n",
              "      <td>9.760553</td>\n",
              "      <td>61.903126</td>\n",
              "      <td>77.204208</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23421</th>\n",
              "      <td>0.0</td>\n",
              "      <td>1.030704</td>\n",
              "      <td>6.658275</td>\n",
              "      <td>7.202956</td>\n",
              "      <td>9.862938</td>\n",
              "      <td>43.088005</td>\n",
              "      <td>81.200432</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23431</th>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.165837</td>\n",
              "      <td>0.987700</td>\n",
              "      <td>-0.111046</td>\n",
              "      <td>1.007662</td>\n",
              "      <td>96.326942</td>\n",
              "      <td>99.531181</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25053</th>\n",
              "      <td>0.0</td>\n",
              "      <td>3.220201</td>\n",
              "      <td>6.949171</td>\n",
              "      <td>4.565742</td>\n",
              "      <td>8.916651</td>\n",
              "      <td>59.199738</td>\n",
              "      <td>65.137291</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25328</th>\n",
              "      <td>0.0</td>\n",
              "      <td>1.632846</td>\n",
              "      <td>9.157822</td>\n",
              "      <td>2.845509</td>\n",
              "      <td>9.727734</td>\n",
              "      <td>72.991409</td>\n",
              "      <td>79.890366</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25657</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.708684</td>\n",
              "      <td>9.187749</td>\n",
              "      <td>0.842759</td>\n",
              "      <td>9.253497</td>\n",
              "      <td>84.774567</td>\n",
              "      <td>85.589310</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25760</th>\n",
              "      <td>0.0</td>\n",
              "      <td>8.186973</td>\n",
              "      <td>4.508282</td>\n",
              "      <td>-2.455254</td>\n",
              "      <td>9.663301</td>\n",
              "      <td>104.719093</td>\n",
              "      <td>28.840052</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26293</th>\n",
              "      <td>0.0</td>\n",
              "      <td>1.045069</td>\n",
              "      <td>9.355343</td>\n",
              "      <td>0.440533</td>\n",
              "      <td>9.423836</td>\n",
              "      <td>87.320633</td>\n",
              "      <td>83.626015</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26432</th>\n",
              "      <td>0.0</td>\n",
              "      <td>6.373365</td>\n",
              "      <td>6.652289</td>\n",
              "      <td>-2.111686</td>\n",
              "      <td>9.451558</td>\n",
              "      <td>102.910103</td>\n",
              "      <td>46.226715</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27065</th>\n",
              "      <td>0.0</td>\n",
              "      <td>1.904588</td>\n",
              "      <td>9.714474</td>\n",
              "      <td>1.030704</td>\n",
              "      <td>9.952929</td>\n",
              "      <td>84.055916</td>\n",
              "      <td>78.907478</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27293</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.118612</td>\n",
              "      <td>0.885806</td>\n",
              "      <td>0.216357</td>\n",
              "      <td>0.919528</td>\n",
              "      <td>76.391212</td>\n",
              "      <td>82.373306</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27754</th>\n",
              "      <td>0.0</td>\n",
              "      <td>2.998738</td>\n",
              "      <td>8.510190</td>\n",
              "      <td>-0.070629</td>\n",
              "      <td>9.023344</td>\n",
              "      <td>90.448479</td>\n",
              "      <td>70.589058</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27858</th>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.052838</td>\n",
              "      <td>0.856031</td>\n",
              "      <td>-0.056987</td>\n",
              "      <td>0.859551</td>\n",
              "      <td>93.801437</td>\n",
              "      <td>93.532097</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28323</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.001197</td>\n",
              "      <td>9.810242</td>\n",
              "      <td>-0.128090</td>\n",
              "      <td>9.811078</td>\n",
              "      <td>90.748055</td>\n",
              "      <td>89.993011</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28928</th>\n",
              "      <td>0.0</td>\n",
              "      <td>7.295133</td>\n",
              "      <td>6.556521</td>\n",
              "      <td>-1.624466</td>\n",
              "      <td>9.942123</td>\n",
              "      <td>99.403847</td>\n",
              "      <td>41.947708</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29853</th>\n",
              "      <td>0.0</td>\n",
              "      <td>2.220622</td>\n",
              "      <td>8.841787</td>\n",
              "      <td>-0.401029</td>\n",
              "      <td>9.125196</td>\n",
              "      <td>92.518814</td>\n",
              "      <td>75.901718</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30096</th>\n",
              "      <td>0.0</td>\n",
              "      <td>2.110489</td>\n",
              "      <td>9.163807</td>\n",
              "      <td>-0.760159</td>\n",
              "      <td>9.434371</td>\n",
              "      <td>94.621521</td>\n",
              "      <td>77.030533</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30761</th>\n",
              "      <td>0.0</td>\n",
              "      <td>6.846220</td>\n",
              "      <td>6.313510</td>\n",
              "      <td>-2.704251</td>\n",
              "      <td>9.697634</td>\n",
              "      <td>106.191986</td>\n",
              "      <td>42.681915</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31224</th>\n",
              "      <td>0.0</td>\n",
              "      <td>2.158373</td>\n",
              "      <td>8.790312</td>\n",
              "      <td>1.083376</td>\n",
              "      <td>9.116022</td>\n",
              "      <td>83.174660</td>\n",
              "      <td>76.204514</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31572</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.203544</td>\n",
              "      <td>0.967077</td>\n",
              "      <td>0.145092</td>\n",
              "      <td>0.998859</td>\n",
              "      <td>81.647797</td>\n",
              "      <td>78.114258</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-2e264d73-610a-43d1-a848-620d3feb35e3')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-2e264d73-610a-43d1-a848-620d3feb35e3 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-2e264d73-610a-43d1-a848-620d3feb35e3');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Setup Neural Network (Task 2)"
      ],
      "metadata": {
        "id": "vIaXNuFNJwLf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Train-Dev-Test Split\n",
        "2. Perform a train-dev-test split."
      ],
      "metadata": {
        "id": "oE_-M5ErLIK_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#shuffle dataset\n",
        "train, test = train_test_split(balanced_all.sample(frac=1, random_state=RANDOM_STATE), test_size=0.2, random_state=RANDOM_STATE)#shuffle dataset\n",
        "train, val = train_test_split(train.sample(frac=1, random_state=RANDOM_STATE), test_size=0.2, random_state=RANDOM_STATE)"
      ],
      "metadata": {
        "id": "rlrPDWwTLSYo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train = train.reset_index(drop=True)\n",
        "train"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "N7-V6C2nMMr6",
        "outputId": "517fd64d-beb5-43b3-f7ff-7a962a77960f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "       Time (s)   X (m/s2)   Y (m/s2)   Z (m/s2)   R (m/s2)   Theta (deg)  \\\n",
              "0      3.999938   0.255528   0.986968  -0.016474   1.019642     90.925743   \n",
              "1      1.979857   9.759963   2.919729   0.678756  10.209917     86.188164   \n",
              "2      4.110168  -4.408922   7.669825   0.389058   8.855291     87.481895   \n",
              "3      2.298580   1.953669   9.520543   1.557428   9.842923     80.895927   \n",
              "4      2.469975   3.408146   9.175778   1.728614   9.939742     79.984810   \n",
              "...         ...        ...        ...        ...        ...           ...   \n",
              "20375  4.490175   5.809530   7.963115   0.034716   9.857131     89.798210   \n",
              "20376  3.199741   2.154782   9.575610   1.076194   9.873884     83.742676   \n",
              "20377  1.030245  -2.466028   8.526949  13.201628  15.908272     33.915710   \n",
              "20378  3.340062   7.097611   8.425196   0.300472  11.020449     88.437637   \n",
              "20379  7.169458  -0.168791   8.708909   1.559822   8.849103     79.847488   \n",
              "\n",
              "        Phi (deg)  Action  \n",
              "0       75.484726       0  \n",
              "1       16.654747       4  \n",
              "2      119.892014       0  \n",
              "3       78.403572       0  \n",
              "4       69.623566       1  \n",
              "...           ...     ...  \n",
              "20375   53.887192       1  \n",
              "20376   77.318085       4  \n",
              "20377  106.130081       4  \n",
              "20378   49.888313       0  \n",
              "20379   91.110336       0  \n",
              "\n",
              "[20380 rows x 8 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-ff9ec3e8-6684-4b36-9a43-0159e3601629\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Time (s)</th>\n",
              "      <th>X (m/s2)</th>\n",
              "      <th>Y (m/s2)</th>\n",
              "      <th>Z (m/s2)</th>\n",
              "      <th>R (m/s2)</th>\n",
              "      <th>Theta (deg)</th>\n",
              "      <th>Phi (deg)</th>\n",
              "      <th>Action</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>3.999938</td>\n",
              "      <td>0.255528</td>\n",
              "      <td>0.986968</td>\n",
              "      <td>-0.016474</td>\n",
              "      <td>1.019642</td>\n",
              "      <td>90.925743</td>\n",
              "      <td>75.484726</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1.979857</td>\n",
              "      <td>9.759963</td>\n",
              "      <td>2.919729</td>\n",
              "      <td>0.678756</td>\n",
              "      <td>10.209917</td>\n",
              "      <td>86.188164</td>\n",
              "      <td>16.654747</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>4.110168</td>\n",
              "      <td>-4.408922</td>\n",
              "      <td>7.669825</td>\n",
              "      <td>0.389058</td>\n",
              "      <td>8.855291</td>\n",
              "      <td>87.481895</td>\n",
              "      <td>119.892014</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2.298580</td>\n",
              "      <td>1.953669</td>\n",
              "      <td>9.520543</td>\n",
              "      <td>1.557428</td>\n",
              "      <td>9.842923</td>\n",
              "      <td>80.895927</td>\n",
              "      <td>78.403572</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2.469975</td>\n",
              "      <td>3.408146</td>\n",
              "      <td>9.175778</td>\n",
              "      <td>1.728614</td>\n",
              "      <td>9.939742</td>\n",
              "      <td>79.984810</td>\n",
              "      <td>69.623566</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20375</th>\n",
              "      <td>4.490175</td>\n",
              "      <td>5.809530</td>\n",
              "      <td>7.963115</td>\n",
              "      <td>0.034716</td>\n",
              "      <td>9.857131</td>\n",
              "      <td>89.798210</td>\n",
              "      <td>53.887192</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20376</th>\n",
              "      <td>3.199741</td>\n",
              "      <td>2.154782</td>\n",
              "      <td>9.575610</td>\n",
              "      <td>1.076194</td>\n",
              "      <td>9.873884</td>\n",
              "      <td>83.742676</td>\n",
              "      <td>77.318085</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20377</th>\n",
              "      <td>1.030245</td>\n",
              "      <td>-2.466028</td>\n",
              "      <td>8.526949</td>\n",
              "      <td>13.201628</td>\n",
              "      <td>15.908272</td>\n",
              "      <td>33.915710</td>\n",
              "      <td>106.130081</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20378</th>\n",
              "      <td>3.340062</td>\n",
              "      <td>7.097611</td>\n",
              "      <td>8.425196</td>\n",
              "      <td>0.300472</td>\n",
              "      <td>11.020449</td>\n",
              "      <td>88.437637</td>\n",
              "      <td>49.888313</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20379</th>\n",
              "      <td>7.169458</td>\n",
              "      <td>-0.168791</td>\n",
              "      <td>8.708909</td>\n",
              "      <td>1.559822</td>\n",
              "      <td>8.849103</td>\n",
              "      <td>79.847488</td>\n",
              "      <td>91.110336</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>20380 rows × 8 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-ff9ec3e8-6684-4b36-9a43-0159e3601629')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-ff9ec3e8-6684-4b36-9a43-0159e3601629 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-ff9ec3e8-6684-4b36-9a43-0159e3601629');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_x = train.loc[:, train.columns != \"Action\"]\n",
        "train_x=(train_x-train_x.mean())/train_x.std() #normalize\n",
        "train_x"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "7CrMCmaKN4ms",
        "outputId": "97e12f7a-a52e-40f9-96c8-2137d0fcaa4e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "       Time (s)   X (m/s2)   Y (m/s2)   Z (m/s2)   R (m/s2)   Theta (deg)  \\\n",
              "0      0.120478  -0.777307  -1.355379  -0.389368  -1.679786      0.655615   \n",
              "1     -0.574560   1.596702  -0.881912  -0.127130   0.353484      0.336485   \n",
              "2      0.158405  -1.942389   0.281717  -0.236403   0.053785      0.423633   \n",
              "3     -0.464899  -0.353147   0.735087   0.204302   0.272290     -0.020007   \n",
              "4     -0.405928   0.010151   0.650630   0.268873   0.293710     -0.081381   \n",
              "...         ...        ...        ...        ...        ...           ...   \n",
              "20375  0.289152   0.609967   0.353565  -0.370059   0.275433      0.579663   \n",
              "20376 -0.154841  -0.302913   0.748577   0.022782   0.279140      0.171754   \n",
              "20377 -0.901289  -1.457095   0.491687   4.596450   1.614197     -3.184661   \n",
              "20378 -0.106562   0.931702   0.466760  -0.269817   0.532807      0.488013   \n",
              "20379  1.210999  -0.883293   0.536261   0.205205   0.052416     -0.090632   \n",
              "\n",
              "        Phi (deg)  \n",
              "0       -0.001492  \n",
              "1       -1.144295  \n",
              "2        0.861142  \n",
              "3        0.055208  \n",
              "4       -0.115349  \n",
              "...           ...  \n",
              "20375   -0.421036  \n",
              "20376    0.034121  \n",
              "20377    0.593809  \n",
              "20378   -0.498716  \n",
              "20379    0.302043  \n",
              "\n",
              "[20380 rows x 7 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-4f4e27c0-6acb-4d95-8fd4-7001bfea60ed\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Time (s)</th>\n",
              "      <th>X (m/s2)</th>\n",
              "      <th>Y (m/s2)</th>\n",
              "      <th>Z (m/s2)</th>\n",
              "      <th>R (m/s2)</th>\n",
              "      <th>Theta (deg)</th>\n",
              "      <th>Phi (deg)</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.120478</td>\n",
              "      <td>-0.777307</td>\n",
              "      <td>-1.355379</td>\n",
              "      <td>-0.389368</td>\n",
              "      <td>-1.679786</td>\n",
              "      <td>0.655615</td>\n",
              "      <td>-0.001492</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>-0.574560</td>\n",
              "      <td>1.596702</td>\n",
              "      <td>-0.881912</td>\n",
              "      <td>-0.127130</td>\n",
              "      <td>0.353484</td>\n",
              "      <td>0.336485</td>\n",
              "      <td>-1.144295</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.158405</td>\n",
              "      <td>-1.942389</td>\n",
              "      <td>0.281717</td>\n",
              "      <td>-0.236403</td>\n",
              "      <td>0.053785</td>\n",
              "      <td>0.423633</td>\n",
              "      <td>0.861142</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>-0.464899</td>\n",
              "      <td>-0.353147</td>\n",
              "      <td>0.735087</td>\n",
              "      <td>0.204302</td>\n",
              "      <td>0.272290</td>\n",
              "      <td>-0.020007</td>\n",
              "      <td>0.055208</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>-0.405928</td>\n",
              "      <td>0.010151</td>\n",
              "      <td>0.650630</td>\n",
              "      <td>0.268873</td>\n",
              "      <td>0.293710</td>\n",
              "      <td>-0.081381</td>\n",
              "      <td>-0.115349</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20375</th>\n",
              "      <td>0.289152</td>\n",
              "      <td>0.609967</td>\n",
              "      <td>0.353565</td>\n",
              "      <td>-0.370059</td>\n",
              "      <td>0.275433</td>\n",
              "      <td>0.579663</td>\n",
              "      <td>-0.421036</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20376</th>\n",
              "      <td>-0.154841</td>\n",
              "      <td>-0.302913</td>\n",
              "      <td>0.748577</td>\n",
              "      <td>0.022782</td>\n",
              "      <td>0.279140</td>\n",
              "      <td>0.171754</td>\n",
              "      <td>0.034121</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20377</th>\n",
              "      <td>-0.901289</td>\n",
              "      <td>-1.457095</td>\n",
              "      <td>0.491687</td>\n",
              "      <td>4.596450</td>\n",
              "      <td>1.614197</td>\n",
              "      <td>-3.184661</td>\n",
              "      <td>0.593809</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20378</th>\n",
              "      <td>-0.106562</td>\n",
              "      <td>0.931702</td>\n",
              "      <td>0.466760</td>\n",
              "      <td>-0.269817</td>\n",
              "      <td>0.532807</td>\n",
              "      <td>0.488013</td>\n",
              "      <td>-0.498716</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20379</th>\n",
              "      <td>1.210999</td>\n",
              "      <td>-0.883293</td>\n",
              "      <td>0.536261</td>\n",
              "      <td>0.205205</td>\n",
              "      <td>0.052416</td>\n",
              "      <td>-0.090632</td>\n",
              "      <td>0.302043</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>20380 rows × 7 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-4f4e27c0-6acb-4d95-8fd4-7001bfea60ed')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-4f4e27c0-6acb-4d95-8fd4-7001bfea60ed button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-4f4e27c0-6acb-4d95-8fd4-7001bfea60ed');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_y = train.loc[:, train.columns == \"Action\"]\n",
        "train_y"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "f3QHH-_ROJ5C",
        "outputId": "8e1d7d28-9268-4556-afd6-32eb09b57682"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "       Action\n",
              "0           0\n",
              "1           4\n",
              "2           0\n",
              "3           0\n",
              "4           1\n",
              "...       ...\n",
              "20375       1\n",
              "20376       4\n",
              "20377       4\n",
              "20378       0\n",
              "20379       0\n",
              "\n",
              "[20380 rows x 1 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-d21c8275-586d-4b5c-ad6f-641f7193b9ac\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Action</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20375</th>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20376</th>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20377</th>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20378</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20379</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>20380 rows × 1 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-d21c8275-586d-4b5c-ad6f-641f7193b9ac')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-d21c8275-586d-4b5c-ad6f-641f7193b9ac button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-d21c8275-586d-4b5c-ad6f-641f7193b9ac');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "val = val.reset_index(drop=True)\n",
        "val"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "m-BebthUNX-q",
        "outputId": "015f10e0-32dd-4396-b5e4-9f47bd5753cd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "      Time (s)   X (m/s2)   Y (m/s2)   Z (m/s2)   R (m/s2)   Theta (deg)  \\\n",
              "0     0.820280   0.383292   0.880803   0.301167   1.006691     72.592552   \n",
              "1     2.790099   0.849942  10.966640  -0.349553  11.005080     91.820190   \n",
              "2     0.768892   4.414908   8.609550   1.865083   9.853643     79.089317   \n",
              "3     2.039904   7.857770   6.654684   1.871069  10.465671     79.701202   \n",
              "4     1.449559  17.051504  -1.085770  -0.095768  17.086306     90.321144   \n",
              "...        ...        ...        ...        ...        ...           ...   \n",
              "5091  2.050196   0.393420   0.879338   0.298238   1.008445     72.798058   \n",
              "5092  2.425933   6.506243   7.214927  -1.222240   9.791845     97.170486   \n",
              "5093  3.370726   0.241814  10.687716   0.835576  10.723057     85.530792   \n",
              "5094  3.389042   0.022745   7.483078   0.629675   7.509558     85.190109   \n",
              "5095  2.629879   5.231331   8.293514   1.370680   9.900908     82.042419   \n",
              "\n",
              "       Phi (deg)  Action  \n",
              "0      66.483147       1  \n",
              "1      85.568298       4  \n",
              "2      62.851612       2  \n",
              "3      40.261002       4  \n",
              "4     356.356567       4  \n",
              "...          ...     ...  \n",
              "5091   65.896019       1  \n",
              "5092   47.956635       4  \n",
              "5093   88.703880       0  \n",
              "5094   89.825851       2  \n",
              "5095   57.757347       2  \n",
              "\n",
              "[5096 rows x 8 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-16d11c79-d678-48e5-96bf-61ac4d2acad4\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Time (s)</th>\n",
              "      <th>X (m/s2)</th>\n",
              "      <th>Y (m/s2)</th>\n",
              "      <th>Z (m/s2)</th>\n",
              "      <th>R (m/s2)</th>\n",
              "      <th>Theta (deg)</th>\n",
              "      <th>Phi (deg)</th>\n",
              "      <th>Action</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.820280</td>\n",
              "      <td>0.383292</td>\n",
              "      <td>0.880803</td>\n",
              "      <td>0.301167</td>\n",
              "      <td>1.006691</td>\n",
              "      <td>72.592552</td>\n",
              "      <td>66.483147</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2.790099</td>\n",
              "      <td>0.849942</td>\n",
              "      <td>10.966640</td>\n",
              "      <td>-0.349553</td>\n",
              "      <td>11.005080</td>\n",
              "      <td>91.820190</td>\n",
              "      <td>85.568298</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.768892</td>\n",
              "      <td>4.414908</td>\n",
              "      <td>8.609550</td>\n",
              "      <td>1.865083</td>\n",
              "      <td>9.853643</td>\n",
              "      <td>79.089317</td>\n",
              "      <td>62.851612</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2.039904</td>\n",
              "      <td>7.857770</td>\n",
              "      <td>6.654684</td>\n",
              "      <td>1.871069</td>\n",
              "      <td>10.465671</td>\n",
              "      <td>79.701202</td>\n",
              "      <td>40.261002</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1.449559</td>\n",
              "      <td>17.051504</td>\n",
              "      <td>-1.085770</td>\n",
              "      <td>-0.095768</td>\n",
              "      <td>17.086306</td>\n",
              "      <td>90.321144</td>\n",
              "      <td>356.356567</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5091</th>\n",
              "      <td>2.050196</td>\n",
              "      <td>0.393420</td>\n",
              "      <td>0.879338</td>\n",
              "      <td>0.298238</td>\n",
              "      <td>1.008445</td>\n",
              "      <td>72.798058</td>\n",
              "      <td>65.896019</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5092</th>\n",
              "      <td>2.425933</td>\n",
              "      <td>6.506243</td>\n",
              "      <td>7.214927</td>\n",
              "      <td>-1.222240</td>\n",
              "      <td>9.791845</td>\n",
              "      <td>97.170486</td>\n",
              "      <td>47.956635</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5093</th>\n",
              "      <td>3.370726</td>\n",
              "      <td>0.241814</td>\n",
              "      <td>10.687716</td>\n",
              "      <td>0.835576</td>\n",
              "      <td>10.723057</td>\n",
              "      <td>85.530792</td>\n",
              "      <td>88.703880</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5094</th>\n",
              "      <td>3.389042</td>\n",
              "      <td>0.022745</td>\n",
              "      <td>7.483078</td>\n",
              "      <td>0.629675</td>\n",
              "      <td>7.509558</td>\n",
              "      <td>85.190109</td>\n",
              "      <td>89.825851</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5095</th>\n",
              "      <td>2.629879</td>\n",
              "      <td>5.231331</td>\n",
              "      <td>8.293514</td>\n",
              "      <td>1.370680</td>\n",
              "      <td>9.900908</td>\n",
              "      <td>82.042419</td>\n",
              "      <td>57.757347</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5096 rows × 8 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-16d11c79-d678-48e5-96bf-61ac4d2acad4')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-16d11c79-d678-48e5-96bf-61ac4d2acad4 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-16d11c79-d678-48e5-96bf-61ac4d2acad4');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test  = test.reset_index(drop=True)\n",
        "test"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "N3q256opNYvS",
        "outputId": "35c27d67-df44-488f-8c54-4183a142ed7b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "       Time (s)   X (m/s2)   Y (m/s2)   Z (m/s2)   R (m/s2)   Theta (deg)  \\\n",
              "0      1.760212   4.745308  -7.856573   1.751359   9.344032     79.197121   \n",
              "1      2.176916   1.734599   8.254010   5.280412   9.950892     57.950817   \n",
              "2      2.550121   8.334216   5.339070   0.667982   9.920233     86.139046   \n",
              "3      0.470366   0.684742   7.862558   5.594052   9.673785     54.671097   \n",
              "4      4.750331   0.360473   0.939742   0.540831   1.142609     61.749329   \n",
              "...         ...        ...        ...        ...        ...           ...   \n",
              "6364  18.860218   4.795586   9.907207  -1.430536  11.099406     97.405106   \n",
              "6365   5.160208   5.942409   7.369353   2.594118   9.815754     74.675766   \n",
              "6366   1.499983 -14.744691  16.468517  10.061633  24.286919     65.525894   \n",
              "6367   2.250166   4.752491   8.382100   2.236184   9.891728     76.934425   \n",
              "6368   3.889659   2.412158   9.397242   1.721431   9.853425     79.938576   \n",
              "\n",
              "       Phi (deg)  Action  \n",
              "0     301.131653       4  \n",
              "1      78.131866       3  \n",
              "2      32.644459       2  \n",
              "3      85.022728       3  \n",
              "4      69.013863       2  \n",
              "...          ...     ...  \n",
              "6364   64.170685       3  \n",
              "6365   51.118416       0  \n",
              "6366  131.838913       4  \n",
              "6367   60.447594       2  \n",
              "6368   75.603676       0  \n",
              "\n",
              "[6369 rows x 8 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-739fa129-8118-4688-afb7-954dcc854abe\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Time (s)</th>\n",
              "      <th>X (m/s2)</th>\n",
              "      <th>Y (m/s2)</th>\n",
              "      <th>Z (m/s2)</th>\n",
              "      <th>R (m/s2)</th>\n",
              "      <th>Theta (deg)</th>\n",
              "      <th>Phi (deg)</th>\n",
              "      <th>Action</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1.760212</td>\n",
              "      <td>4.745308</td>\n",
              "      <td>-7.856573</td>\n",
              "      <td>1.751359</td>\n",
              "      <td>9.344032</td>\n",
              "      <td>79.197121</td>\n",
              "      <td>301.131653</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2.176916</td>\n",
              "      <td>1.734599</td>\n",
              "      <td>8.254010</td>\n",
              "      <td>5.280412</td>\n",
              "      <td>9.950892</td>\n",
              "      <td>57.950817</td>\n",
              "      <td>78.131866</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2.550121</td>\n",
              "      <td>8.334216</td>\n",
              "      <td>5.339070</td>\n",
              "      <td>0.667982</td>\n",
              "      <td>9.920233</td>\n",
              "      <td>86.139046</td>\n",
              "      <td>32.644459</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.470366</td>\n",
              "      <td>0.684742</td>\n",
              "      <td>7.862558</td>\n",
              "      <td>5.594052</td>\n",
              "      <td>9.673785</td>\n",
              "      <td>54.671097</td>\n",
              "      <td>85.022728</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4.750331</td>\n",
              "      <td>0.360473</td>\n",
              "      <td>0.939742</td>\n",
              "      <td>0.540831</td>\n",
              "      <td>1.142609</td>\n",
              "      <td>61.749329</td>\n",
              "      <td>69.013863</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6364</th>\n",
              "      <td>18.860218</td>\n",
              "      <td>4.795586</td>\n",
              "      <td>9.907207</td>\n",
              "      <td>-1.430536</td>\n",
              "      <td>11.099406</td>\n",
              "      <td>97.405106</td>\n",
              "      <td>64.170685</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6365</th>\n",
              "      <td>5.160208</td>\n",
              "      <td>5.942409</td>\n",
              "      <td>7.369353</td>\n",
              "      <td>2.594118</td>\n",
              "      <td>9.815754</td>\n",
              "      <td>74.675766</td>\n",
              "      <td>51.118416</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6366</th>\n",
              "      <td>1.499983</td>\n",
              "      <td>-14.744691</td>\n",
              "      <td>16.468517</td>\n",
              "      <td>10.061633</td>\n",
              "      <td>24.286919</td>\n",
              "      <td>65.525894</td>\n",
              "      <td>131.838913</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6367</th>\n",
              "      <td>2.250166</td>\n",
              "      <td>4.752491</td>\n",
              "      <td>8.382100</td>\n",
              "      <td>2.236184</td>\n",
              "      <td>9.891728</td>\n",
              "      <td>76.934425</td>\n",
              "      <td>60.447594</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6368</th>\n",
              "      <td>3.889659</td>\n",
              "      <td>2.412158</td>\n",
              "      <td>9.397242</td>\n",
              "      <td>1.721431</td>\n",
              "      <td>9.853425</td>\n",
              "      <td>79.938576</td>\n",
              "      <td>75.603676</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>6369 rows × 8 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-739fa129-8118-4688-afb7-954dcc854abe')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-739fa129-8118-4688-afb7-954dcc854abe button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-739fa129-8118-4688-afb7-954dcc854abe');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "val_x = val.loc[:, val.columns != \"Action\"]\n",
        "val_x=(val_x-val_x.mean())/val_x.std() #normalize\n",
        "val_y = val.loc[:, val.columns == \"Action\"]\n",
        "test_x = test.loc[:, test.columns != \"Action\"]\n",
        "test_x=(test_x-test_x.mean())/test_x.std() #normalize\n",
        "test_y = test.loc[:, test.columns == \"Action\"]"
      ],
      "metadata": {
        "id": "S72VVdcxOcay"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Pytorch Dataset Preparation"
      ],
      "metadata": {
        "id": "BMPZ_LyKbBCj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_y.values"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n2mBaMLdJKdi",
        "outputId": "e0752e96-c0ae-453c-b697-1c05ca907d34"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0],\n",
              "       [4],\n",
              "       [0],\n",
              "       ...,\n",
              "       [4],\n",
              "       [0],\n",
              "       [0]])"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_y.values.flatten()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K51PeBnrJadl",
        "outputId": "7c6f7ae8-9806-4751-fd49-5ef84bc9ccee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 4, 0, ..., 4, 0, 0])"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class BasketballDatasetTrain(Dataset):\n",
        "  def __init__(self):\n",
        "    self.x_train=torch.tensor(train_x.values,dtype=torch.float32)\n",
        "    self.y_train=torch.tensor(train_y.values.flatten(),dtype=torch.float32)\n",
        " \n",
        "  def __len__(self):\n",
        "    return len(self.y_train)\n",
        "   \n",
        "  def __getitem__(self,idx):\n",
        "    return self.x_train[idx],self.y_train[idx]\n",
        "\n",
        "class BasketballDatasetVal(Dataset):\n",
        "  def __init__(self):\n",
        "    self.x_val=torch.tensor(val_x.values,dtype=torch.float32)\n",
        "    self.y_val=torch.tensor(val_y.values.flatten(),dtype=torch.float32)\n",
        " \n",
        "  def __len__(self):\n",
        "    return len(self.y_val)\n",
        "   \n",
        "  def __getitem__(self,idx):\n",
        "    return self.x_val[idx],self.y_val[idx]\n",
        "\n",
        "class BasketballDatasetTest(Dataset):\n",
        "  def __init__(self):\n",
        "    self.x_test=torch.tensor(test_x.values,dtype=torch.float32)\n",
        "    self.y_test=torch.tensor(test_y.values.flatten(),dtype=torch.float32)\n",
        " \n",
        "  def __len__(self):\n",
        "    return len(self.y_test)\n",
        "   \n",
        "  def __getitem__(self,idx):\n",
        "    return self.x_test[idx],self.y_test[idx]"
      ],
      "metadata": {
        "id": "7OP2dCSVVp0x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 64\n",
        "\n",
        "# Create data loaders.\n",
        "train_data=BasketballDatasetTrain()\n",
        "val_data=BasketballDatasetVal()\n",
        "test_data=BasketballDatasetTest()\n",
        "train_dataloader = DataLoader(train_data, batch_size=batch_size)\n",
        "val_dataloader = DataLoader(val_data, batch_size=batch_size)\n",
        "test_dataloader = DataLoader(test_data, batch_size=batch_size)"
      ],
      "metadata": {
        "id": "hxwDswoaelwt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#print some dataloader values to make sure dataloader is \n",
        "#working as intended\n",
        "#We want to make sure  X and y look something like\n",
        "#torch.Size([64, 7]) torch.Size([64])\n",
        "#as we want to pass properly sized \n",
        "#tensors of features and their labels respectively\n",
        "#to the NN.\n",
        "for i, (data, labels) in enumerate(train_dataloader):\n",
        "  print(data.shape, labels.shape)\n",
        "  print(data,labels)\n",
        "  break;"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FWXy2VbThJ8K",
        "outputId": "59335a61-5242-4989-d4a9-6500f3644c8d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([64, 7]) torch.Size([64])\n",
            "tensor([[ 1.2048e-01, -7.7731e-01, -1.3554e+00, -3.8937e-01, -1.6798e+00,\n",
            "          6.5562e-01, -1.4925e-03],\n",
            "        [-5.7456e-01,  1.5967e+00, -8.8191e-01, -1.2713e-01,  3.5348e-01,\n",
            "          3.3649e-01, -1.1443e+00],\n",
            "        [ 1.5840e-01, -1.9424e+00,  2.8172e-01, -2.3640e-01,  5.3785e-02,\n",
            "          4.2363e-01,  8.6114e-01],\n",
            "        [-4.6490e-01, -3.5315e-01,  7.3509e-01,  2.0430e-01,  2.7229e-01,\n",
            "         -2.0007e-02,  5.5208e-02],\n",
            "        [-4.0593e-01,  1.0151e-02,  6.5063e-01,  2.6887e-01,  2.9371e-01,\n",
            "         -8.1381e-02, -1.1535e-01],\n",
            "        [ 1.8204e+00,  8.4499e-01,  6.8611e-01, -9.6564e-01,  6.6358e-01,\n",
            "          1.1081e+00, -4.1717e-01],\n",
            "        [ 4.3017e-01, -5.8069e-01,  9.9256e-01, -2.0796e-01,  4.4710e-01,\n",
            "          4.2461e-01,  1.7105e-01],\n",
            "        [-4.6095e-01,  6.8861e-01,  3.1280e-01,  2.6842e-01,  3.2116e-01,\n",
            "         -7.2513e-02, -4.6061e-01],\n",
            "        [-5.4736e-01,  2.5174e+00, -6.2766e-01,  9.5522e-01,  1.2934e+00,\n",
            "         -3.6368e-01, -1.1492e+00],\n",
            "        [ 3.6471e+00, -4.1235e-01,  7.3831e-01,  2.4946e-01,  2.6968e-01,\n",
            "         -6.8394e-02,  8.2192e-02],\n",
            "        [ 1.2077e+00, -9.4093e-01, -1.4297e+00, -5.4195e-01, -1.7070e+00,\n",
            "          2.4796e+00,  8.6922e-01],\n",
            "        [-6.5027e-01, -8.3216e-01, -2.5930e+00,  5.2851e-01, -8.5897e-01,\n",
            "         -1.4769e+00,  3.7869e+00],\n",
            "        [-9.6675e-01,  5.3043e-01,  3.1603e-01, -6.9784e-02,  2.1482e-01,\n",
            "          2.5824e-01, -4.0157e-01],\n",
            "        [-9.8368e-01, -4.2969e-01, -4.7752e-01,  2.9619e+00,  3.3176e-01,\n",
            "         -3.5349e+00, -1.0452e-01],\n",
            "        [-8.8092e-01,  1.4119e+00, -1.7869e-01, -6.7304e-01,  4.7215e-01,\n",
            "          8.6951e-01, -8.3265e-01],\n",
            "        [-1.9259e-01,  1.7626e-02,  6.8113e-01,  2.2331e-02,  3.0118e-01,\n",
            "          1.7645e-01, -1.1363e-01],\n",
            "        [-9.6355e-01,  4.6032e-02,  6.0870e-01,  3.7724e-01,  2.8214e-01,\n",
            "         -1.9920e-01, -1.3769e-01],\n",
            "        [-7.1208e-01,  6.3987e-01,  3.1486e-01, -3.0007e-01,  2.6374e-01,\n",
            "          5.0654e-01, -4.4259e-01],\n",
            "        [ 9.9457e-01, -5.5677e-01,  7.5444e-01, -4.5540e-01,  2.3375e-01,\n",
            "          6.6972e-01,  1.4909e-01],\n",
            "        [-1.1080e+00, -2.0185e-01,  6.9286e-01,  1.6953e-01,  2.6331e-01,\n",
            "          1.4164e-02, -1.6962e-02],\n",
            "        [ 3.9607e-01, -1.2298e+00,  1.7486e+00, -1.1042e-01,  1.1400e+00,\n",
            "          3.9043e-01,  4.0675e-01],\n",
            "        [-9.2945e-02,  1.3141e-02,  6.4301e-01,  2.8061e-01,  2.8948e-01,\n",
            "         -9.4996e-02, -1.1786e-01],\n",
            "        [-1.1277e+00,  9.2540e-03,  6.2570e-01,  1.2754e-01,  2.5965e-01,\n",
            "          5.7550e-02, -1.1903e-01],\n",
            "        [-9.9764e-01, -8.0669e-01, -1.3880e+00, -2.8000e-01, -1.7047e+00,\n",
            "         -5.8883e-01,  1.0224e-01],\n",
            "        [ 1.3935e+00, -9.8316e-01,  5.9609e-01,  1.0722e+00,  2.5519e-01,\n",
            "         -9.7433e-01,  3.5107e-01],\n",
            "        [-2.2022e-01, -4.7604e-01,  8.3303e-01,  1.5558e-02,  3.2541e-01,\n",
            "          1.8790e-01,  1.1766e-01],\n",
            "        [ 1.9357e+00,  1.0241e+00,  3.7937e-01,  1.6596e+00,  8.0602e-01,\n",
            "         -1.1733e+00, -5.5064e-01],\n",
            "        [-6.8841e-01, -6.2637e-01, -1.5700e+00, -2.5851e-01, -1.7001e+00,\n",
            "         -8.1227e-01, -1.3250e+00],\n",
            "        [ 1.6584e+00, -8.0316e-01,  5.1797e-03,  2.4272e+00,  2.8839e-01,\n",
            "         -2.6881e+00,  2.5461e-01],\n",
            "        [-8.9805e-01,  2.4695e+00, -1.8719e-01, -1.1892e+00,  1.3263e+00,\n",
            "          1.1599e+00, -1.0119e+00],\n",
            "        [-1.0323e+00,  1.1598e+00, -1.9160e-02, -7.3039e-01,  3.7800e-01,\n",
            "          9.3797e-01, -7.1406e-01],\n",
            "        [-7.1937e-01,  5.3252e-01, -3.0608e+00,  4.2511e-01, -4.7296e-02,\n",
            "         -4.0249e-01,  4.6051e+00],\n",
            "        [-5.2290e-01,  4.5149e-01,  2.8230e-01, -2.4651e-04,  1.5437e-01,\n",
            "          1.7158e-01, -3.8000e-01],\n",
            "        [ 2.4498e+00, -1.1159e+00,  2.0722e-02,  2.3496e+00,  2.7715e-01,\n",
            "         -2.5900e+00,  4.6419e-01],\n",
            "        [-1.0940e+00, -4.3671e-02,  6.1221e-01,  4.0298e-01,  2.6097e-01,\n",
            "         -2.3457e-01, -9.8201e-02],\n",
            "        [-1.0837e+00, -7.6612e-01, -1.3655e+00, -3.2350e-01, -1.6831e+00,\n",
            "         -1.6826e-02, -6.1790e-02],\n",
            "        [-6.7518e-03, -4.7455e-01, -7.5200e-01, -2.4227e-01, -1.0718e+00,\n",
            "          2.1004e-01, -1.6718e-01],\n",
            "        [-5.1603e-01,  1.2346e+00, -2.7341e-01, -1.4429e-01,  2.9219e-01,\n",
            "          3.4703e-01, -8.2612e-01],\n",
            "        [-4.1684e-01,  1.8268e-01,  4.3069e-01,  1.3079e+00,  3.6625e-01,\n",
            "         -1.1507e+00, -2.3124e-01],\n",
            "        [-1.0046e+00,  3.3474e-02,  6.1573e-01,  2.8919e-01,  2.7404e-01,\n",
            "         -1.0898e-01, -1.3110e-01],\n",
            "        [ 6.1944e-01, -1.2381e-01,  6.3069e-01,  1.6461e-02,  2.1760e-01,\n",
            "          1.6627e-01, -5.9962e-02],\n",
            "        [-8.5800e-02, -1.7404e-01,  7.0430e-01, -1.2442e-01,  2.6083e-01,\n",
            "          3.2265e-01, -2.7793e-02],\n",
            "        [ 1.6138e+00, -7.5594e-01, -1.3669e+00, -3.0767e-01, -1.6798e+00,\n",
            "         -1.6919e-01, -1.0698e-01],\n",
            "        [ 5.9216e-01, -2.1056e+00,  1.5049e+00, -1.3418e+00,  1.1637e+00,\n",
            "          1.3044e+00,  7.0378e-01],\n",
            "        [-4.5778e-01,  6.7485e-01,  3.0049e-01, -4.5585e-01,  2.7226e-01,\n",
            "          6.6883e-01, -4.5922e-01],\n",
            "        [ 1.0916e+00, -5.2940e-02,  7.6324e-01,  6.9291e-02,  3.5344e-01,\n",
            "          1.3877e-01, -7.1775e-02],\n",
            "        [ 1.8683e+00, -8.9495e-01, -1.2825e-01,  1.9527e+00,  2.3152e-03,\n",
            "         -2.4989e+00,  3.2045e-01],\n",
            "        [ 1.8649e+00, -1.3577e-01,  6.0400e-01, -8.8346e-01,  1.9901e-01,\n",
            "          1.1332e+00, -5.8443e-02],\n",
            "        [ 3.3727e-01, -2.9574e-01,  7.0518e-01,  1.0225e-01,  2.4823e-01,\n",
            "          8.1519e-02,  2.6401e-02],\n",
            "        [-9.5984e-01, -3.9979e-01,  7.0635e-01,  4.7929e-01,  2.7102e-01,\n",
            "         -3.1209e-01,  7.3746e-02],\n",
            "        [-6.0545e-01, -7.4503e-01, -1.3824e+00, -2.7117e-01, -1.6836e+00,\n",
            "         -5.6735e-01, -1.7984e-01],\n",
            "        [ 1.8924e-01, -1.0567e+00,  2.0703e+00, -8.2566e-01,  1.4225e+00,\n",
            "          8.9458e-01,  3.4457e-01],\n",
            "        [ 3.3766e-01, -7.9023e-01, -1.3498e+00, -2.6781e-01, -1.6677e+00,\n",
            "         -5.2073e-01,  5.8782e-02],\n",
            "        [-8.5369e-01, -3.5345e-01,  7.3861e-01,  1.4967e-01,  2.7048e-01,\n",
            "          3.6982e-02,  5.5672e-02],\n",
            "        [ 7.5605e-02,  7.0894e-01,  3.0313e-01, -5.5474e-01,  2.9476e-01,\n",
            "          7.6987e-01, -4.7051e-01],\n",
            "        [-1.3748e-01, -9.8588e-01, -1.1237e+00, -3.6981e-01, -1.4589e+00,\n",
            "          5.2557e-01,  6.0471e-01],\n",
            "        [ 9.2494e-02, -4.2475e-02,  1.1483e+00, -7.7460e-02,  6.7930e-01,\n",
            "          3.2530e-01, -2.8849e-02],\n",
            "        [ 2.7152e-01,  1.0543e+00, -6.7547e-02, -2.4178e-02,  2.7897e-01,\n",
            "          2.2065e-01, -7.0150e-01],\n",
            "        [-3.7494e-01, -2.5238e-01, -1.2268e-01, -3.4568e-01, -4.7508e-01,\n",
            "          5.3394e-01, -1.3495e-01],\n",
            "        [-2.0531e-02,  3.6597e-01,  3.8494e-01,  6.2875e-01,  2.6256e-01,\n",
            "         -4.7705e-01, -3.1878e-01],\n",
            "        [-1.8242e-01, -2.4192e-01,  5.2482e-01,  1.1218e+00,  2.7033e-01,\n",
            "         -1.0191e+00, -2.0233e-02],\n",
            "        [ 4.8069e-02,  6.9953e-02,  5.9726e-01,  3.7544e-01,  2.8026e-01,\n",
            "         -1.9799e-01, -1.4991e-01],\n",
            "        [-1.2282e+00, -7.6499e-01, -1.3716e+00, -3.1176e-01, -1.6867e+00,\n",
            "         -1.5049e-01, -7.5332e-02],\n",
            "        [-4.2999e-01,  7.3166e-01,  2.7996e-01, -4.1837e-01,  2.8898e-01,\n",
            "          6.2959e-01, -4.8512e-01]]) tensor([0., 4., 0., 0., 1., 0., 3., 2., 4., 3., 0., 4., 4., 3., 4., 4., 1., 1.,\n",
            "        0., 4., 3., 4., 2., 4., 3., 4., 0., 4., 3., 4., 4., 4., 4., 3., 4., 0.,\n",
            "        3., 2., 3., 1., 2., 4., 0., 3., 1., 2., 3., 0., 2., 0., 1., 3., 2., 0.,\n",
            "        1., 0., 0., 2., 4., 4., 3., 1., 1., 1.])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Forward Propagation, Cost Function, Gradient Descent"
      ],
      "metadata": {
        "id": "wuO-psZJOmXk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Hyperparameter Study 1: Choice of Activation Function"
      ],
      "metadata": {
        "id": "C0hR4YzDQk_g"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##1A: ReLU"
      ],
      "metadata": {
        "id": "P7_WMmLuQqzQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#check what device we're using\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"Using {device} device\")\n",
        "torch.set_grad_enabled(True) "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NuAY05a-PkwK",
        "outputId": "34b45962-3c9c-4428-f456-f534ce18059e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using cpu device\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch.autograd.grad_mode.set_grad_enabled at 0x7f260f35a0d0>"
            ]
          },
          "metadata": {},
          "execution_count": 80
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class NeuralNetwork(nn.Module):\n",
        "    def __init__(self):\n",
        "        #super(NeuralNetwork, self).__init__()\n",
        "        super().__init__()\n",
        "        self.linear_relu_stack = nn.Sequential(\n",
        "            nn.Linear(7, 256), #input layer \n",
        "            nn.ReLU(), \n",
        "            nn.Linear(256, 256), #hidden layer 1\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(256, 256), #hidden layer 2\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(256, 5), #output layer\n",
        "        )\n",
        "    # \"Every nn.Module subclass implements the operations \n",
        "    # on input data in the forward method.\"\n",
        "    # We already preprocessed the input data\n",
        "    # so we just return x\n",
        "    def forward(self, x): \n",
        "        logits = self.linear_relu_stack(x) #we already preprocessed the data, so we just pass it into the NN here\n",
        "        return logits"
      ],
      "metadata": {
        "id": "gW3TICH0KE-W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Implement forward propagation (clearly describe the activation functions and other\n",
        "hyper-parameters you are using)."
      ],
      "metadata": {
        "id": "qiYSGDU7LIQn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Make sure nn was implmented as intended\n",
        "model = NeuralNetwork().to(device)\n",
        "print(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H6260c6Xskx3",
        "outputId": "2b761c09-0ac6-46e5-fc7d-6bbc5c96d78e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NeuralNetwork(\n",
            "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
            "  (linear_relu_stack): Sequential(\n",
            "    (0): Linear(in_features=7, out_features=256, bias=True)\n",
            "    (1): ReLU()\n",
            "    (2): Linear(in_features=256, out_features=256, bias=True)\n",
            "    (3): ReLU()\n",
            "    (4): Linear(in_features=256, out_features=256, bias=True)\n",
            "    (5): ReLU()\n",
            "    (6): Linear(in_features=256, out_features=5, bias=True)\n",
            "  )\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "4. Compute the final cost function.\n",
        "\n",
        "5. Implement gradient descent (any variant of gradient descent depending upon your\n",
        "data and project can be used) to train your model. In this step it is up to you as someone\n",
        "in charge of their project to improvise using optimization algorithms (Adams, RMSProp\n",
        "etc.) and/or regularization."
      ],
      "metadata": {
        "id": "deTZVhb4Qz51"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "loss_fn = nn.CrossEntropyLoss()\n",
        "#Opted for regular SGD \n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=1e-2) "
      ],
      "metadata": {
        "id": "MLCzrUAJq2oF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training/testing"
      ],
      "metadata": {
        "id": "yzS4B2i0rdtG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Code snippet attribution:\n",
        "#https://pytorch.org/tutorials/beginner/basics/intro.html\n",
        "#was used as part of the research process\n",
        "#Modifications were made so that data could be processed as longs,\n",
        "#additional statistics could be reported, \n",
        "#and a separate validate process was created. \n",
        "\n",
        "def train(dataloader, model, loss_fn, optimizer):\n",
        "    size = len(dataloader.dataset)\n",
        "    model.train()\n",
        "    for batch, (X, y) in enumerate(dataloader):\n",
        "        X, y = X.to(device), y.to(device)\n",
        "\n",
        "        # Compute prediction error\n",
        "        pred = model(X)\n",
        "        loss = loss_fn(pred, y.long()) #loss function requires longs\n",
        "\n",
        "        # Backpropagation\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        #When all batches complete\n",
        "        if batch % 100 == 0:\n",
        "            #calculate loss, \"correctness\" (accuracy)\n",
        "            loss, current = loss.item(), batch * len(X)\n",
        "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
        "            correct = 0\n",
        "            with torch.no_grad():\n",
        "                for X, y in dataloader:\n",
        "                    X, y = X.to(device), y.to(device)\n",
        "                    pred = model(X)\n",
        "                    correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
        "            correct /= size\n",
        "            print(f\"Train Error: Accuracy: {(100*correct):>0.1f}%\")\n",
        "\n",
        "\n",
        "#validation\n",
        "def validate(dataloader, model, loss_fn):\n",
        "    size = len(dataloader.dataset)\n",
        "    num_batches = len(dataloader)\n",
        "    model.eval()\n",
        "    test_loss, correct = 0, 0\n",
        "    with torch.no_grad():\n",
        "        for X, y in dataloader:\n",
        "            X, y = X.to(device), y.to(device)\n",
        "            pred = model(X)\n",
        "            test_loss += loss_fn(pred, y.long()).item()\n",
        "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
        "    test_loss /= num_batches\n",
        "    correct /= size\n",
        "    print(f\"Val Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n",
        "    return correct\n",
        "\n",
        "#test - only use when training/validation are over\n",
        "def test(dataloader, model, loss_fn):\n",
        "    size = len(dataloader.dataset)\n",
        "    num_batches = len(dataloader)\n",
        "    model.eval()\n",
        "    test_loss, correct = 0, 0\n",
        "    with torch.no_grad():\n",
        "        for X, y in dataloader:\n",
        "            X, y = X.to(device), y.to(device)\n",
        "            pred = model(X)\n",
        "            test_loss += loss_fn(pred, y.long()).item()\n",
        "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
        "    test_loss /= num_batches\n",
        "    correct /= size\n",
        "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"
      ],
      "metadata": {
        "id": "kX_8hd_33QsN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Measure validation, and stop the NN if learning has stopped\n",
        "#increasing in validation\n",
        "epochs = 100\n",
        "bestval = 0\n",
        "decval = 0\n",
        "for t in range(epochs):\n",
        "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
        "    train(train_dataloader, model, loss_fn, optimizer)\n",
        "    tempval = validate(val_dataloader, model, loss_fn)\n",
        "    if(tempval < bestval):\n",
        "      decval+=1 \n",
        "      if(decval==3):\n",
        "        print(\"Validation no longer rising. Done!\")\n",
        "        break\n",
        "    else:\n",
        "      bestval=tempval\n",
        "print(\"Max number of epochs hit or validation stop reached. Done!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vGXq0Cmg3Vte",
        "outputId": "ada83565-2e31-4cc0-dffe-605b0baeacb9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1\n",
            "-------------------------------\n",
            "loss: 1.610569  [    0/20380]\n",
            "Train Error: Accuracy: 20.4%\n",
            "loss: 1.592173  [ 6400/20380]\n",
            "Train Error: Accuracy: 37.9%\n",
            "loss: 1.528603  [12800/20380]\n",
            "Train Error: Accuracy: 39.3%\n",
            "loss: 1.374272  [19200/20380]\n",
            "Train Error: Accuracy: 40.4%\n",
            "Val Error: \n",
            " Accuracy: 40.5%, Avg loss: 1.447697 \n",
            "\n",
            "Epoch 2\n",
            "-------------------------------\n",
            "loss: 1.424915  [    0/20380]\n",
            "Train Error: Accuracy: 40.3%\n",
            "loss: 1.475593  [ 6400/20380]\n",
            "Train Error: Accuracy: 42.8%\n",
            "loss: 1.370492  [12800/20380]\n",
            "Train Error: Accuracy: 47.3%\n",
            "loss: 1.144742  [19200/20380]\n",
            "Train Error: Accuracy: 47.5%\n",
            "Val Error: \n",
            " Accuracy: 48.2%, Avg loss: 1.291887 \n",
            "\n",
            "Epoch 3\n",
            "-------------------------------\n",
            "loss: 1.259045  [    0/20380]\n",
            "Train Error: Accuracy: 47.8%\n",
            "loss: 1.345087  [ 6400/20380]\n",
            "Train Error: Accuracy: 48.5%\n",
            "loss: 1.261577  [12800/20380]\n",
            "Train Error: Accuracy: 48.4%\n",
            "loss: 1.007090  [19200/20380]\n",
            "Train Error: Accuracy: 49.7%\n",
            "Val Error: \n",
            " Accuracy: 50.0%, Avg loss: 1.198367 \n",
            "\n",
            "Epoch 4\n",
            "-------------------------------\n",
            "loss: 1.169803  [    0/20380]\n",
            "Train Error: Accuracy: 49.8%\n",
            "loss: 1.249216  [ 6400/20380]\n",
            "Train Error: Accuracy: 49.9%\n",
            "loss: 1.187233  [12800/20380]\n",
            "Train Error: Accuracy: 50.4%\n",
            "loss: 0.926598  [19200/20380]\n",
            "Train Error: Accuracy: 49.9%\n",
            "Val Error: \n",
            " Accuracy: 50.3%, Avg loss: 1.129204 \n",
            "\n",
            "Epoch 5\n",
            "-------------------------------\n",
            "loss: 1.113905  [    0/20380]\n",
            "Train Error: Accuracy: 50.0%\n",
            "loss: 1.163642  [ 6400/20380]\n",
            "Train Error: Accuracy: 50.8%\n",
            "loss: 1.130026  [12800/20380]\n",
            "Train Error: Accuracy: 50.9%\n",
            "loss: 0.879531  [19200/20380]\n",
            "Train Error: Accuracy: 50.8%\n",
            "Val Error: \n",
            " Accuracy: 50.4%, Avg loss: 1.072181 \n",
            "\n",
            "Epoch 6\n",
            "-------------------------------\n",
            "loss: 1.072884  [    0/20380]\n",
            "Train Error: Accuracy: 50.4%\n",
            "loss: 1.092278  [ 6400/20380]\n",
            "Train Error: Accuracy: 52.7%\n",
            "loss: 1.081138  [12800/20380]\n",
            "Train Error: Accuracy: 52.8%\n",
            "loss: 0.847495  [19200/20380]\n",
            "Train Error: Accuracy: 52.4%\n",
            "Val Error: \n",
            " Accuracy: 52.7%, Avg loss: 1.028036 \n",
            "\n",
            "Epoch 7\n",
            "-------------------------------\n",
            "loss: 1.041343  [    0/20380]\n",
            "Train Error: Accuracy: 52.3%\n",
            "loss: 1.033813  [ 6400/20380]\n",
            "Train Error: Accuracy: 54.5%\n",
            "loss: 1.034344  [12800/20380]\n",
            "Train Error: Accuracy: 54.8%\n",
            "loss: 0.820210  [19200/20380]\n",
            "Train Error: Accuracy: 54.5%\n",
            "Val Error: \n",
            " Accuracy: 55.4%, Avg loss: 0.990252 \n",
            "\n",
            "Epoch 8\n",
            "-------------------------------\n",
            "loss: 1.011196  [    0/20380]\n",
            "Train Error: Accuracy: 54.6%\n",
            "loss: 0.979492  [ 6400/20380]\n",
            "Train Error: Accuracy: 56.4%\n",
            "loss: 0.987017  [12800/20380]\n",
            "Train Error: Accuracy: 56.8%\n",
            "loss: 0.793722  [19200/20380]\n",
            "Train Error: Accuracy: 56.7%\n",
            "Val Error: \n",
            " Accuracy: 58.0%, Avg loss: 0.954907 \n",
            "\n",
            "Epoch 9\n",
            "-------------------------------\n",
            "loss: 0.979970  [    0/20380]\n",
            "Train Error: Accuracy: 56.8%\n",
            "loss: 0.927261  [ 6400/20380]\n",
            "Train Error: Accuracy: 58.0%\n",
            "loss: 0.938085  [12800/20380]\n",
            "Train Error: Accuracy: 58.9%\n",
            "loss: 0.767783  [19200/20380]\n",
            "Train Error: Accuracy: 59.5%\n",
            "Val Error: \n",
            " Accuracy: 60.8%, Avg loss: 0.920350 \n",
            "\n",
            "Epoch 10\n",
            "-------------------------------\n",
            "loss: 0.948222  [    0/20380]\n",
            "Train Error: Accuracy: 60.1%\n",
            "loss: 0.876984  [ 6400/20380]\n",
            "Train Error: Accuracy: 59.9%\n",
            "loss: 0.889511  [12800/20380]\n",
            "Train Error: Accuracy: 61.6%\n",
            "loss: 0.742176  [19200/20380]\n",
            "Train Error: Accuracy: 62.1%\n",
            "Val Error: \n",
            " Accuracy: 63.2%, Avg loss: 0.886794 \n",
            "\n",
            "Epoch 11\n",
            "-------------------------------\n",
            "loss: 0.917070  [    0/20380]\n",
            "Train Error: Accuracy: 62.6%\n",
            "loss: 0.830323  [ 6400/20380]\n",
            "Train Error: Accuracy: 62.2%\n",
            "loss: 0.843421  [12800/20380]\n",
            "Train Error: Accuracy: 64.1%\n",
            "loss: 0.716613  [19200/20380]\n",
            "Train Error: Accuracy: 64.4%\n",
            "Val Error: \n",
            " Accuracy: 65.2%, Avg loss: 0.855011 \n",
            "\n",
            "Epoch 12\n",
            "-------------------------------\n",
            "loss: 0.886717  [    0/20380]\n",
            "Train Error: Accuracy: 64.8%\n",
            "loss: 0.787382  [ 6400/20380]\n",
            "Train Error: Accuracy: 64.4%\n",
            "loss: 0.800675  [12800/20380]\n",
            "Train Error: Accuracy: 65.8%\n",
            "loss: 0.692896  [19200/20380]\n",
            "Train Error: Accuracy: 66.1%\n",
            "Val Error: \n",
            " Accuracy: 66.6%, Avg loss: 0.825120 \n",
            "\n",
            "Epoch 13\n",
            "-------------------------------\n",
            "loss: 0.858816  [    0/20380]\n",
            "Train Error: Accuracy: 66.6%\n",
            "loss: 0.749708  [ 6400/20380]\n",
            "Train Error: Accuracy: 66.0%\n",
            "loss: 0.762062  [12800/20380]\n",
            "Train Error: Accuracy: 67.2%\n",
            "loss: 0.671005  [19200/20380]\n",
            "Train Error: Accuracy: 67.7%\n",
            "Val Error: \n",
            " Accuracy: 68.3%, Avg loss: 0.797644 \n",
            "\n",
            "Epoch 14\n",
            "-------------------------------\n",
            "loss: 0.834032  [    0/20380]\n",
            "Train Error: Accuracy: 68.5%\n",
            "loss: 0.715608  [ 6400/20380]\n",
            "Train Error: Accuracy: 67.5%\n",
            "loss: 0.728946  [12800/20380]\n",
            "Train Error: Accuracy: 68.6%\n",
            "loss: 0.650825  [19200/20380]\n",
            "Train Error: Accuracy: 69.5%\n",
            "Val Error: \n",
            " Accuracy: 69.8%, Avg loss: 0.772215 \n",
            "\n",
            "Epoch 15\n",
            "-------------------------------\n",
            "loss: 0.811736  [    0/20380]\n",
            "Train Error: Accuracy: 70.0%\n",
            "loss: 0.685100  [ 6400/20380]\n",
            "Train Error: Accuracy: 69.2%\n",
            "loss: 0.700261  [12800/20380]\n",
            "Train Error: Accuracy: 70.3%\n",
            "loss: 0.633329  [19200/20380]\n",
            "Train Error: Accuracy: 71.0%\n",
            "Val Error: \n",
            " Accuracy: 70.9%, Avg loss: 0.749073 \n",
            "\n",
            "Epoch 16\n",
            "-------------------------------\n",
            "loss: 0.791187  [    0/20380]\n",
            "Train Error: Accuracy: 71.2%\n",
            "loss: 0.656643  [ 6400/20380]\n",
            "Train Error: Accuracy: 70.6%\n",
            "loss: 0.675182  [12800/20380]\n",
            "Train Error: Accuracy: 71.7%\n",
            "loss: 0.617987  [19200/20380]\n",
            "Train Error: Accuracy: 72.2%\n",
            "Val Error: \n",
            " Accuracy: 72.0%, Avg loss: 0.727840 \n",
            "\n",
            "Epoch 17\n",
            "-------------------------------\n",
            "loss: 0.772724  [    0/20380]\n",
            "Train Error: Accuracy: 72.3%\n",
            "loss: 0.631224  [ 6400/20380]\n",
            "Train Error: Accuracy: 71.8%\n",
            "loss: 0.651748  [12800/20380]\n",
            "Train Error: Accuracy: 72.8%\n",
            "loss: 0.605330  [19200/20380]\n",
            "Train Error: Accuracy: 73.2%\n",
            "Val Error: \n",
            " Accuracy: 72.7%, Avg loss: 0.708413 \n",
            "\n",
            "Epoch 18\n",
            "-------------------------------\n",
            "loss: 0.756686  [    0/20380]\n",
            "Train Error: Accuracy: 73.6%\n",
            "loss: 0.607753  [ 6400/20380]\n",
            "Train Error: Accuracy: 72.7%\n",
            "loss: 0.630802  [12800/20380]\n",
            "Train Error: Accuracy: 73.6%\n",
            "loss: 0.594021  [19200/20380]\n",
            "Train Error: Accuracy: 74.2%\n",
            "Val Error: \n",
            " Accuracy: 73.5%, Avg loss: 0.690715 \n",
            "\n",
            "Epoch 19\n",
            "-------------------------------\n",
            "loss: 0.742085  [    0/20380]\n",
            "Train Error: Accuracy: 74.3%\n",
            "loss: 0.586536  [ 6400/20380]\n",
            "Train Error: Accuracy: 73.8%\n",
            "loss: 0.611550  [12800/20380]\n",
            "Train Error: Accuracy: 74.7%\n",
            "loss: 0.583561  [19200/20380]\n",
            "Train Error: Accuracy: 75.1%\n",
            "Val Error: \n",
            " Accuracy: 74.4%, Avg loss: 0.674374 \n",
            "\n",
            "Epoch 20\n",
            "-------------------------------\n",
            "loss: 0.727923  [    0/20380]\n",
            "Train Error: Accuracy: 75.2%\n",
            "loss: 0.567312  [ 6400/20380]\n",
            "Train Error: Accuracy: 74.7%\n",
            "loss: 0.594009  [12800/20380]\n",
            "Train Error: Accuracy: 75.5%\n",
            "loss: 0.574300  [19200/20380]\n",
            "Train Error: Accuracy: 75.8%\n",
            "Val Error: \n",
            " Accuracy: 75.2%, Avg loss: 0.659339 \n",
            "\n",
            "Epoch 21\n",
            "-------------------------------\n",
            "loss: 0.715049  [    0/20380]\n",
            "Train Error: Accuracy: 75.9%\n",
            "loss: 0.550009  [ 6400/20380]\n",
            "Train Error: Accuracy: 75.5%\n",
            "loss: 0.577918  [12800/20380]\n",
            "Train Error: Accuracy: 76.2%\n",
            "loss: 0.566522  [19200/20380]\n",
            "Train Error: Accuracy: 76.5%\n",
            "Val Error: \n",
            " Accuracy: 75.9%, Avg loss: 0.645644 \n",
            "\n",
            "Epoch 22\n",
            "-------------------------------\n",
            "loss: 0.704553  [    0/20380]\n",
            "Train Error: Accuracy: 76.5%\n",
            "loss: 0.533993  [ 6400/20380]\n",
            "Train Error: Accuracy: 76.3%\n",
            "loss: 0.562731  [12800/20380]\n",
            "Train Error: Accuracy: 77.2%\n",
            "loss: 0.559328  [19200/20380]\n",
            "Train Error: Accuracy: 77.3%\n",
            "Val Error: \n",
            " Accuracy: 76.4%, Avg loss: 0.632887 \n",
            "\n",
            "Epoch 23\n",
            "-------------------------------\n",
            "loss: 0.695027  [    0/20380]\n",
            "Train Error: Accuracy: 77.2%\n",
            "loss: 0.519204  [ 6400/20380]\n",
            "Train Error: Accuracy: 77.2%\n",
            "loss: 0.547754  [12800/20380]\n",
            "Train Error: Accuracy: 78.0%\n",
            "loss: 0.553420  [19200/20380]\n",
            "Train Error: Accuracy: 78.0%\n",
            "Val Error: \n",
            " Accuracy: 76.9%, Avg loss: 0.620882 \n",
            "\n",
            "Epoch 24\n",
            "-------------------------------\n",
            "loss: 0.686377  [    0/20380]\n",
            "Train Error: Accuracy: 77.8%\n",
            "loss: 0.505510  [ 6400/20380]\n",
            "Train Error: Accuracy: 77.8%\n",
            "loss: 0.533330  [12800/20380]\n",
            "Train Error: Accuracy: 78.5%\n",
            "loss: 0.547426  [19200/20380]\n",
            "Train Error: Accuracy: 78.5%\n",
            "Val Error: \n",
            " Accuracy: 77.7%, Avg loss: 0.609866 \n",
            "\n",
            "Epoch 25\n",
            "-------------------------------\n",
            "loss: 0.678541  [    0/20380]\n",
            "Train Error: Accuracy: 78.5%\n",
            "loss: 0.493329  [ 6400/20380]\n",
            "Train Error: Accuracy: 78.4%\n",
            "loss: 0.520097  [12800/20380]\n",
            "Train Error: Accuracy: 79.0%\n",
            "loss: 0.542545  [19200/20380]\n",
            "Train Error: Accuracy: 79.2%\n",
            "Val Error: \n",
            " Accuracy: 78.3%, Avg loss: 0.599564 \n",
            "\n",
            "Epoch 26\n",
            "-------------------------------\n",
            "loss: 0.670524  [    0/20380]\n",
            "Train Error: Accuracy: 79.0%\n",
            "loss: 0.481992  [ 6400/20380]\n",
            "Train Error: Accuracy: 78.8%\n",
            "loss: 0.507496  [12800/20380]\n",
            "Train Error: Accuracy: 79.4%\n",
            "loss: 0.538421  [19200/20380]\n",
            "Train Error: Accuracy: 79.6%\n",
            "Val Error: \n",
            " Accuracy: 78.9%, Avg loss: 0.589913 \n",
            "\n",
            "Epoch 27\n",
            "-------------------------------\n",
            "loss: 0.662180  [    0/20380]\n",
            "Train Error: Accuracy: 79.4%\n",
            "loss: 0.471830  [ 6400/20380]\n",
            "Train Error: Accuracy: 79.3%\n",
            "loss: 0.495460  [12800/20380]\n",
            "Train Error: Accuracy: 79.8%\n",
            "loss: 0.533263  [19200/20380]\n",
            "Train Error: Accuracy: 80.2%\n",
            "Val Error: \n",
            " Accuracy: 79.5%, Avg loss: 0.580452 \n",
            "\n",
            "Epoch 28\n",
            "-------------------------------\n",
            "loss: 0.654522  [    0/20380]\n",
            "Train Error: Accuracy: 79.9%\n",
            "loss: 0.462662  [ 6400/20380]\n",
            "Train Error: Accuracy: 79.5%\n",
            "loss: 0.484938  [12800/20380]\n",
            "Train Error: Accuracy: 80.2%\n",
            "loss: 0.528370  [19200/20380]\n",
            "Train Error: Accuracy: 80.6%\n",
            "Val Error: \n",
            " Accuracy: 80.0%, Avg loss: 0.571641 \n",
            "\n",
            "Epoch 29\n",
            "-------------------------------\n",
            "loss: 0.647453  [    0/20380]\n",
            "Train Error: Accuracy: 80.3%\n",
            "loss: 0.454422  [ 6400/20380]\n",
            "Train Error: Accuracy: 79.8%\n",
            "loss: 0.474951  [12800/20380]\n",
            "Train Error: Accuracy: 80.5%\n",
            "loss: 0.523086  [19200/20380]\n",
            "Train Error: Accuracy: 80.8%\n",
            "Val Error: \n",
            " Accuracy: 80.4%, Avg loss: 0.563513 \n",
            "\n",
            "Epoch 30\n",
            "-------------------------------\n",
            "loss: 0.640426  [    0/20380]\n",
            "Train Error: Accuracy: 80.7%\n",
            "loss: 0.446087  [ 6400/20380]\n",
            "Train Error: Accuracy: 80.2%\n",
            "loss: 0.465646  [12800/20380]\n",
            "Train Error: Accuracy: 80.9%\n",
            "loss: 0.520130  [19200/20380]\n",
            "Train Error: Accuracy: 81.2%\n",
            "Val Error: \n",
            " Accuracy: 80.7%, Avg loss: 0.556010 \n",
            "\n",
            "Epoch 31\n",
            "-------------------------------\n",
            "loss: 0.633363  [    0/20380]\n",
            "Train Error: Accuracy: 81.0%\n",
            "loss: 0.437810  [ 6400/20380]\n",
            "Train Error: Accuracy: 80.4%\n",
            "loss: 0.457499  [12800/20380]\n",
            "Train Error: Accuracy: 81.0%\n",
            "loss: 0.516071  [19200/20380]\n",
            "Train Error: Accuracy: 81.5%\n",
            "Val Error: \n",
            " Accuracy: 80.9%, Avg loss: 0.549437 \n",
            "\n",
            "Epoch 32\n",
            "-------------------------------\n",
            "loss: 0.627914  [    0/20380]\n",
            "Train Error: Accuracy: 81.2%\n",
            "loss: 0.430882  [ 6400/20380]\n",
            "Train Error: Accuracy: 80.7%\n",
            "loss: 0.450108  [12800/20380]\n",
            "Train Error: Accuracy: 81.3%\n",
            "loss: 0.511065  [19200/20380]\n",
            "Train Error: Accuracy: 81.8%\n",
            "Val Error: \n",
            " Accuracy: 81.4%, Avg loss: 0.542495 \n",
            "\n",
            "Epoch 33\n",
            "-------------------------------\n",
            "loss: 0.621495  [    0/20380]\n",
            "Train Error: Accuracy: 81.4%\n",
            "loss: 0.423994  [ 6400/20380]\n",
            "Train Error: Accuracy: 81.0%\n",
            "loss: 0.443609  [12800/20380]\n",
            "Train Error: Accuracy: 81.5%\n",
            "loss: 0.506503  [19200/20380]\n",
            "Train Error: Accuracy: 82.2%\n",
            "Val Error: \n",
            " Accuracy: 81.6%, Avg loss: 0.535811 \n",
            "\n",
            "Epoch 34\n",
            "-------------------------------\n",
            "loss: 0.615129  [    0/20380]\n",
            "Train Error: Accuracy: 81.7%\n",
            "loss: 0.417980  [ 6400/20380]\n",
            "Train Error: Accuracy: 81.3%\n",
            "loss: 0.436948  [12800/20380]\n",
            "Train Error: Accuracy: 81.7%\n",
            "loss: 0.500716  [19200/20380]\n",
            "Train Error: Accuracy: 82.6%\n",
            "Val Error: \n",
            " Accuracy: 81.9%, Avg loss: 0.529953 \n",
            "\n",
            "Epoch 35\n",
            "-------------------------------\n",
            "loss: 0.609868  [    0/20380]\n",
            "Train Error: Accuracy: 82.0%\n",
            "loss: 0.412248  [ 6400/20380]\n",
            "Train Error: Accuracy: 81.6%\n",
            "loss: 0.431364  [12800/20380]\n",
            "Train Error: Accuracy: 82.0%\n",
            "loss: 0.494484  [19200/20380]\n",
            "Train Error: Accuracy: 82.8%\n",
            "Val Error: \n",
            " Accuracy: 82.3%, Avg loss: 0.524066 \n",
            "\n",
            "Epoch 36\n",
            "-------------------------------\n",
            "loss: 0.604004  [    0/20380]\n",
            "Train Error: Accuracy: 82.2%\n",
            "loss: 0.407106  [ 6400/20380]\n",
            "Train Error: Accuracy: 81.7%\n",
            "loss: 0.425135  [12800/20380]\n",
            "Train Error: Accuracy: 82.3%\n",
            "loss: 0.488179  [19200/20380]\n",
            "Train Error: Accuracy: 83.1%\n",
            "Val Error: \n",
            " Accuracy: 82.4%, Avg loss: 0.518038 \n",
            "\n",
            "Epoch 37\n",
            "-------------------------------\n",
            "loss: 0.598671  [    0/20380]\n",
            "Train Error: Accuracy: 82.5%\n",
            "loss: 0.401309  [ 6400/20380]\n",
            "Train Error: Accuracy: 82.0%\n",
            "loss: 0.419699  [12800/20380]\n",
            "Train Error: Accuracy: 82.4%\n",
            "loss: 0.482577  [19200/20380]\n",
            "Train Error: Accuracy: 83.4%\n",
            "Val Error: \n",
            " Accuracy: 82.6%, Avg loss: 0.513027 \n",
            "\n",
            "Epoch 38\n",
            "-------------------------------\n",
            "loss: 0.593275  [    0/20380]\n",
            "Train Error: Accuracy: 82.8%\n",
            "loss: 0.396579  [ 6400/20380]\n",
            "Train Error: Accuracy: 82.1%\n",
            "loss: 0.414424  [12800/20380]\n",
            "Train Error: Accuracy: 82.5%\n",
            "loss: 0.475808  [19200/20380]\n",
            "Train Error: Accuracy: 83.5%\n",
            "Val Error: \n",
            " Accuracy: 82.8%, Avg loss: 0.508581 \n",
            "\n",
            "Epoch 39\n",
            "-------------------------------\n",
            "loss: 0.589927  [    0/20380]\n",
            "Train Error: Accuracy: 83.0%\n",
            "loss: 0.391884  [ 6400/20380]\n",
            "Train Error: Accuracy: 82.3%\n",
            "loss: 0.409696  [12800/20380]\n",
            "Train Error: Accuracy: 82.7%\n",
            "loss: 0.469433  [19200/20380]\n",
            "Train Error: Accuracy: 83.7%\n",
            "Val Error: \n",
            " Accuracy: 83.0%, Avg loss: 0.503453 \n",
            "\n",
            "Epoch 40\n",
            "-------------------------------\n",
            "loss: 0.586265  [    0/20380]\n",
            "Train Error: Accuracy: 83.2%\n",
            "loss: 0.387320  [ 6400/20380]\n",
            "Train Error: Accuracy: 82.5%\n",
            "loss: 0.405457  [12800/20380]\n",
            "Train Error: Accuracy: 82.9%\n",
            "loss: 0.462004  [19200/20380]\n",
            "Train Error: Accuracy: 83.7%\n",
            "Val Error: \n",
            " Accuracy: 83.2%, Avg loss: 0.498984 \n",
            "\n",
            "Epoch 41\n",
            "-------------------------------\n",
            "loss: 0.580855  [    0/20380]\n",
            "Train Error: Accuracy: 83.3%\n",
            "loss: 0.382484  [ 6400/20380]\n",
            "Train Error: Accuracy: 82.6%\n",
            "loss: 0.401080  [12800/20380]\n",
            "Train Error: Accuracy: 83.1%\n",
            "loss: 0.455125  [19200/20380]\n",
            "Train Error: Accuracy: 83.9%\n",
            "Val Error: \n",
            " Accuracy: 83.3%, Avg loss: 0.494791 \n",
            "\n",
            "Epoch 42\n",
            "-------------------------------\n",
            "loss: 0.576517  [    0/20380]\n",
            "Train Error: Accuracy: 83.5%\n",
            "loss: 0.378245  [ 6400/20380]\n",
            "Train Error: Accuracy: 82.7%\n",
            "loss: 0.396118  [12800/20380]\n",
            "Train Error: Accuracy: 83.3%\n",
            "loss: 0.449253  [19200/20380]\n",
            "Train Error: Accuracy: 84.0%\n",
            "Val Error: \n",
            " Accuracy: 83.4%, Avg loss: 0.491541 \n",
            "\n",
            "Epoch 43\n",
            "-------------------------------\n",
            "loss: 0.572142  [    0/20380]\n",
            "Train Error: Accuracy: 83.7%\n",
            "loss: 0.374212  [ 6400/20380]\n",
            "Train Error: Accuracy: 82.8%\n",
            "loss: 0.391623  [12800/20380]\n",
            "Train Error: Accuracy: 83.5%\n",
            "loss: 0.443248  [19200/20380]\n",
            "Train Error: Accuracy: 84.3%\n",
            "Val Error: \n",
            " Accuracy: 83.5%, Avg loss: 0.487583 \n",
            "\n",
            "Epoch 44\n",
            "-------------------------------\n",
            "loss: 0.567444  [    0/20380]\n",
            "Train Error: Accuracy: 83.8%\n",
            "loss: 0.371017  [ 6400/20380]\n",
            "Train Error: Accuracy: 83.0%\n",
            "loss: 0.387337  [12800/20380]\n",
            "Train Error: Accuracy: 83.5%\n",
            "loss: 0.437151  [19200/20380]\n",
            "Train Error: Accuracy: 84.4%\n",
            "Val Error: \n",
            " Accuracy: 83.4%, Avg loss: 0.484278 \n",
            "\n",
            "Epoch 45\n",
            "-------------------------------\n",
            "loss: 0.562793  [    0/20380]\n",
            "Train Error: Accuracy: 84.0%\n",
            "loss: 0.368165  [ 6400/20380]\n",
            "Train Error: Accuracy: 83.2%\n",
            "loss: 0.383075  [12800/20380]\n",
            "Train Error: Accuracy: 83.7%\n",
            "loss: 0.430484  [19200/20380]\n",
            "Train Error: Accuracy: 84.6%\n",
            "Val Error: \n",
            " Accuracy: 83.4%, Avg loss: 0.481327 \n",
            "\n",
            "Epoch 46\n",
            "-------------------------------\n",
            "loss: 0.559730  [    0/20380]\n",
            "Train Error: Accuracy: 84.1%\n",
            "loss: 0.364883  [ 6400/20380]\n",
            "Train Error: Accuracy: 83.3%\n",
            "loss: 0.379115  [12800/20380]\n",
            "Train Error: Accuracy: 83.9%\n",
            "loss: 0.424549  [19200/20380]\n",
            "Train Error: Accuracy: 84.7%\n",
            "Val Error: \n",
            " Accuracy: 83.6%, Avg loss: 0.478324 \n",
            "\n",
            "Epoch 47\n",
            "-------------------------------\n",
            "loss: 0.556358  [    0/20380]\n",
            "Train Error: Accuracy: 84.3%\n",
            "loss: 0.361932  [ 6400/20380]\n",
            "Train Error: Accuracy: 83.5%\n",
            "loss: 0.374550  [12800/20380]\n",
            "Train Error: Accuracy: 84.1%\n",
            "loss: 0.418395  [19200/20380]\n",
            "Train Error: Accuracy: 84.9%\n",
            "Val Error: \n",
            " Accuracy: 83.7%, Avg loss: 0.475906 \n",
            "\n",
            "Epoch 48\n",
            "-------------------------------\n",
            "loss: 0.553155  [    0/20380]\n",
            "Train Error: Accuracy: 84.4%\n",
            "loss: 0.358028  [ 6400/20380]\n",
            "Train Error: Accuracy: 83.7%\n",
            "loss: 0.370842  [12800/20380]\n",
            "Train Error: Accuracy: 84.2%\n",
            "loss: 0.412284  [19200/20380]\n",
            "Train Error: Accuracy: 85.0%\n",
            "Val Error: \n",
            " Accuracy: 83.9%, Avg loss: 0.472886 \n",
            "\n",
            "Epoch 49\n",
            "-------------------------------\n",
            "loss: 0.550356  [    0/20380]\n",
            "Train Error: Accuracy: 84.5%\n",
            "loss: 0.354932  [ 6400/20380]\n",
            "Train Error: Accuracy: 83.9%\n",
            "loss: 0.367150  [12800/20380]\n",
            "Train Error: Accuracy: 84.4%\n",
            "loss: 0.406299  [19200/20380]\n",
            "Train Error: Accuracy: 85.2%\n",
            "Val Error: \n",
            " Accuracy: 83.9%, Avg loss: 0.470620 \n",
            "\n",
            "Epoch 50\n",
            "-------------------------------\n",
            "loss: 0.548557  [    0/20380]\n",
            "Train Error: Accuracy: 84.6%\n",
            "loss: 0.351699  [ 6400/20380]\n",
            "Train Error: Accuracy: 84.1%\n",
            "loss: 0.362877  [12800/20380]\n",
            "Train Error: Accuracy: 84.6%\n",
            "loss: 0.400353  [19200/20380]\n",
            "Train Error: Accuracy: 85.3%\n",
            "Val Error: \n",
            " Accuracy: 84.1%, Avg loss: 0.467986 \n",
            "\n",
            "Epoch 51\n",
            "-------------------------------\n",
            "loss: 0.547261  [    0/20380]\n",
            "Train Error: Accuracy: 84.8%\n",
            "loss: 0.348810  [ 6400/20380]\n",
            "Train Error: Accuracy: 84.2%\n",
            "loss: 0.358904  [12800/20380]\n",
            "Train Error: Accuracy: 84.8%\n",
            "loss: 0.394405  [19200/20380]\n",
            "Train Error: Accuracy: 85.4%\n",
            "Val Error: \n",
            " Accuracy: 84.1%, Avg loss: 0.465706 \n",
            "\n",
            "Epoch 52\n",
            "-------------------------------\n",
            "loss: 0.545816  [    0/20380]\n",
            "Train Error: Accuracy: 84.9%\n",
            "loss: 0.345630  [ 6400/20380]\n",
            "Train Error: Accuracy: 84.3%\n",
            "loss: 0.355514  [12800/20380]\n",
            "Train Error: Accuracy: 84.8%\n",
            "loss: 0.388983  [19200/20380]\n",
            "Train Error: Accuracy: 85.5%\n",
            "Val Error: \n",
            " Accuracy: 84.1%, Avg loss: 0.464459 \n",
            "\n",
            "Validation no longer rising. Done!\n",
            "Max number of epochs hit or validation stop reached. Done!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##1B: LeakyReLU"
      ],
      "metadata": {
        "id": "2G5quPs0Q1BI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#check what device we're using\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"Using {device} device\")\n",
        "torch.set_grad_enabled(True) "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4486783c-065a-4c44-c7a9-b23bda9e744c",
        "id": "XwnlSbCPeO_o"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using cpu device\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch.autograd.grad_mode.set_grad_enabled at 0x7f260f2e1e50>"
            ]
          },
          "metadata": {},
          "execution_count": 88
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class NeuralNetwork(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.linear_relu_stack = nn.Sequential(\n",
        "            nn.Linear(7, 256), #input layer \n",
        "            nn.LeakyReLU(),\n",
        "            nn.Linear(256, 256), #hidden layer 1\n",
        "            nn.LeakyReLU(),\n",
        "            nn.Linear(256, 256), #hidden layer 2\n",
        "            nn.LeakyReLU(),\n",
        "            nn.Linear(256, 5), #output layer\n",
        "        )\n",
        "    # \"Every nn.Module subclass implements the operations \n",
        "    # on input data in the forward method.\"\n",
        "    # We already preprocessed the input data\n",
        "    # so we just return x\n",
        "    def forward(self, x): \n",
        "        logits = self.linear_relu_stack(x) #we already preprocessed the data, so we just pass it into the NN here\n",
        "        return logits"
      ],
      "metadata": {
        "id": "Vn4XRktmeO_r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Implement forward propagation (clearly describe the activation functions and other\n",
        "hyper-parameters you are using)."
      ],
      "metadata": {
        "id": "Rgh9roImeO_u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = NeuralNetwork().to(device)\n",
        "print(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aa354201-78df-4028-ce5e-253d19ad2e23",
        "id": "6BOzhZVQeO_z"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NeuralNetwork(\n",
            "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
            "  (linear_relu_stack): Sequential(\n",
            "    (0): Linear(in_features=7, out_features=256, bias=True)\n",
            "    (1): LeakyReLU(negative_slope=0.01)\n",
            "    (2): Linear(in_features=256, out_features=256, bias=True)\n",
            "    (3): LeakyReLU(negative_slope=0.01)\n",
            "    (4): Linear(in_features=256, out_features=256, bias=True)\n",
            "    (5): LeakyReLU(negative_slope=0.01)\n",
            "    (6): Linear(in_features=256, out_features=5, bias=True)\n",
            "  )\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "4. Compute the final cost function.\n",
        "\n",
        "5. Implement gradient descent (any variant of gradient descent depending upon your\n",
        "data and project can be used) to train your model. In this step it is up to you as someone\n",
        "in charge of their project to improvise using optimization algorithms (Adams, RMSProp\n",
        "etc.) and/or regularization."
      ],
      "metadata": {
        "id": "m6XgnwRAeO_0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=1e-2) "
      ],
      "metadata": {
        "id": "KPQ8B3hGeO_1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Code snippet attribution:\n",
        "#https://pytorch.org/tutorials/beginner/basics/intro.html\n",
        "#was used as part of the research process\n",
        "#Modifications were made so that data could be processed as longs,\n",
        "#additional statitics could be reported, \n",
        "#and a separate validate process was created. \n",
        "def train(dataloader, model, loss_fn, optimizer):\n",
        "    size = len(dataloader.dataset)\n",
        "    model.train()\n",
        "    for batch, (X, y) in enumerate(dataloader):\n",
        "        X, y = X.to(device), y.to(device)\n",
        "\n",
        "        # Compute prediction error\n",
        "        pred = model(X)\n",
        "        loss = loss_fn(pred, y.long()) \n",
        "\n",
        "        # Backpropagation\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if batch % 100 == 0:\n",
        "            loss, current = loss.item(), batch * len(X)\n",
        "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
        "            correct = 0\n",
        "            with torch.no_grad():\n",
        "                for X, y in dataloader:\n",
        "                    X, y = X.to(device), y.to(device)\n",
        "                    pred = model(X)\n",
        "                    correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
        "            correct /= size\n",
        "            print(f\"Train Error: Accuracy: {(100*correct):>0.1f}%\")\n",
        "\n",
        "\n",
        "def validate(dataloader, model, loss_fn):\n",
        "    size = len(dataloader.dataset)\n",
        "    num_batches = len(dataloader)\n",
        "    model.eval()\n",
        "    test_loss, correct = 0, 0\n",
        "    with torch.no_grad():\n",
        "        for X, y in dataloader:\n",
        "            X, y = X.to(device), y.to(device)\n",
        "            pred = model(X)\n",
        "            test_loss += loss_fn(pred, y.long()).item()\n",
        "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
        "    test_loss /= num_batches\n",
        "    correct /= size\n",
        "    print(f\"Val Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n",
        "    return correct\n",
        "\n",
        "def test(dataloader, model, loss_fn):\n",
        "    size = len(dataloader.dataset)\n",
        "    num_batches = len(dataloader)\n",
        "    model.eval()\n",
        "    test_loss, correct = 0, 0\n",
        "    with torch.no_grad():\n",
        "        for X, y in dataloader:\n",
        "            X, y = X.to(device), y.to(device)\n",
        "            pred = model(X)\n",
        "            test_loss += loss_fn(pred, y.long()).item()\n",
        "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
        "    test_loss /= num_batches\n",
        "    correct /= size\n",
        "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"
      ],
      "metadata": {
        "id": "yxFtJuYZeO_3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 100\n",
        "bestval = 0\n",
        "decval = 0\n",
        "for t in range(epochs):\n",
        "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
        "    train(train_dataloader, model, loss_fn, optimizer)\n",
        "    tempval = validate(val_dataloader, model, loss_fn)\n",
        "    if(tempval < bestval):\n",
        "      decval+=1 \n",
        "      if(decval==3):\n",
        "        print(\"Validation no longer rising. Done!\")\n",
        "        break\n",
        "    else:\n",
        "      bestval=tempval\n",
        "print(\"Max number of epochs hit or validation stop reached. Done!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8d453226-d03e-4de6-86fe-c890449031b0",
        "id": "li_C8ZW4eO_4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1\n",
            "-------------------------------\n",
            "loss: 1.612147  [    0/20380]\n",
            "Train Error: Accuracy: 21.1%\n",
            "loss: 1.590777  [ 6400/20380]\n",
            "Train Error: Accuracy: 28.5%\n",
            "loss: 1.525511  [12800/20380]\n",
            "Train Error: Accuracy: 34.8%\n",
            "loss: 1.365149  [19200/20380]\n",
            "Train Error: Accuracy: 39.1%\n",
            "Val Error: \n",
            " Accuracy: 38.1%, Avg loss: 1.436867 \n",
            "\n",
            "Epoch 2\n",
            "-------------------------------\n",
            "loss: 1.410685  [    0/20380]\n",
            "Train Error: Accuracy: 37.3%\n",
            "loss: 1.459715  [ 6400/20380]\n",
            "Train Error: Accuracy: 39.9%\n",
            "loss: 1.366254  [12800/20380]\n",
            "Train Error: Accuracy: 46.7%\n",
            "loss: 1.147103  [19200/20380]\n",
            "Train Error: Accuracy: 48.6%\n",
            "Val Error: \n",
            " Accuracy: 48.9%, Avg loss: 1.282867 \n",
            "\n",
            "Epoch 3\n",
            "-------------------------------\n",
            "loss: 1.239849  [    0/20380]\n",
            "Train Error: Accuracy: 48.7%\n",
            "loss: 1.325363  [ 6400/20380]\n",
            "Train Error: Accuracy: 48.7%\n",
            "loss: 1.251921  [12800/20380]\n",
            "Train Error: Accuracy: 49.1%\n",
            "loss: 1.008436  [19200/20380]\n",
            "Train Error: Accuracy: 49.9%\n",
            "Val Error: \n",
            " Accuracy: 50.7%, Avg loss: 1.187492 \n",
            "\n",
            "Epoch 4\n",
            "-------------------------------\n",
            "loss: 1.152487  [    0/20380]\n",
            "Train Error: Accuracy: 50.5%\n",
            "loss: 1.229248  [ 6400/20380]\n",
            "Train Error: Accuracy: 50.4%\n",
            "loss: 1.175781  [12800/20380]\n",
            "Train Error: Accuracy: 50.8%\n",
            "loss: 0.925681  [19200/20380]\n",
            "Train Error: Accuracy: 51.3%\n",
            "Val Error: \n",
            " Accuracy: 51.7%, Avg loss: 1.115418 \n",
            "\n",
            "Epoch 5\n",
            "-------------------------------\n",
            "loss: 1.103971  [    0/20380]\n",
            "Train Error: Accuracy: 51.2%\n",
            "loss: 1.144779  [ 6400/20380]\n",
            "Train Error: Accuracy: 52.3%\n",
            "loss: 1.119169  [12800/20380]\n",
            "Train Error: Accuracy: 52.8%\n",
            "loss: 0.880740  [19200/20380]\n",
            "Train Error: Accuracy: 52.9%\n",
            "Val Error: \n",
            " Accuracy: 52.7%, Avg loss: 1.061595 \n",
            "\n",
            "Epoch 6\n",
            "-------------------------------\n",
            "loss: 1.067078  [    0/20380]\n",
            "Train Error: Accuracy: 52.4%\n",
            "loss: 1.081815  [ 6400/20380]\n",
            "Train Error: Accuracy: 53.3%\n",
            "loss: 1.068516  [12800/20380]\n",
            "Train Error: Accuracy: 53.5%\n",
            "loss: 0.850515  [19200/20380]\n",
            "Train Error: Accuracy: 53.1%\n",
            "Val Error: \n",
            " Accuracy: 53.4%, Avg loss: 1.019224 \n",
            "\n",
            "Epoch 7\n",
            "-------------------------------\n",
            "loss: 1.036279  [    0/20380]\n",
            "Train Error: Accuracy: 52.9%\n",
            "loss: 1.027777  [ 6400/20380]\n",
            "Train Error: Accuracy: 54.1%\n",
            "loss: 1.017727  [12800/20380]\n",
            "Train Error: Accuracy: 54.4%\n",
            "loss: 0.825214  [19200/20380]\n",
            "Train Error: Accuracy: 54.4%\n",
            "Val Error: \n",
            " Accuracy: 54.9%, Avg loss: 0.981040 \n",
            "\n",
            "Epoch 8\n",
            "-------------------------------\n",
            "loss: 1.007105  [    0/20380]\n",
            "Train Error: Accuracy: 54.3%\n",
            "loss: 0.973716  [ 6400/20380]\n",
            "Train Error: Accuracy: 55.7%\n",
            "loss: 0.967212  [12800/20380]\n",
            "Train Error: Accuracy: 56.2%\n",
            "loss: 0.800078  [19200/20380]\n",
            "Train Error: Accuracy: 56.9%\n",
            "Val Error: \n",
            " Accuracy: 58.8%, Avg loss: 0.943875 \n",
            "\n",
            "Epoch 9\n",
            "-------------------------------\n",
            "loss: 0.976017  [    0/20380]\n",
            "Train Error: Accuracy: 57.7%\n",
            "loss: 0.919858  [ 6400/20380]\n",
            "Train Error: Accuracy: 57.6%\n",
            "loss: 0.918864  [12800/20380]\n",
            "Train Error: Accuracy: 59.3%\n",
            "loss: 0.775694  [19200/20380]\n",
            "Train Error: Accuracy: 60.4%\n",
            "Val Error: \n",
            " Accuracy: 61.3%, Avg loss: 0.907620 \n",
            "\n",
            "Epoch 10\n",
            "-------------------------------\n",
            "loss: 0.945475  [    0/20380]\n",
            "Train Error: Accuracy: 60.3%\n",
            "loss: 0.868261  [ 6400/20380]\n",
            "Train Error: Accuracy: 60.4%\n",
            "loss: 0.873769  [12800/20380]\n",
            "Train Error: Accuracy: 62.4%\n",
            "loss: 0.754031  [19200/20380]\n",
            "Train Error: Accuracy: 62.9%\n",
            "Val Error: \n",
            " Accuracy: 63.8%, Avg loss: 0.873120 \n",
            "\n",
            "Epoch 11\n",
            "-------------------------------\n",
            "loss: 0.916557  [    0/20380]\n",
            "Train Error: Accuracy: 62.6%\n",
            "loss: 0.820770  [ 6400/20380]\n",
            "Train Error: Accuracy: 62.3%\n",
            "loss: 0.833170  [12800/20380]\n",
            "Train Error: Accuracy: 64.5%\n",
            "loss: 0.733448  [19200/20380]\n",
            "Train Error: Accuracy: 64.6%\n",
            "Val Error: \n",
            " Accuracy: 65.5%, Avg loss: 0.840945 \n",
            "\n",
            "Epoch 12\n",
            "-------------------------------\n",
            "loss: 0.889098  [    0/20380]\n",
            "Train Error: Accuracy: 64.5%\n",
            "loss: 0.779414  [ 6400/20380]\n",
            "Train Error: Accuracy: 64.3%\n",
            "loss: 0.796109  [12800/20380]\n",
            "Train Error: Accuracy: 66.2%\n",
            "loss: 0.713370  [19200/20380]\n",
            "Train Error: Accuracy: 66.6%\n",
            "Val Error: \n",
            " Accuracy: 67.4%, Avg loss: 0.812237 \n",
            "\n",
            "Epoch 13\n",
            "-------------------------------\n",
            "loss: 0.863942  [    0/20380]\n",
            "Train Error: Accuracy: 66.6%\n",
            "loss: 0.743285  [ 6400/20380]\n",
            "Train Error: Accuracy: 66.1%\n",
            "loss: 0.762473  [12800/20380]\n",
            "Train Error: Accuracy: 67.9%\n",
            "loss: 0.693927  [19200/20380]\n",
            "Train Error: Accuracy: 68.5%\n",
            "Val Error: \n",
            " Accuracy: 68.9%, Avg loss: 0.786737 \n",
            "\n",
            "Epoch 14\n",
            "-------------------------------\n",
            "loss: 0.841606  [    0/20380]\n",
            "Train Error: Accuracy: 68.7%\n",
            "loss: 0.709858  [ 6400/20380]\n",
            "Train Error: Accuracy: 68.0%\n",
            "loss: 0.732115  [12800/20380]\n",
            "Train Error: Accuracy: 69.3%\n",
            "loss: 0.677378  [19200/20380]\n",
            "Train Error: Accuracy: 70.0%\n",
            "Val Error: \n",
            " Accuracy: 70.2%, Avg loss: 0.763358 \n",
            "\n",
            "Epoch 15\n",
            "-------------------------------\n",
            "loss: 0.822859  [    0/20380]\n",
            "Train Error: Accuracy: 70.3%\n",
            "loss: 0.679758  [ 6400/20380]\n",
            "Train Error: Accuracy: 69.3%\n",
            "loss: 0.704564  [12800/20380]\n",
            "Train Error: Accuracy: 70.7%\n",
            "loss: 0.662135  [19200/20380]\n",
            "Train Error: Accuracy: 71.2%\n",
            "Val Error: \n",
            " Accuracy: 71.1%, Avg loss: 0.742254 \n",
            "\n",
            "Epoch 16\n",
            "-------------------------------\n",
            "loss: 0.806363  [    0/20380]\n",
            "Train Error: Accuracy: 71.5%\n",
            "loss: 0.652663  [ 6400/20380]\n",
            "Train Error: Accuracy: 70.5%\n",
            "loss: 0.678899  [12800/20380]\n",
            "Train Error: Accuracy: 71.9%\n",
            "loss: 0.648835  [19200/20380]\n",
            "Train Error: Accuracy: 72.3%\n",
            "Val Error: \n",
            " Accuracy: 72.1%, Avg loss: 0.722928 \n",
            "\n",
            "Epoch 17\n",
            "-------------------------------\n",
            "loss: 0.790531  [    0/20380]\n",
            "Train Error: Accuracy: 72.4%\n",
            "loss: 0.628787  [ 6400/20380]\n",
            "Train Error: Accuracy: 71.5%\n",
            "loss: 0.655407  [12800/20380]\n",
            "Train Error: Accuracy: 73.0%\n",
            "loss: 0.636437  [19200/20380]\n",
            "Train Error: Accuracy: 73.3%\n",
            "Val Error: \n",
            " Accuracy: 73.2%, Avg loss: 0.705379 \n",
            "\n",
            "Epoch 18\n",
            "-------------------------------\n",
            "loss: 0.776788  [    0/20380]\n",
            "Train Error: Accuracy: 73.4%\n",
            "loss: 0.604735  [ 6400/20380]\n",
            "Train Error: Accuracy: 72.8%\n",
            "loss: 0.633240  [12800/20380]\n",
            "Train Error: Accuracy: 74.1%\n",
            "loss: 0.625117  [19200/20380]\n",
            "Train Error: Accuracy: 74.5%\n",
            "Val Error: \n",
            " Accuracy: 73.9%, Avg loss: 0.688954 \n",
            "\n",
            "Epoch 19\n",
            "-------------------------------\n",
            "loss: 0.763629  [    0/20380]\n",
            "Train Error: Accuracy: 74.3%\n",
            "loss: 0.583402  [ 6400/20380]\n",
            "Train Error: Accuracy: 73.8%\n",
            "loss: 0.615032  [12800/20380]\n",
            "Train Error: Accuracy: 75.0%\n",
            "loss: 0.617396  [19200/20380]\n",
            "Train Error: Accuracy: 75.1%\n",
            "Val Error: \n",
            " Accuracy: 74.8%, Avg loss: 0.674067 \n",
            "\n",
            "Epoch 20\n",
            "-------------------------------\n",
            "loss: 0.752692  [    0/20380]\n",
            "Train Error: Accuracy: 75.1%\n",
            "loss: 0.563292  [ 6400/20380]\n",
            "Train Error: Accuracy: 74.8%\n",
            "loss: 0.597962  [12800/20380]\n",
            "Train Error: Accuracy: 75.8%\n",
            "loss: 0.609161  [19200/20380]\n",
            "Train Error: Accuracy: 75.9%\n",
            "Val Error: \n",
            " Accuracy: 75.5%, Avg loss: 0.660317 \n",
            "\n",
            "Epoch 21\n",
            "-------------------------------\n",
            "loss: 0.742769  [    0/20380]\n",
            "Train Error: Accuracy: 75.8%\n",
            "loss: 0.545118  [ 6400/20380]\n",
            "Train Error: Accuracy: 75.5%\n",
            "loss: 0.581515  [12800/20380]\n",
            "Train Error: Accuracy: 76.4%\n",
            "loss: 0.602314  [19200/20380]\n",
            "Train Error: Accuracy: 76.5%\n",
            "Val Error: \n",
            " Accuracy: 76.3%, Avg loss: 0.648284 \n",
            "\n",
            "Epoch 22\n",
            "-------------------------------\n",
            "loss: 0.733928  [    0/20380]\n",
            "Train Error: Accuracy: 76.5%\n",
            "loss: 0.528013  [ 6400/20380]\n",
            "Train Error: Accuracy: 76.2%\n",
            "loss: 0.566148  [12800/20380]\n",
            "Train Error: Accuracy: 77.0%\n",
            "loss: 0.595059  [19200/20380]\n",
            "Train Error: Accuracy: 77.1%\n",
            "Val Error: \n",
            " Accuracy: 76.9%, Avg loss: 0.636947 \n",
            "\n",
            "Epoch 23\n",
            "-------------------------------\n",
            "loss: 0.725505  [    0/20380]\n",
            "Train Error: Accuracy: 77.0%\n",
            "loss: 0.511981  [ 6400/20380]\n",
            "Train Error: Accuracy: 76.9%\n",
            "loss: 0.551574  [12800/20380]\n",
            "Train Error: Accuracy: 77.6%\n",
            "loss: 0.588361  [19200/20380]\n",
            "Train Error: Accuracy: 77.8%\n",
            "Val Error: \n",
            " Accuracy: 77.3%, Avg loss: 0.626411 \n",
            "\n",
            "Epoch 24\n",
            "-------------------------------\n",
            "loss: 0.718030  [    0/20380]\n",
            "Train Error: Accuracy: 77.8%\n",
            "loss: 0.497722  [ 6400/20380]\n",
            "Train Error: Accuracy: 77.6%\n",
            "loss: 0.536913  [12800/20380]\n",
            "Train Error: Accuracy: 78.2%\n",
            "loss: 0.582295  [19200/20380]\n",
            "Train Error: Accuracy: 78.4%\n",
            "Val Error: \n",
            " Accuracy: 77.8%, Avg loss: 0.615766 \n",
            "\n",
            "Epoch 25\n",
            "-------------------------------\n",
            "loss: 0.711152  [    0/20380]\n",
            "Train Error: Accuracy: 78.5%\n",
            "loss: 0.484463  [ 6400/20380]\n",
            "Train Error: Accuracy: 78.1%\n",
            "loss: 0.523755  [12800/20380]\n",
            "Train Error: Accuracy: 78.8%\n",
            "loss: 0.576389  [19200/20380]\n",
            "Train Error: Accuracy: 78.9%\n",
            "Val Error: \n",
            " Accuracy: 78.3%, Avg loss: 0.606252 \n",
            "\n",
            "Epoch 26\n",
            "-------------------------------\n",
            "loss: 0.704320  [    0/20380]\n",
            "Train Error: Accuracy: 79.0%\n",
            "loss: 0.471955  [ 6400/20380]\n",
            "Train Error: Accuracy: 78.6%\n",
            "loss: 0.510907  [12800/20380]\n",
            "Train Error: Accuracy: 79.3%\n",
            "loss: 0.570639  [19200/20380]\n",
            "Train Error: Accuracy: 79.4%\n",
            "Val Error: \n",
            " Accuracy: 78.7%, Avg loss: 0.597255 \n",
            "\n",
            "Epoch 27\n",
            "-------------------------------\n",
            "loss: 0.698026  [    0/20380]\n",
            "Train Error: Accuracy: 79.4%\n",
            "loss: 0.460706  [ 6400/20380]\n",
            "Train Error: Accuracy: 79.0%\n",
            "loss: 0.498130  [12800/20380]\n",
            "Train Error: Accuracy: 79.7%\n",
            "loss: 0.564413  [19200/20380]\n",
            "Train Error: Accuracy: 79.9%\n",
            "Val Error: \n",
            " Accuracy: 79.6%, Avg loss: 0.588473 \n",
            "\n",
            "Epoch 28\n",
            "-------------------------------\n",
            "loss: 0.691032  [    0/20380]\n",
            "Train Error: Accuracy: 79.7%\n",
            "loss: 0.449954  [ 6400/20380]\n",
            "Train Error: Accuracy: 79.4%\n",
            "loss: 0.486478  [12800/20380]\n",
            "Train Error: Accuracy: 80.1%\n",
            "loss: 0.559046  [19200/20380]\n",
            "Train Error: Accuracy: 80.2%\n",
            "Val Error: \n",
            " Accuracy: 80.2%, Avg loss: 0.580665 \n",
            "\n",
            "Epoch 29\n",
            "-------------------------------\n",
            "loss: 0.684265  [    0/20380]\n",
            "Train Error: Accuracy: 80.1%\n",
            "loss: 0.440811  [ 6400/20380]\n",
            "Train Error: Accuracy: 79.7%\n",
            "loss: 0.474964  [12800/20380]\n",
            "Train Error: Accuracy: 80.4%\n",
            "loss: 0.554125  [19200/20380]\n",
            "Train Error: Accuracy: 80.5%\n",
            "Val Error: \n",
            " Accuracy: 80.6%, Avg loss: 0.572651 \n",
            "\n",
            "Epoch 30\n",
            "-------------------------------\n",
            "loss: 0.677824  [    0/20380]\n",
            "Train Error: Accuracy: 80.4%\n",
            "loss: 0.432368  [ 6400/20380]\n",
            "Train Error: Accuracy: 79.9%\n",
            "loss: 0.464330  [12800/20380]\n",
            "Train Error: Accuracy: 80.7%\n",
            "loss: 0.548473  [19200/20380]\n",
            "Train Error: Accuracy: 80.8%\n",
            "Val Error: \n",
            " Accuracy: 80.8%, Avg loss: 0.564749 \n",
            "\n",
            "Epoch 31\n",
            "-------------------------------\n",
            "loss: 0.672348  [    0/20380]\n",
            "Train Error: Accuracy: 80.8%\n",
            "loss: 0.423928  [ 6400/20380]\n",
            "Train Error: Accuracy: 80.2%\n",
            "loss: 0.454152  [12800/20380]\n",
            "Train Error: Accuracy: 81.0%\n",
            "loss: 0.542985  [19200/20380]\n",
            "Train Error: Accuracy: 81.3%\n",
            "Val Error: \n",
            " Accuracy: 81.1%, Avg loss: 0.557339 \n",
            "\n",
            "Epoch 32\n",
            "-------------------------------\n",
            "loss: 0.666261  [    0/20380]\n",
            "Train Error: Accuracy: 81.1%\n",
            "loss: 0.416057  [ 6400/20380]\n",
            "Train Error: Accuracy: 80.4%\n",
            "loss: 0.445141  [12800/20380]\n",
            "Train Error: Accuracy: 81.2%\n",
            "loss: 0.537831  [19200/20380]\n",
            "Train Error: Accuracy: 81.6%\n",
            "Val Error: \n",
            " Accuracy: 81.4%, Avg loss: 0.550313 \n",
            "\n",
            "Epoch 33\n",
            "-------------------------------\n",
            "loss: 0.661040  [    0/20380]\n",
            "Train Error: Accuracy: 81.4%\n",
            "loss: 0.408729  [ 6400/20380]\n",
            "Train Error: Accuracy: 80.7%\n",
            "loss: 0.436503  [12800/20380]\n",
            "Train Error: Accuracy: 81.4%\n",
            "loss: 0.533235  [19200/20380]\n",
            "Train Error: Accuracy: 81.9%\n",
            "Val Error: \n",
            " Accuracy: 81.6%, Avg loss: 0.544195 \n",
            "\n",
            "Epoch 34\n",
            "-------------------------------\n",
            "loss: 0.655628  [    0/20380]\n",
            "Train Error: Accuracy: 81.6%\n",
            "loss: 0.401577  [ 6400/20380]\n",
            "Train Error: Accuracy: 80.9%\n",
            "loss: 0.428180  [12800/20380]\n",
            "Train Error: Accuracy: 81.6%\n",
            "loss: 0.526720  [19200/20380]\n",
            "Train Error: Accuracy: 82.2%\n",
            "Val Error: \n",
            " Accuracy: 81.8%, Avg loss: 0.538032 \n",
            "\n",
            "Epoch 35\n",
            "-------------------------------\n",
            "loss: 0.650904  [    0/20380]\n",
            "Train Error: Accuracy: 81.9%\n",
            "loss: 0.394825  [ 6400/20380]\n",
            "Train Error: Accuracy: 81.1%\n",
            "loss: 0.420153  [12800/20380]\n",
            "Train Error: Accuracy: 81.8%\n",
            "loss: 0.521115  [19200/20380]\n",
            "Train Error: Accuracy: 82.5%\n",
            "Val Error: \n",
            " Accuracy: 82.0%, Avg loss: 0.532687 \n",
            "\n",
            "Epoch 36\n",
            "-------------------------------\n",
            "loss: 0.645677  [    0/20380]\n",
            "Train Error: Accuracy: 82.1%\n",
            "loss: 0.388153  [ 6400/20380]\n",
            "Train Error: Accuracy: 81.3%\n",
            "loss: 0.412698  [12800/20380]\n",
            "Train Error: Accuracy: 82.0%\n",
            "loss: 0.515861  [19200/20380]\n",
            "Train Error: Accuracy: 82.7%\n",
            "Val Error: \n",
            " Accuracy: 82.1%, Avg loss: 0.527802 \n",
            "\n",
            "Epoch 37\n",
            "-------------------------------\n",
            "loss: 0.641356  [    0/20380]\n",
            "Train Error: Accuracy: 82.4%\n",
            "loss: 0.381885  [ 6400/20380]\n",
            "Train Error: Accuracy: 81.5%\n",
            "loss: 0.405943  [12800/20380]\n",
            "Train Error: Accuracy: 82.1%\n",
            "loss: 0.510702  [19200/20380]\n",
            "Train Error: Accuracy: 83.0%\n",
            "Val Error: \n",
            " Accuracy: 82.3%, Avg loss: 0.522349 \n",
            "\n",
            "Epoch 38\n",
            "-------------------------------\n",
            "loss: 0.635693  [    0/20380]\n",
            "Train Error: Accuracy: 82.6%\n",
            "loss: 0.376359  [ 6400/20380]\n",
            "Train Error: Accuracy: 81.7%\n",
            "loss: 0.399930  [12800/20380]\n",
            "Train Error: Accuracy: 82.3%\n",
            "loss: 0.504595  [19200/20380]\n",
            "Train Error: Accuracy: 83.3%\n",
            "Val Error: \n",
            " Accuracy: 82.5%, Avg loss: 0.518155 \n",
            "\n",
            "Epoch 39\n",
            "-------------------------------\n",
            "loss: 0.631842  [    0/20380]\n",
            "Train Error: Accuracy: 82.9%\n",
            "loss: 0.371189  [ 6400/20380]\n",
            "Train Error: Accuracy: 81.9%\n",
            "loss: 0.394186  [12800/20380]\n",
            "Train Error: Accuracy: 82.4%\n",
            "loss: 0.499037  [19200/20380]\n",
            "Train Error: Accuracy: 83.5%\n",
            "Val Error: \n",
            " Accuracy: 82.7%, Avg loss: 0.513440 \n",
            "\n",
            "Epoch 40\n",
            "-------------------------------\n",
            "loss: 0.627141  [    0/20380]\n",
            "Train Error: Accuracy: 83.1%\n",
            "loss: 0.366042  [ 6400/20380]\n",
            "Train Error: Accuracy: 82.1%\n",
            "loss: 0.388543  [12800/20380]\n",
            "Train Error: Accuracy: 82.7%\n",
            "loss: 0.493902  [19200/20380]\n",
            "Train Error: Accuracy: 83.7%\n",
            "Val Error: \n",
            " Accuracy: 82.7%, Avg loss: 0.509328 \n",
            "\n",
            "Epoch 41\n",
            "-------------------------------\n",
            "loss: 0.623002  [    0/20380]\n",
            "Train Error: Accuracy: 83.2%\n",
            "loss: 0.361361  [ 6400/20380]\n",
            "Train Error: Accuracy: 82.3%\n",
            "loss: 0.383095  [12800/20380]\n",
            "Train Error: Accuracy: 82.9%\n",
            "loss: 0.487962  [19200/20380]\n",
            "Train Error: Accuracy: 83.8%\n",
            "Val Error: \n",
            " Accuracy: 82.8%, Avg loss: 0.505307 \n",
            "\n",
            "Epoch 42\n",
            "-------------------------------\n",
            "loss: 0.618872  [    0/20380]\n",
            "Train Error: Accuracy: 83.3%\n",
            "loss: 0.356737  [ 6400/20380]\n",
            "Train Error: Accuracy: 82.5%\n",
            "loss: 0.377824  [12800/20380]\n",
            "Train Error: Accuracy: 83.1%\n",
            "loss: 0.482493  [19200/20380]\n",
            "Train Error: Accuracy: 84.0%\n",
            "Val Error: \n",
            " Accuracy: 82.9%, Avg loss: 0.501605 \n",
            "\n",
            "Epoch 43\n",
            "-------------------------------\n",
            "loss: 0.615904  [    0/20380]\n",
            "Train Error: Accuracy: 83.5%\n",
            "loss: 0.352054  [ 6400/20380]\n",
            "Train Error: Accuracy: 82.6%\n",
            "loss: 0.373246  [12800/20380]\n",
            "Train Error: Accuracy: 83.3%\n",
            "loss: 0.476654  [19200/20380]\n",
            "Train Error: Accuracy: 84.1%\n",
            "Val Error: \n",
            " Accuracy: 83.0%, Avg loss: 0.498272 \n",
            "\n",
            "Epoch 44\n",
            "-------------------------------\n",
            "loss: 0.611868  [    0/20380]\n",
            "Train Error: Accuracy: 83.7%\n",
            "loss: 0.348777  [ 6400/20380]\n",
            "Train Error: Accuracy: 82.8%\n",
            "loss: 0.368961  [12800/20380]\n",
            "Train Error: Accuracy: 83.5%\n",
            "loss: 0.470632  [19200/20380]\n",
            "Train Error: Accuracy: 84.3%\n",
            "Val Error: \n",
            " Accuracy: 83.0%, Avg loss: 0.494711 \n",
            "\n",
            "Epoch 45\n",
            "-------------------------------\n",
            "loss: 0.607964  [    0/20380]\n",
            "Train Error: Accuracy: 83.9%\n",
            "loss: 0.345542  [ 6400/20380]\n",
            "Train Error: Accuracy: 83.0%\n",
            "loss: 0.364144  [12800/20380]\n",
            "Train Error: Accuracy: 83.6%\n",
            "loss: 0.464969  [19200/20380]\n",
            "Train Error: Accuracy: 84.5%\n",
            "Val Error: \n",
            " Accuracy: 83.0%, Avg loss: 0.491521 \n",
            "\n",
            "Epoch 46\n",
            "-------------------------------\n",
            "loss: 0.604225  [    0/20380]\n",
            "Train Error: Accuracy: 84.0%\n",
            "loss: 0.342653  [ 6400/20380]\n",
            "Train Error: Accuracy: 83.2%\n",
            "loss: 0.359793  [12800/20380]\n",
            "Train Error: Accuracy: 83.7%\n",
            "loss: 0.458781  [19200/20380]\n",
            "Train Error: Accuracy: 84.6%\n",
            "Val Error: \n",
            " Accuracy: 83.0%, Avg loss: 0.488964 \n",
            "\n",
            "Epoch 47\n",
            "-------------------------------\n",
            "loss: 0.600105  [    0/20380]\n",
            "Train Error: Accuracy: 84.1%\n",
            "loss: 0.339645  [ 6400/20380]\n",
            "Train Error: Accuracy: 83.2%\n",
            "loss: 0.355889  [12800/20380]\n",
            "Train Error: Accuracy: 83.9%\n",
            "loss: 0.453214  [19200/20380]\n",
            "Train Error: Accuracy: 84.7%\n",
            "Val Error: \n",
            " Accuracy: 83.1%, Avg loss: 0.485990 \n",
            "\n",
            "Epoch 48\n",
            "-------------------------------\n",
            "loss: 0.596697  [    0/20380]\n",
            "Train Error: Accuracy: 84.2%\n",
            "loss: 0.336867  [ 6400/20380]\n",
            "Train Error: Accuracy: 83.3%\n",
            "loss: 0.352472  [12800/20380]\n",
            "Train Error: Accuracy: 84.0%\n",
            "loss: 0.447014  [19200/20380]\n",
            "Train Error: Accuracy: 84.9%\n",
            "Val Error: \n",
            " Accuracy: 83.0%, Avg loss: 0.483427 \n",
            "\n",
            "Epoch 49\n",
            "-------------------------------\n",
            "loss: 0.593999  [    0/20380]\n",
            "Train Error: Accuracy: 84.4%\n",
            "loss: 0.333869  [ 6400/20380]\n",
            "Train Error: Accuracy: 83.5%\n",
            "loss: 0.348676  [12800/20380]\n",
            "Train Error: Accuracy: 84.1%\n",
            "loss: 0.440840  [19200/20380]\n",
            "Train Error: Accuracy: 85.1%\n",
            "Val Error: \n",
            " Accuracy: 83.1%, Avg loss: 0.480983 \n",
            "\n",
            "Epoch 50\n",
            "-------------------------------\n",
            "loss: 0.591002  [    0/20380]\n",
            "Train Error: Accuracy: 84.5%\n",
            "loss: 0.330907  [ 6400/20380]\n",
            "Train Error: Accuracy: 83.7%\n",
            "loss: 0.344930  [12800/20380]\n",
            "Train Error: Accuracy: 84.3%\n",
            "loss: 0.435119  [19200/20380]\n",
            "Train Error: Accuracy: 85.2%\n",
            "Val Error: \n",
            " Accuracy: 83.2%, Avg loss: 0.479137 \n",
            "\n",
            "Epoch 51\n",
            "-------------------------------\n",
            "loss: 0.589203  [    0/20380]\n",
            "Train Error: Accuracy: 84.6%\n",
            "loss: 0.327788  [ 6400/20380]\n",
            "Train Error: Accuracy: 83.8%\n",
            "loss: 0.341068  [12800/20380]\n",
            "Train Error: Accuracy: 84.4%\n",
            "loss: 0.429334  [19200/20380]\n",
            "Train Error: Accuracy: 85.4%\n",
            "Val Error: \n",
            " Accuracy: 83.3%, Avg loss: 0.476571 \n",
            "\n",
            "Epoch 52\n",
            "-------------------------------\n",
            "loss: 0.587022  [    0/20380]\n",
            "Train Error: Accuracy: 84.7%\n",
            "loss: 0.325863  [ 6400/20380]\n",
            "Train Error: Accuracy: 83.9%\n",
            "loss: 0.337825  [12800/20380]\n",
            "Train Error: Accuracy: 84.6%\n",
            "loss: 0.423213  [19200/20380]\n",
            "Train Error: Accuracy: 85.5%\n",
            "Val Error: \n",
            " Accuracy: 83.3%, Avg loss: 0.474241 \n",
            "\n",
            "Epoch 53\n",
            "-------------------------------\n",
            "loss: 0.585346  [    0/20380]\n",
            "Train Error: Accuracy: 84.9%\n",
            "loss: 0.323674  [ 6400/20380]\n",
            "Train Error: Accuracy: 84.0%\n",
            "loss: 0.334444  [12800/20380]\n",
            "Train Error: Accuracy: 84.7%\n",
            "loss: 0.417873  [19200/20380]\n",
            "Train Error: Accuracy: 85.6%\n",
            "Val Error: \n",
            " Accuracy: 83.3%, Avg loss: 0.472201 \n",
            "\n",
            "Epoch 54\n",
            "-------------------------------\n",
            "loss: 0.583110  [    0/20380]\n",
            "Train Error: Accuracy: 85.0%\n",
            "loss: 0.321187  [ 6400/20380]\n",
            "Train Error: Accuracy: 84.1%\n",
            "loss: 0.331648  [12800/20380]\n",
            "Train Error: Accuracy: 84.8%\n",
            "loss: 0.412018  [19200/20380]\n",
            "Train Error: Accuracy: 85.7%\n",
            "Val Error: \n",
            " Accuracy: 83.4%, Avg loss: 0.470665 \n",
            "\n",
            "Epoch 55\n",
            "-------------------------------\n",
            "loss: 0.582294  [    0/20380]\n",
            "Train Error: Accuracy: 85.1%\n",
            "loss: 0.318921  [ 6400/20380]\n",
            "Train Error: Accuracy: 84.3%\n",
            "loss: 0.328785  [12800/20380]\n",
            "Train Error: Accuracy: 85.0%\n",
            "loss: 0.406808  [19200/20380]\n",
            "Train Error: Accuracy: 85.8%\n",
            "Val Error: \n",
            " Accuracy: 83.5%, Avg loss: 0.468392 \n",
            "\n",
            "Epoch 56\n",
            "-------------------------------\n",
            "loss: 0.579982  [    0/20380]\n",
            "Train Error: Accuracy: 85.2%\n",
            "loss: 0.316997  [ 6400/20380]\n",
            "Train Error: Accuracy: 84.3%\n",
            "loss: 0.325702  [12800/20380]\n",
            "Train Error: Accuracy: 85.0%\n",
            "loss: 0.400600  [19200/20380]\n",
            "Train Error: Accuracy: 85.9%\n",
            "Val Error: \n",
            " Accuracy: 83.4%, Avg loss: 0.466898 \n",
            "\n",
            "Validation no longer rising. Done!\n",
            "Max number of epochs hit or validation stop reached. Done!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##1C: ELU"
      ],
      "metadata": {
        "id": "nLOnFGLMQ2BZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#check what device we're using\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"Using {device} device\")\n",
        "torch.set_grad_enabled(True) "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_madwGL_fTDf",
        "outputId": "41c20d12-a184-4df6-ee1f-34ae819efae0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using cpu device\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch.autograd.grad_mode.set_grad_enabled at 0x7f260f35af90>"
            ]
          },
          "metadata": {},
          "execution_count": 95
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class NeuralNetwork(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.linear_relu_stack = nn.Sequential(\n",
        "            nn.Linear(7, 256), #input layer \n",
        "            nn.ELU(), \n",
        "            nn.Linear(256, 256), #hidden layer 1\n",
        "            nn.ELU(),\n",
        "            nn.Linear(256, 256), #hidden layer 2\n",
        "            nn.ELU(),\n",
        "            nn.Linear(256, 5), #output layer\n",
        "        )\n",
        "    # \"Every nn.Module subclass implements the operations \n",
        "    # on input data in the forward method.\"\n",
        "    # We already preprocessed the input data\n",
        "    # so we just return x\n",
        "    def forward(self, x): \n",
        "        logits = self.linear_relu_stack(x) #we already preprocessed the data, so we just pass it into the NN here\n",
        "        return logits"
      ],
      "metadata": {
        "id": "qy2qZIukfTDh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Implement forward propagation (clearly describe the activation functions and other\n",
        "hyper-parameters you are using)."
      ],
      "metadata": {
        "id": "eAbTMxebfTDj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = NeuralNetwork().to(device)\n",
        "print(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yppj7xXxfTDl",
        "outputId": "eae90a27-a6ed-4b23-e886-1a49d78e1a06"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NeuralNetwork(\n",
            "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
            "  (linear_relu_stack): Sequential(\n",
            "    (0): Linear(in_features=7, out_features=256, bias=True)\n",
            "    (1): ELU(alpha=1.0)\n",
            "    (2): Linear(in_features=256, out_features=256, bias=True)\n",
            "    (3): ELU(alpha=1.0)\n",
            "    (4): Linear(in_features=256, out_features=256, bias=True)\n",
            "    (5): ELU(alpha=1.0)\n",
            "    (6): Linear(in_features=256, out_features=5, bias=True)\n",
            "  )\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "4. Compute the final cost function.\n",
        "\n",
        "5. Implement gradient descent (any variant of gradient descent depending upon your\n",
        "data and project can be used) to train your model. In this step it is up to you as someone\n",
        "in charge of their project to improvise using optimization algorithms (Adams, RMSProp\n",
        "etc.) and/or regularization."
      ],
      "metadata": {
        "id": "1bb4xQpTfTDm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=1e-2) "
      ],
      "metadata": {
        "id": "egxjvqVXfTDn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Code snippet attribution:\n",
        "#https://pytorch.org/tutorials/beginner/basics/intro.html\n",
        "#was used as part of the research process\n",
        "#Modifications were made so that data could be processed as longs,\n",
        "#additional statitics could be reported, \n",
        "#and a separate validate process was created.  \n",
        "def train(dataloader, model, loss_fn, optimizer):\n",
        "    size = len(dataloader.dataset)\n",
        "    model.train()\n",
        "    for batch, (X, y) in enumerate(dataloader):\n",
        "        X, y = X.to(device), y.to(device)\n",
        "\n",
        "        # Compute prediction error\n",
        "        pred = model(X)\n",
        "        loss = loss_fn(pred, y.long()) \n",
        "\n",
        "        # Backpropagation\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if batch % 100 == 0:\n",
        "            loss, current = loss.item(), batch * len(X)\n",
        "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
        "            correct = 0\n",
        "            with torch.no_grad():\n",
        "                for X, y in dataloader:\n",
        "                    X, y = X.to(device), y.to(device)\n",
        "                    pred = model(X)\n",
        "                    correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
        "            correct /= size\n",
        "            print(f\"Train Error: Accuracy: {(100*correct):>0.1f}%\")\n",
        "\n",
        "\n",
        "def validate(dataloader, model, loss_fn):\n",
        "    size = len(dataloader.dataset)\n",
        "    num_batches = len(dataloader)\n",
        "    model.eval()\n",
        "    test_loss, correct = 0, 0\n",
        "    with torch.no_grad():\n",
        "        for X, y in dataloader:\n",
        "            X, y = X.to(device), y.to(device)\n",
        "            pred = model(X)\n",
        "            test_loss += loss_fn(pred, y.long()).item()\n",
        "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
        "    test_loss /= num_batches\n",
        "    correct /= size\n",
        "    print(f\"Val Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n",
        "    return correct\n",
        "\n",
        "def test(dataloader, model, loss_fn):\n",
        "    size = len(dataloader.dataset)\n",
        "    num_batches = len(dataloader)\n",
        "    model.eval()\n",
        "    test_loss, correct = 0, 0\n",
        "    with torch.no_grad():\n",
        "        for X, y in dataloader:\n",
        "            X, y = X.to(device), y.to(device)\n",
        "            pred = model(X)\n",
        "            test_loss += loss_fn(pred, y.long()).item()\n",
        "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
        "    test_loss /= num_batches\n",
        "    correct /= size\n",
        "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"
      ],
      "metadata": {
        "id": "0bHMbhDjfTDo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 100\n",
        "bestval = 0\n",
        "decval = 0\n",
        "for t in range(epochs):\n",
        "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
        "    train(train_dataloader, model, loss_fn, optimizer)\n",
        "    #TODO VALIDATE\n",
        "    tempval = validate(val_dataloader, model, loss_fn)\n",
        "    if(tempval < bestval):\n",
        "      decval+=1 \n",
        "      if(decval==3):\n",
        "        print(\"Validation no longer rising. Done!\")\n",
        "        break\n",
        "    else:\n",
        "      bestval=tempval\n",
        "print(\"Max number of epochs hit or validation stop reached. Done!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xp77ODisfTDq",
        "outputId": "404efc76-aee5-454b-dffc-bb4bb497fb89"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1\n",
            "-------------------------------\n",
            "loss: 1.611489  [    0/20380]\n",
            "Train Error: Accuracy: 23.7%\n",
            "loss: 1.447748  [ 6400/20380]\n",
            "Train Error: Accuracy: 40.7%\n",
            "loss: 1.359314  [12800/20380]\n",
            "Train Error: Accuracy: 44.5%\n",
            "loss: 1.101026  [19200/20380]\n",
            "Train Error: Accuracy: 45.1%\n",
            "Val Error: \n",
            " Accuracy: 45.5%, Avg loss: 1.270604 \n",
            "\n",
            "Epoch 2\n",
            "-------------------------------\n",
            "loss: 1.239544  [    0/20380]\n",
            "Train Error: Accuracy: 45.3%\n",
            "loss: 1.304550  [ 6400/20380]\n",
            "Train Error: Accuracy: 45.5%\n",
            "loss: 1.296463  [12800/20380]\n",
            "Train Error: Accuracy: 46.0%\n",
            "loss: 1.017027  [19200/20380]\n",
            "Train Error: Accuracy: 46.0%\n",
            "Val Error: \n",
            " Accuracy: 45.1%, Avg loss: 1.221825 \n",
            "\n",
            "Epoch 3\n",
            "-------------------------------\n",
            "loss: 1.200430  [    0/20380]\n",
            "Train Error: Accuracy: 45.7%\n",
            "loss: 1.256796  [ 6400/20380]\n",
            "Train Error: Accuracy: 46.1%\n",
            "loss: 1.259418  [12800/20380]\n",
            "Train Error: Accuracy: 46.2%\n",
            "loss: 0.971211  [19200/20380]\n",
            "Train Error: Accuracy: 45.8%\n",
            "Val Error: \n",
            " Accuracy: 45.7%, Avg loss: 1.187727 \n",
            "\n",
            "Epoch 4\n",
            "-------------------------------\n",
            "loss: 1.178792  [    0/20380]\n",
            "Train Error: Accuracy: 45.5%\n",
            "loss: 1.217790  [ 6400/20380]\n",
            "Train Error: Accuracy: 46.0%\n",
            "loss: 1.221881  [12800/20380]\n",
            "Train Error: Accuracy: 46.3%\n",
            "loss: 0.938398  [19200/20380]\n",
            "Train Error: Accuracy: 46.5%\n",
            "Val Error: \n",
            " Accuracy: 46.2%, Avg loss: 1.155669 \n",
            "\n",
            "Epoch 5\n",
            "-------------------------------\n",
            "loss: 1.157443  [    0/20380]\n",
            "Train Error: Accuracy: 45.9%\n",
            "loss: 1.181640  [ 6400/20380]\n",
            "Train Error: Accuracy: 46.5%\n",
            "loss: 1.189185  [12800/20380]\n",
            "Train Error: Accuracy: 47.0%\n",
            "loss: 0.912166  [19200/20380]\n",
            "Train Error: Accuracy: 47.6%\n",
            "Val Error: \n",
            " Accuracy: 47.1%, Avg loss: 1.125420 \n",
            "\n",
            "Epoch 6\n",
            "-------------------------------\n",
            "loss: 1.137844  [    0/20380]\n",
            "Train Error: Accuracy: 46.6%\n",
            "loss: 1.150504  [ 6400/20380]\n",
            "Train Error: Accuracy: 47.4%\n",
            "loss: 1.164236  [12800/20380]\n",
            "Train Error: Accuracy: 48.2%\n",
            "loss: 0.890445  [19200/20380]\n",
            "Train Error: Accuracy: 48.9%\n",
            "Val Error: \n",
            " Accuracy: 49.1%, Avg loss: 1.098713 \n",
            "\n",
            "Epoch 7\n",
            "-------------------------------\n",
            "loss: 1.119984  [    0/20380]\n",
            "Train Error: Accuracy: 48.0%\n",
            "loss: 1.124475  [ 6400/20380]\n",
            "Train Error: Accuracy: 48.7%\n",
            "loss: 1.141332  [12800/20380]\n",
            "Train Error: Accuracy: 49.5%\n",
            "loss: 0.873168  [19200/20380]\n",
            "Train Error: Accuracy: 50.0%\n",
            "Val Error: \n",
            " Accuracy: 51.3%, Avg loss: 1.075733 \n",
            "\n",
            "Epoch 8\n",
            "-------------------------------\n",
            "loss: 1.103304  [    0/20380]\n",
            "Train Error: Accuracy: 49.7%\n",
            "loss: 1.099547  [ 6400/20380]\n",
            "Train Error: Accuracy: 49.7%\n",
            "loss: 1.115635  [12800/20380]\n",
            "Train Error: Accuracy: 50.8%\n",
            "loss: 0.858865  [19200/20380]\n",
            "Train Error: Accuracy: 51.0%\n",
            "Val Error: \n",
            " Accuracy: 53.0%, Avg loss: 1.054938 \n",
            "\n",
            "Epoch 9\n",
            "-------------------------------\n",
            "loss: 1.087491  [    0/20380]\n",
            "Train Error: Accuracy: 52.6%\n",
            "loss: 1.072372  [ 6400/20380]\n",
            "Train Error: Accuracy: 50.8%\n",
            "loss: 1.086782  [12800/20380]\n",
            "Train Error: Accuracy: 53.2%\n",
            "loss: 0.845472  [19200/20380]\n",
            "Train Error: Accuracy: 53.3%\n",
            "Val Error: \n",
            " Accuracy: 55.2%, Avg loss: 1.034925 \n",
            "\n",
            "Epoch 10\n",
            "-------------------------------\n",
            "loss: 1.072073  [    0/20380]\n",
            "Train Error: Accuracy: 53.9%\n",
            "loss: 1.042452  [ 6400/20380]\n",
            "Train Error: Accuracy: 53.2%\n",
            "loss: 1.055991  [12800/20380]\n",
            "Train Error: Accuracy: 55.2%\n",
            "loss: 0.832287  [19200/20380]\n",
            "Train Error: Accuracy: 55.1%\n",
            "Val Error: \n",
            " Accuracy: 56.7%, Avg loss: 1.015105 \n",
            "\n",
            "Epoch 11\n",
            "-------------------------------\n",
            "loss: 1.056361  [    0/20380]\n",
            "Train Error: Accuracy: 55.3%\n",
            "loss: 1.011256  [ 6400/20380]\n",
            "Train Error: Accuracy: 54.9%\n",
            "loss: 1.024686  [12800/20380]\n",
            "Train Error: Accuracy: 56.6%\n",
            "loss: 0.819305  [19200/20380]\n",
            "Train Error: Accuracy: 56.4%\n",
            "Val Error: \n",
            " Accuracy: 57.6%, Avg loss: 0.995505 \n",
            "\n",
            "Epoch 12\n",
            "-------------------------------\n",
            "loss: 1.039585  [    0/20380]\n",
            "Train Error: Accuracy: 56.5%\n",
            "loss: 0.980743  [ 6400/20380]\n",
            "Train Error: Accuracy: 56.2%\n",
            "loss: 0.993976  [12800/20380]\n",
            "Train Error: Accuracy: 57.8%\n",
            "loss: 0.806786  [19200/20380]\n",
            "Train Error: Accuracy: 57.5%\n",
            "Val Error: \n",
            " Accuracy: 58.5%, Avg loss: 0.976480 \n",
            "\n",
            "Epoch 13\n",
            "-------------------------------\n",
            "loss: 1.021435  [    0/20380]\n",
            "Train Error: Accuracy: 57.6%\n",
            "loss: 0.952403  [ 6400/20380]\n",
            "Train Error: Accuracy: 57.4%\n",
            "loss: 0.964737  [12800/20380]\n",
            "Train Error: Accuracy: 58.8%\n",
            "loss: 0.794954  [19200/20380]\n",
            "Train Error: Accuracy: 58.7%\n",
            "Val Error: \n",
            " Accuracy: 59.5%, Avg loss: 0.958518 \n",
            "\n",
            "Epoch 14\n",
            "-------------------------------\n",
            "loss: 1.002611  [    0/20380]\n",
            "Train Error: Accuracy: 58.7%\n",
            "loss: 0.926916  [ 6400/20380]\n",
            "Train Error: Accuracy: 58.5%\n",
            "loss: 0.938042  [12800/20380]\n",
            "Train Error: Accuracy: 59.9%\n",
            "loss: 0.783989  [19200/20380]\n",
            "Train Error: Accuracy: 59.8%\n",
            "Val Error: \n",
            " Accuracy: 60.2%, Avg loss: 0.942057 \n",
            "\n",
            "Epoch 15\n",
            "-------------------------------\n",
            "loss: 0.984329  [    0/20380]\n",
            "Train Error: Accuracy: 59.7%\n",
            "loss: 0.904081  [ 6400/20380]\n",
            "Train Error: Accuracy: 59.2%\n",
            "loss: 0.914549  [12800/20380]\n",
            "Train Error: Accuracy: 60.9%\n",
            "loss: 0.774366  [19200/20380]\n",
            "Train Error: Accuracy: 60.7%\n",
            "Val Error: \n",
            " Accuracy: 60.8%, Avg loss: 0.927220 \n",
            "\n",
            "Epoch 16\n",
            "-------------------------------\n",
            "loss: 0.967524  [    0/20380]\n",
            "Train Error: Accuracy: 60.6%\n",
            "loss: 0.882879  [ 6400/20380]\n",
            "Train Error: Accuracy: 59.9%\n",
            "loss: 0.894082  [12800/20380]\n",
            "Train Error: Accuracy: 61.8%\n",
            "loss: 0.766173  [19200/20380]\n",
            "Train Error: Accuracy: 61.3%\n",
            "Val Error: \n",
            " Accuracy: 61.6%, Avg loss: 0.913769 \n",
            "\n",
            "Epoch 17\n",
            "-------------------------------\n",
            "loss: 0.952778  [    0/20380]\n",
            "Train Error: Accuracy: 61.4%\n",
            "loss: 0.862597  [ 6400/20380]\n",
            "Train Error: Accuracy: 60.6%\n",
            "loss: 0.875879  [12800/20380]\n",
            "Train Error: Accuracy: 62.5%\n",
            "loss: 0.758942  [19200/20380]\n",
            "Train Error: Accuracy: 61.7%\n",
            "Val Error: \n",
            " Accuracy: 62.2%, Avg loss: 0.901302 \n",
            "\n",
            "Epoch 18\n",
            "-------------------------------\n",
            "loss: 0.939964  [    0/20380]\n",
            "Train Error: Accuracy: 62.0%\n",
            "loss: 0.843181  [ 6400/20380]\n",
            "Train Error: Accuracy: 61.5%\n",
            "loss: 0.859075  [12800/20380]\n",
            "Train Error: Accuracy: 63.0%\n",
            "loss: 0.752018  [19200/20380]\n",
            "Train Error: Accuracy: 62.2%\n",
            "Val Error: \n",
            " Accuracy: 62.9%, Avg loss: 0.889374 \n",
            "\n",
            "Epoch 19\n",
            "-------------------------------\n",
            "loss: 0.928470  [    0/20380]\n",
            "Train Error: Accuracy: 62.6%\n",
            "loss: 0.824561  [ 6400/20380]\n",
            "Train Error: Accuracy: 62.1%\n",
            "loss: 0.842908  [12800/20380]\n",
            "Train Error: Accuracy: 63.5%\n",
            "loss: 0.744846  [19200/20380]\n",
            "Train Error: Accuracy: 62.7%\n",
            "Val Error: \n",
            " Accuracy: 63.8%, Avg loss: 0.877524 \n",
            "\n",
            "Epoch 20\n",
            "-------------------------------\n",
            "loss: 0.917633  [    0/20380]\n",
            "Train Error: Accuracy: 63.4%\n",
            "loss: 0.806436  [ 6400/20380]\n",
            "Train Error: Accuracy: 62.7%\n",
            "loss: 0.826853  [12800/20380]\n",
            "Train Error: Accuracy: 64.1%\n",
            "loss: 0.737040  [19200/20380]\n",
            "Train Error: Accuracy: 63.4%\n",
            "Val Error: \n",
            " Accuracy: 64.4%, Avg loss: 0.865383 \n",
            "\n",
            "Epoch 21\n",
            "-------------------------------\n",
            "loss: 0.907021  [    0/20380]\n",
            "Train Error: Accuracy: 64.0%\n",
            "loss: 0.788411  [ 6400/20380]\n",
            "Train Error: Accuracy: 63.1%\n",
            "loss: 0.810640  [12800/20380]\n",
            "Train Error: Accuracy: 64.8%\n",
            "loss: 0.728502  [19200/20380]\n",
            "Train Error: Accuracy: 64.1%\n",
            "Val Error: \n",
            " Accuracy: 65.3%, Avg loss: 0.852754 \n",
            "\n",
            "Epoch 22\n",
            "-------------------------------\n",
            "loss: 0.896477  [    0/20380]\n",
            "Train Error: Accuracy: 65.2%\n",
            "loss: 0.770259  [ 6400/20380]\n",
            "Train Error: Accuracy: 63.7%\n",
            "loss: 0.794251  [12800/20380]\n",
            "Train Error: Accuracy: 65.6%\n",
            "loss: 0.719417  [19200/20380]\n",
            "Train Error: Accuracy: 65.1%\n",
            "Val Error: \n",
            " Accuracy: 66.3%, Avg loss: 0.839685 \n",
            "\n",
            "Epoch 23\n",
            "-------------------------------\n",
            "loss: 0.886088  [    0/20380]\n",
            "Train Error: Accuracy: 66.3%\n",
            "loss: 0.751963  [ 6400/20380]\n",
            "Train Error: Accuracy: 64.6%\n",
            "loss: 0.778050  [12800/20380]\n",
            "Train Error: Accuracy: 66.4%\n",
            "loss: 0.710167  [19200/20380]\n",
            "Train Error: Accuracy: 66.5%\n",
            "Val Error: \n",
            " Accuracy: 67.7%, Avg loss: 0.826480 \n",
            "\n",
            "Epoch 24\n",
            "-------------------------------\n",
            "loss: 0.876066  [    0/20380]\n",
            "Train Error: Accuracy: 67.5%\n",
            "loss: 0.733779  [ 6400/20380]\n",
            "Train Error: Accuracy: 65.9%\n",
            "loss: 0.762665  [12800/20380]\n",
            "Train Error: Accuracy: 67.0%\n",
            "loss: 0.701241  [19200/20380]\n",
            "Train Error: Accuracy: 67.5%\n",
            "Val Error: \n",
            " Accuracy: 68.4%, Avg loss: 0.813592 \n",
            "\n",
            "Epoch 25\n",
            "-------------------------------\n",
            "loss: 0.866714  [    0/20380]\n",
            "Train Error: Accuracy: 68.4%\n",
            "loss: 0.716103  [ 6400/20380]\n",
            "Train Error: Accuracy: 66.8%\n",
            "loss: 0.748793  [12800/20380]\n",
            "Train Error: Accuracy: 67.7%\n",
            "loss: 0.693110  [19200/20380]\n",
            "Train Error: Accuracy: 68.2%\n",
            "Val Error: \n",
            " Accuracy: 69.2%, Avg loss: 0.801446 \n",
            "\n",
            "Epoch 26\n",
            "-------------------------------\n",
            "loss: 0.858250  [    0/20380]\n",
            "Train Error: Accuracy: 68.9%\n",
            "loss: 0.699313  [ 6400/20380]\n",
            "Train Error: Accuracy: 67.4%\n",
            "loss: 0.736745  [12800/20380]\n",
            "Train Error: Accuracy: 68.4%\n",
            "loss: 0.686007  [19200/20380]\n",
            "Train Error: Accuracy: 68.5%\n",
            "Val Error: \n",
            " Accuracy: 69.5%, Avg loss: 0.790280 \n",
            "\n",
            "Epoch 27\n",
            "-------------------------------\n",
            "loss: 0.850854  [    0/20380]\n",
            "Train Error: Accuracy: 69.5%\n",
            "loss: 0.683586  [ 6400/20380]\n",
            "Train Error: Accuracy: 68.0%\n",
            "loss: 0.726484  [12800/20380]\n",
            "Train Error: Accuracy: 68.7%\n",
            "loss: 0.679969  [19200/20380]\n",
            "Train Error: Accuracy: 69.1%\n",
            "Val Error: \n",
            " Accuracy: 70.3%, Avg loss: 0.780066 \n",
            "\n",
            "Epoch 28\n",
            "-------------------------------\n",
            "loss: 0.844342  [    0/20380]\n",
            "Train Error: Accuracy: 70.0%\n",
            "loss: 0.668920  [ 6400/20380]\n",
            "Train Error: Accuracy: 68.7%\n",
            "loss: 0.717588  [12800/20380]\n",
            "Train Error: Accuracy: 69.2%\n",
            "loss: 0.674841  [19200/20380]\n",
            "Train Error: Accuracy: 69.5%\n",
            "Val Error: \n",
            " Accuracy: 70.6%, Avg loss: 0.770716 \n",
            "\n",
            "Epoch 29\n",
            "-------------------------------\n",
            "loss: 0.838347  [    0/20380]\n",
            "Train Error: Accuracy: 70.5%\n",
            "loss: 0.655347  [ 6400/20380]\n",
            "Train Error: Accuracy: 69.1%\n",
            "loss: 0.709611  [12800/20380]\n",
            "Train Error: Accuracy: 69.6%\n",
            "loss: 0.670512  [19200/20380]\n",
            "Train Error: Accuracy: 70.0%\n",
            "Val Error: \n",
            " Accuracy: 71.3%, Avg loss: 0.762059 \n",
            "\n",
            "Epoch 30\n",
            "-------------------------------\n",
            "loss: 0.832668  [    0/20380]\n",
            "Train Error: Accuracy: 70.9%\n",
            "loss: 0.642841  [ 6400/20380]\n",
            "Train Error: Accuracy: 69.7%\n",
            "loss: 0.702146  [12800/20380]\n",
            "Train Error: Accuracy: 70.1%\n",
            "loss: 0.666904  [19200/20380]\n",
            "Train Error: Accuracy: 70.4%\n",
            "Val Error: \n",
            " Accuracy: 71.8%, Avg loss: 0.753864 \n",
            "\n",
            "Epoch 31\n",
            "-------------------------------\n",
            "loss: 0.827384  [    0/20380]\n",
            "Train Error: Accuracy: 71.3%\n",
            "loss: 0.631274  [ 6400/20380]\n",
            "Train Error: Accuracy: 70.2%\n",
            "loss: 0.694926  [12800/20380]\n",
            "Train Error: Accuracy: 70.4%\n",
            "loss: 0.663847  [19200/20380]\n",
            "Train Error: Accuracy: 70.8%\n",
            "Val Error: \n",
            " Accuracy: 72.1%, Avg loss: 0.745927 \n",
            "\n",
            "Epoch 32\n",
            "-------------------------------\n",
            "loss: 0.822424  [    0/20380]\n",
            "Train Error: Accuracy: 71.7%\n",
            "loss: 0.620449  [ 6400/20380]\n",
            "Train Error: Accuracy: 70.8%\n",
            "loss: 0.687721  [12800/20380]\n",
            "Train Error: Accuracy: 70.8%\n",
            "loss: 0.661230  [19200/20380]\n",
            "Train Error: Accuracy: 71.1%\n",
            "Val Error: \n",
            " Accuracy: 72.4%, Avg loss: 0.738129 \n",
            "\n",
            "Epoch 33\n",
            "-------------------------------\n",
            "loss: 0.817690  [    0/20380]\n",
            "Train Error: Accuracy: 72.1%\n",
            "loss: 0.610246  [ 6400/20380]\n",
            "Train Error: Accuracy: 71.2%\n",
            "loss: 0.680487  [12800/20380]\n",
            "Train Error: Accuracy: 71.3%\n",
            "loss: 0.659010  [19200/20380]\n",
            "Train Error: Accuracy: 71.5%\n",
            "Val Error: \n",
            " Accuracy: 72.5%, Avg loss: 0.730422 \n",
            "\n",
            "Epoch 34\n",
            "-------------------------------\n",
            "loss: 0.813166  [    0/20380]\n",
            "Train Error: Accuracy: 72.5%\n",
            "loss: 0.600489  [ 6400/20380]\n",
            "Train Error: Accuracy: 71.8%\n",
            "loss: 0.673059  [12800/20380]\n",
            "Train Error: Accuracy: 71.8%\n",
            "loss: 0.657185  [19200/20380]\n",
            "Train Error: Accuracy: 71.9%\n",
            "Val Error: \n",
            " Accuracy: 72.7%, Avg loss: 0.722728 \n",
            "\n",
            "Epoch 35\n",
            "-------------------------------\n",
            "loss: 0.808841  [    0/20380]\n",
            "Train Error: Accuracy: 72.9%\n",
            "loss: 0.591080  [ 6400/20380]\n",
            "Train Error: Accuracy: 72.1%\n",
            "loss: 0.665378  [12800/20380]\n",
            "Train Error: Accuracy: 72.4%\n",
            "loss: 0.655702  [19200/20380]\n",
            "Train Error: Accuracy: 72.3%\n",
            "Val Error: \n",
            " Accuracy: 72.8%, Avg loss: 0.715011 \n",
            "\n",
            "Epoch 36\n",
            "-------------------------------\n",
            "loss: 0.804791  [    0/20380]\n",
            "Train Error: Accuracy: 73.4%\n",
            "loss: 0.581989  [ 6400/20380]\n",
            "Train Error: Accuracy: 72.6%\n",
            "loss: 0.657432  [12800/20380]\n",
            "Train Error: Accuracy: 72.9%\n",
            "loss: 0.654534  [19200/20380]\n",
            "Train Error: Accuracy: 72.8%\n",
            "Val Error: \n",
            " Accuracy: 73.1%, Avg loss: 0.707271 \n",
            "\n",
            "Epoch 37\n",
            "-------------------------------\n",
            "loss: 0.801057  [    0/20380]\n",
            "Train Error: Accuracy: 73.9%\n",
            "loss: 0.573181  [ 6400/20380]\n",
            "Train Error: Accuracy: 73.0%\n",
            "loss: 0.649252  [12800/20380]\n",
            "Train Error: Accuracy: 73.5%\n",
            "loss: 0.653686  [19200/20380]\n",
            "Train Error: Accuracy: 73.3%\n",
            "Val Error: \n",
            " Accuracy: 73.4%, Avg loss: 0.699512 \n",
            "\n",
            "Epoch 38\n",
            "-------------------------------\n",
            "loss: 0.797668  [    0/20380]\n",
            "Train Error: Accuracy: 74.3%\n",
            "loss: 0.564599  [ 6400/20380]\n",
            "Train Error: Accuracy: 73.5%\n",
            "loss: 0.640798  [12800/20380]\n",
            "Train Error: Accuracy: 73.9%\n",
            "loss: 0.653101  [19200/20380]\n",
            "Train Error: Accuracy: 73.8%\n",
            "Val Error: \n",
            " Accuracy: 74.0%, Avg loss: 0.691754 \n",
            "\n",
            "Epoch 39\n",
            "-------------------------------\n",
            "loss: 0.794638  [    0/20380]\n",
            "Train Error: Accuracy: 74.7%\n",
            "loss: 0.556149  [ 6400/20380]\n",
            "Train Error: Accuracy: 74.1%\n",
            "loss: 0.632093  [12800/20380]\n",
            "Train Error: Accuracy: 74.4%\n",
            "loss: 0.652702  [19200/20380]\n",
            "Train Error: Accuracy: 74.3%\n",
            "Val Error: \n",
            " Accuracy: 74.5%, Avg loss: 0.684034 \n",
            "\n",
            "Epoch 40\n",
            "-------------------------------\n",
            "loss: 0.792011  [    0/20380]\n",
            "Train Error: Accuracy: 75.0%\n",
            "loss: 0.547744  [ 6400/20380]\n",
            "Train Error: Accuracy: 74.6%\n",
            "loss: 0.623115  [12800/20380]\n",
            "Train Error: Accuracy: 74.8%\n",
            "loss: 0.652466  [19200/20380]\n",
            "Train Error: Accuracy: 74.6%\n",
            "Val Error: \n",
            " Accuracy: 74.6%, Avg loss: 0.676391 \n",
            "\n",
            "Epoch 41\n",
            "-------------------------------\n",
            "loss: 0.789751  [    0/20380]\n",
            "Train Error: Accuracy: 75.4%\n",
            "loss: 0.539355  [ 6400/20380]\n",
            "Train Error: Accuracy: 75.2%\n",
            "loss: 0.614012  [12800/20380]\n",
            "Train Error: Accuracy: 75.3%\n",
            "loss: 0.652312  [19200/20380]\n",
            "Train Error: Accuracy: 75.1%\n",
            "Val Error: \n",
            " Accuracy: 74.7%, Avg loss: 0.668847 \n",
            "\n",
            "Epoch 42\n",
            "-------------------------------\n",
            "loss: 0.787773  [    0/20380]\n",
            "Train Error: Accuracy: 75.8%\n",
            "loss: 0.530972  [ 6400/20380]\n",
            "Train Error: Accuracy: 75.7%\n",
            "loss: 0.604879  [12800/20380]\n",
            "Train Error: Accuracy: 75.8%\n",
            "loss: 0.652225  [19200/20380]\n",
            "Train Error: Accuracy: 75.6%\n",
            "Val Error: \n",
            " Accuracy: 75.1%, Avg loss: 0.661458 \n",
            "\n",
            "Epoch 43\n",
            "-------------------------------\n",
            "loss: 0.785878  [    0/20380]\n",
            "Train Error: Accuracy: 76.2%\n",
            "loss: 0.522589  [ 6400/20380]\n",
            "Train Error: Accuracy: 76.1%\n",
            "loss: 0.595780  [12800/20380]\n",
            "Train Error: Accuracy: 76.2%\n",
            "loss: 0.652138  [19200/20380]\n",
            "Train Error: Accuracy: 75.9%\n",
            "Val Error: \n",
            " Accuracy: 75.5%, Avg loss: 0.654247 \n",
            "\n",
            "Epoch 44\n",
            "-------------------------------\n",
            "loss: 0.783912  [    0/20380]\n",
            "Train Error: Accuracy: 76.6%\n",
            "loss: 0.514188  [ 6400/20380]\n",
            "Train Error: Accuracy: 76.6%\n",
            "loss: 0.586726  [12800/20380]\n",
            "Train Error: Accuracy: 76.6%\n",
            "loss: 0.652021  [19200/20380]\n",
            "Train Error: Accuracy: 76.3%\n",
            "Val Error: \n",
            " Accuracy: 75.9%, Avg loss: 0.647201 \n",
            "\n",
            "Epoch 45\n",
            "-------------------------------\n",
            "loss: 0.781733  [    0/20380]\n",
            "Train Error: Accuracy: 77.1%\n",
            "loss: 0.505764  [ 6400/20380]\n",
            "Train Error: Accuracy: 76.8%\n",
            "loss: 0.577736  [12800/20380]\n",
            "Train Error: Accuracy: 77.0%\n",
            "loss: 0.651777  [19200/20380]\n",
            "Train Error: Accuracy: 76.7%\n",
            "Val Error: \n",
            " Accuracy: 76.0%, Avg loss: 0.640309 \n",
            "\n",
            "Epoch 46\n",
            "-------------------------------\n",
            "loss: 0.779297  [    0/20380]\n",
            "Train Error: Accuracy: 77.5%\n",
            "loss: 0.497338  [ 6400/20380]\n",
            "Train Error: Accuracy: 77.1%\n",
            "loss: 0.568906  [12800/20380]\n",
            "Train Error: Accuracy: 77.4%\n",
            "loss: 0.651353  [19200/20380]\n",
            "Train Error: Accuracy: 77.1%\n",
            "Val Error: \n",
            " Accuracy: 76.4%, Avg loss: 0.633556 \n",
            "\n",
            "Epoch 47\n",
            "-------------------------------\n",
            "loss: 0.776570  [    0/20380]\n",
            "Train Error: Accuracy: 77.9%\n",
            "loss: 0.488896  [ 6400/20380]\n",
            "Train Error: Accuracy: 77.4%\n",
            "loss: 0.560265  [12800/20380]\n",
            "Train Error: Accuracy: 77.7%\n",
            "loss: 0.650732  [19200/20380]\n",
            "Train Error: Accuracy: 77.5%\n",
            "Val Error: \n",
            " Accuracy: 76.8%, Avg loss: 0.626945 \n",
            "\n",
            "Epoch 48\n",
            "-------------------------------\n",
            "loss: 0.773548  [    0/20380]\n",
            "Train Error: Accuracy: 78.3%\n",
            "loss: 0.480490  [ 6400/20380]\n",
            "Train Error: Accuracy: 77.6%\n",
            "loss: 0.551950  [12800/20380]\n",
            "Train Error: Accuracy: 78.1%\n",
            "loss: 0.649763  [19200/20380]\n",
            "Train Error: Accuracy: 77.9%\n",
            "Val Error: \n",
            " Accuracy: 77.4%, Avg loss: 0.620485 \n",
            "\n",
            "Epoch 49\n",
            "-------------------------------\n",
            "loss: 0.770250  [    0/20380]\n",
            "Train Error: Accuracy: 78.6%\n",
            "loss: 0.472159  [ 6400/20380]\n",
            "Train Error: Accuracy: 77.7%\n",
            "loss: 0.543991  [12800/20380]\n",
            "Train Error: Accuracy: 78.3%\n",
            "loss: 0.648426  [19200/20380]\n",
            "Train Error: Accuracy: 78.3%\n",
            "Val Error: \n",
            " Accuracy: 78.0%, Avg loss: 0.614175 \n",
            "\n",
            "Epoch 50\n",
            "-------------------------------\n",
            "loss: 0.766697  [    0/20380]\n",
            "Train Error: Accuracy: 78.8%\n",
            "loss: 0.463928  [ 6400/20380]\n",
            "Train Error: Accuracy: 77.9%\n",
            "loss: 0.536328  [12800/20380]\n",
            "Train Error: Accuracy: 78.6%\n",
            "loss: 0.646726  [19200/20380]\n",
            "Train Error: Accuracy: 78.6%\n",
            "Val Error: \n",
            " Accuracy: 78.5%, Avg loss: 0.608012 \n",
            "\n",
            "Epoch 51\n",
            "-------------------------------\n",
            "loss: 0.762876  [    0/20380]\n",
            "Train Error: Accuracy: 79.0%\n",
            "loss: 0.455830  [ 6400/20380]\n",
            "Train Error: Accuracy: 78.1%\n",
            "loss: 0.529028  [12800/20380]\n",
            "Train Error: Accuracy: 78.8%\n",
            "loss: 0.644634  [19200/20380]\n",
            "Train Error: Accuracy: 78.9%\n",
            "Val Error: \n",
            " Accuracy: 78.8%, Avg loss: 0.602000 \n",
            "\n",
            "Epoch 52\n",
            "-------------------------------\n",
            "loss: 0.758778  [    0/20380]\n",
            "Train Error: Accuracy: 79.2%\n",
            "loss: 0.447906  [ 6400/20380]\n",
            "Train Error: Accuracy: 78.2%\n",
            "loss: 0.522064  [12800/20380]\n",
            "Train Error: Accuracy: 79.1%\n",
            "loss: 0.642096  [19200/20380]\n",
            "Train Error: Accuracy: 79.1%\n",
            "Val Error: \n",
            " Accuracy: 79.2%, Avg loss: 0.596111 \n",
            "\n",
            "Epoch 53\n",
            "-------------------------------\n",
            "loss: 0.754247  [    0/20380]\n",
            "Train Error: Accuracy: 79.4%\n",
            "loss: 0.440225  [ 6400/20380]\n",
            "Train Error: Accuracy: 78.3%\n",
            "loss: 0.515429  [12800/20380]\n",
            "Train Error: Accuracy: 79.2%\n",
            "loss: 0.639264  [19200/20380]\n",
            "Train Error: Accuracy: 79.5%\n",
            "Val Error: \n",
            " Accuracy: 79.4%, Avg loss: 0.590257 \n",
            "\n",
            "Epoch 54\n",
            "-------------------------------\n",
            "loss: 0.748729  [    0/20380]\n",
            "Train Error: Accuracy: 79.5%\n",
            "loss: 0.432792  [ 6400/20380]\n",
            "Train Error: Accuracy: 78.4%\n",
            "loss: 0.509075  [12800/20380]\n",
            "Train Error: Accuracy: 79.4%\n",
            "loss: 0.636203  [19200/20380]\n",
            "Train Error: Accuracy: 79.8%\n",
            "Val Error: \n",
            " Accuracy: 79.7%, Avg loss: 0.584560 \n",
            "\n",
            "Epoch 55\n",
            "-------------------------------\n",
            "loss: 0.742233  [    0/20380]\n",
            "Train Error: Accuracy: 79.7%\n",
            "loss: 0.425343  [ 6400/20380]\n",
            "Train Error: Accuracy: 78.6%\n",
            "loss: 0.502648  [12800/20380]\n",
            "Train Error: Accuracy: 79.6%\n",
            "loss: 0.632376  [19200/20380]\n",
            "Train Error: Accuracy: 80.1%\n",
            "Val Error: \n",
            " Accuracy: 79.9%, Avg loss: 0.579158 \n",
            "\n",
            "Epoch 56\n",
            "-------------------------------\n",
            "loss: 0.737000  [    0/20380]\n",
            "Train Error: Accuracy: 79.8%\n",
            "loss: 0.418097  [ 6400/20380]\n",
            "Train Error: Accuracy: 78.8%\n",
            "loss: 0.496433  [12800/20380]\n",
            "Train Error: Accuracy: 79.8%\n",
            "loss: 0.628128  [19200/20380]\n",
            "Train Error: Accuracy: 80.4%\n",
            "Val Error: \n",
            " Accuracy: 80.2%, Avg loss: 0.573922 \n",
            "\n",
            "Epoch 57\n",
            "-------------------------------\n",
            "loss: 0.732035  [    0/20380]\n",
            "Train Error: Accuracy: 80.0%\n",
            "loss: 0.411121  [ 6400/20380]\n",
            "Train Error: Accuracy: 78.9%\n",
            "loss: 0.490467  [12800/20380]\n",
            "Train Error: Accuracy: 79.9%\n",
            "loss: 0.623600  [19200/20380]\n",
            "Train Error: Accuracy: 80.5%\n",
            "Val Error: \n",
            " Accuracy: 80.3%, Avg loss: 0.568847 \n",
            "\n",
            "Epoch 58\n",
            "-------------------------------\n",
            "loss: 0.727054  [    0/20380]\n",
            "Train Error: Accuracy: 80.1%\n",
            "loss: 0.404398  [ 6400/20380]\n",
            "Train Error: Accuracy: 79.2%\n",
            "loss: 0.484782  [12800/20380]\n",
            "Train Error: Accuracy: 80.1%\n",
            "loss: 0.618786  [19200/20380]\n",
            "Train Error: Accuracy: 80.8%\n",
            "Val Error: \n",
            " Accuracy: 80.4%, Avg loss: 0.563942 \n",
            "\n",
            "Epoch 59\n",
            "-------------------------------\n",
            "loss: 0.721985  [    0/20380]\n",
            "Train Error: Accuracy: 80.2%\n",
            "loss: 0.397915  [ 6400/20380]\n",
            "Train Error: Accuracy: 79.4%\n",
            "loss: 0.479379  [12800/20380]\n",
            "Train Error: Accuracy: 80.3%\n",
            "loss: 0.613682  [19200/20380]\n",
            "Train Error: Accuracy: 81.0%\n",
            "Val Error: \n",
            " Accuracy: 80.5%, Avg loss: 0.559200 \n",
            "\n",
            "Epoch 60\n",
            "-------------------------------\n",
            "loss: 0.716892  [    0/20380]\n",
            "Train Error: Accuracy: 80.3%\n",
            "loss: 0.391642  [ 6400/20380]\n",
            "Train Error: Accuracy: 79.7%\n",
            "loss: 0.474259  [12800/20380]\n",
            "Train Error: Accuracy: 80.4%\n",
            "loss: 0.608268  [19200/20380]\n",
            "Train Error: Accuracy: 81.3%\n",
            "Val Error: \n",
            " Accuracy: 80.8%, Avg loss: 0.554620 \n",
            "\n",
            "Epoch 61\n",
            "-------------------------------\n",
            "loss: 0.711811  [    0/20380]\n",
            "Train Error: Accuracy: 80.5%\n",
            "loss: 0.385595  [ 6400/20380]\n",
            "Train Error: Accuracy: 79.9%\n",
            "loss: 0.469372  [12800/20380]\n",
            "Train Error: Accuracy: 80.6%\n",
            "loss: 0.602620  [19200/20380]\n",
            "Train Error: Accuracy: 81.5%\n",
            "Val Error: \n",
            " Accuracy: 80.9%, Avg loss: 0.550211 \n",
            "\n",
            "Epoch 62\n",
            "-------------------------------\n",
            "loss: 0.706757  [    0/20380]\n",
            "Train Error: Accuracy: 80.6%\n",
            "loss: 0.379783  [ 6400/20380]\n",
            "Train Error: Accuracy: 80.1%\n",
            "loss: 0.464696  [12800/20380]\n",
            "Train Error: Accuracy: 80.7%\n",
            "loss: 0.596746  [19200/20380]\n",
            "Train Error: Accuracy: 81.8%\n",
            "Val Error: \n",
            " Accuracy: 81.1%, Avg loss: 0.545980 \n",
            "\n",
            "Epoch 63\n",
            "-------------------------------\n",
            "loss: 0.701775  [    0/20380]\n",
            "Train Error: Accuracy: 80.7%\n",
            "loss: 0.374228  [ 6400/20380]\n",
            "Train Error: Accuracy: 80.3%\n",
            "loss: 0.460235  [12800/20380]\n",
            "Train Error: Accuracy: 80.8%\n",
            "loss: 0.590697  [19200/20380]\n",
            "Train Error: Accuracy: 82.0%\n",
            "Val Error: \n",
            " Accuracy: 81.4%, Avg loss: 0.541925 \n",
            "\n",
            "Epoch 64\n",
            "-------------------------------\n",
            "loss: 0.696900  [    0/20380]\n",
            "Train Error: Accuracy: 80.8%\n",
            "loss: 0.368896  [ 6400/20380]\n",
            "Train Error: Accuracy: 80.5%\n",
            "loss: 0.455969  [12800/20380]\n",
            "Train Error: Accuracy: 80.9%\n",
            "loss: 0.584464  [19200/20380]\n",
            "Train Error: Accuracy: 82.1%\n",
            "Val Error: \n",
            " Accuracy: 81.6%, Avg loss: 0.538036 \n",
            "\n",
            "Epoch 65\n",
            "-------------------------------\n",
            "loss: 0.692133  [    0/20380]\n",
            "Train Error: Accuracy: 80.9%\n",
            "loss: 0.363762  [ 6400/20380]\n",
            "Train Error: Accuracy: 80.7%\n",
            "loss: 0.451869  [12800/20380]\n",
            "Train Error: Accuracy: 81.0%\n",
            "loss: 0.578102  [19200/20380]\n",
            "Train Error: Accuracy: 82.2%\n",
            "Val Error: \n",
            " Accuracy: 81.8%, Avg loss: 0.534300 \n",
            "\n",
            "Epoch 66\n",
            "-------------------------------\n",
            "loss: 0.687526  [    0/20380]\n",
            "Train Error: Accuracy: 81.0%\n",
            "loss: 0.358811  [ 6400/20380]\n",
            "Train Error: Accuracy: 80.8%\n",
            "loss: 0.447884  [12800/20380]\n",
            "Train Error: Accuracy: 81.1%\n",
            "loss: 0.571662  [19200/20380]\n",
            "Train Error: Accuracy: 82.4%\n",
            "Val Error: \n",
            " Accuracy: 82.0%, Avg loss: 0.530717 \n",
            "\n",
            "Epoch 67\n",
            "-------------------------------\n",
            "loss: 0.683089  [    0/20380]\n",
            "Train Error: Accuracy: 81.1%\n",
            "loss: 0.354011  [ 6400/20380]\n",
            "Train Error: Accuracy: 80.9%\n",
            "loss: 0.444020  [12800/20380]\n",
            "Train Error: Accuracy: 81.2%\n",
            "loss: 0.565188  [19200/20380]\n",
            "Train Error: Accuracy: 82.5%\n",
            "Val Error: \n",
            " Accuracy: 82.1%, Avg loss: 0.527281 \n",
            "\n",
            "Epoch 68\n",
            "-------------------------------\n",
            "loss: 0.678763  [    0/20380]\n",
            "Train Error: Accuracy: 81.2%\n",
            "loss: 0.349363  [ 6400/20380]\n",
            "Train Error: Accuracy: 81.1%\n",
            "loss: 0.440264  [12800/20380]\n",
            "Train Error: Accuracy: 81.3%\n",
            "loss: 0.558672  [19200/20380]\n",
            "Train Error: Accuracy: 82.7%\n",
            "Val Error: \n",
            " Accuracy: 82.3%, Avg loss: 0.523973 \n",
            "\n",
            "Epoch 69\n",
            "-------------------------------\n",
            "loss: 0.674480  [    0/20380]\n",
            "Train Error: Accuracy: 81.3%\n",
            "loss: 0.344845  [ 6400/20380]\n",
            "Train Error: Accuracy: 81.3%\n",
            "loss: 0.436647  [12800/20380]\n",
            "Train Error: Accuracy: 81.4%\n",
            "loss: 0.552058  [19200/20380]\n",
            "Train Error: Accuracy: 82.8%\n",
            "Val Error: \n",
            " Accuracy: 82.4%, Avg loss: 0.520772 \n",
            "\n",
            "Epoch 70\n",
            "-------------------------------\n",
            "loss: 0.670208  [    0/20380]\n",
            "Train Error: Accuracy: 81.5%\n",
            "loss: 0.340533  [ 6400/20380]\n",
            "Train Error: Accuracy: 81.4%\n",
            "loss: 0.433128  [12800/20380]\n",
            "Train Error: Accuracy: 81.6%\n",
            "loss: 0.545345  [19200/20380]\n",
            "Train Error: Accuracy: 82.9%\n",
            "Val Error: \n",
            " Accuracy: 82.5%, Avg loss: 0.517676 \n",
            "\n",
            "Epoch 71\n",
            "-------------------------------\n",
            "loss: 0.665859  [    0/20380]\n",
            "Train Error: Accuracy: 81.6%\n",
            "loss: 0.336414  [ 6400/20380]\n",
            "Train Error: Accuracy: 81.5%\n",
            "loss: 0.429677  [12800/20380]\n",
            "Train Error: Accuracy: 81.7%\n",
            "loss: 0.538544  [19200/20380]\n",
            "Train Error: Accuracy: 83.1%\n",
            "Val Error: \n",
            " Accuracy: 82.5%, Avg loss: 0.514678 \n",
            "\n",
            "Epoch 72\n",
            "-------------------------------\n",
            "loss: 0.661519  [    0/20380]\n",
            "Train Error: Accuracy: 81.6%\n",
            "loss: 0.332492  [ 6400/20380]\n",
            "Train Error: Accuracy: 81.7%\n",
            "loss: 0.426300  [12800/20380]\n",
            "Train Error: Accuracy: 81.8%\n",
            "loss: 0.531781  [19200/20380]\n",
            "Train Error: Accuracy: 83.2%\n",
            "Val Error: \n",
            " Accuracy: 82.6%, Avg loss: 0.511789 \n",
            "\n",
            "Epoch 73\n",
            "-------------------------------\n",
            "loss: 0.657299  [    0/20380]\n",
            "Train Error: Accuracy: 81.8%\n",
            "loss: 0.328724  [ 6400/20380]\n",
            "Train Error: Accuracy: 81.8%\n",
            "loss: 0.422985  [12800/20380]\n",
            "Train Error: Accuracy: 82.0%\n",
            "loss: 0.525071  [19200/20380]\n",
            "Train Error: Accuracy: 83.3%\n",
            "Val Error: \n",
            " Accuracy: 82.8%, Avg loss: 0.509011 \n",
            "\n",
            "Epoch 74\n",
            "-------------------------------\n",
            "loss: 0.653233  [    0/20380]\n",
            "Train Error: Accuracy: 81.9%\n",
            "loss: 0.325121  [ 6400/20380]\n",
            "Train Error: Accuracy: 81.9%\n",
            "loss: 0.419726  [12800/20380]\n",
            "Train Error: Accuracy: 82.1%\n",
            "loss: 0.518414  [19200/20380]\n",
            "Train Error: Accuracy: 83.4%\n",
            "Val Error: \n",
            " Accuracy: 82.8%, Avg loss: 0.506353 \n",
            "\n",
            "Epoch 75\n",
            "-------------------------------\n",
            "loss: 0.649382  [    0/20380]\n",
            "Train Error: Accuracy: 82.0%\n",
            "loss: 0.321688  [ 6400/20380]\n",
            "Train Error: Accuracy: 82.0%\n",
            "loss: 0.416537  [12800/20380]\n",
            "Train Error: Accuracy: 82.2%\n",
            "loss: 0.511872  [19200/20380]\n",
            "Train Error: Accuracy: 83.5%\n",
            "Val Error: \n",
            " Accuracy: 82.8%, Avg loss: 0.503804 \n",
            "\n",
            "Epoch 76\n",
            "-------------------------------\n",
            "loss: 0.645740  [    0/20380]\n",
            "Train Error: Accuracy: 82.1%\n",
            "loss: 0.318362  [ 6400/20380]\n",
            "Train Error: Accuracy: 82.1%\n",
            "loss: 0.413424  [12800/20380]\n",
            "Train Error: Accuracy: 82.3%\n",
            "loss: 0.505441  [19200/20380]\n",
            "Train Error: Accuracy: 83.6%\n",
            "Val Error: \n",
            " Accuracy: 83.0%, Avg loss: 0.501359 \n",
            "\n",
            "Epoch 77\n",
            "-------------------------------\n",
            "loss: 0.642252  [    0/20380]\n",
            "Train Error: Accuracy: 82.1%\n",
            "loss: 0.315035  [ 6400/20380]\n",
            "Train Error: Accuracy: 82.2%\n",
            "loss: 0.410318  [12800/20380]\n",
            "Train Error: Accuracy: 82.5%\n",
            "loss: 0.499144  [19200/20380]\n",
            "Train Error: Accuracy: 83.7%\n",
            "Val Error: \n",
            " Accuracy: 83.1%, Avg loss: 0.499018 \n",
            "\n",
            "Epoch 78\n",
            "-------------------------------\n",
            "loss: 0.638870  [    0/20380]\n",
            "Train Error: Accuracy: 82.2%\n",
            "loss: 0.311657  [ 6400/20380]\n",
            "Train Error: Accuracy: 82.3%\n",
            "loss: 0.407186  [12800/20380]\n",
            "Train Error: Accuracy: 82.6%\n",
            "loss: 0.493035  [19200/20380]\n",
            "Train Error: Accuracy: 83.8%\n",
            "Val Error: \n",
            " Accuracy: 83.2%, Avg loss: 0.496780 \n",
            "\n",
            "Epoch 79\n",
            "-------------------------------\n",
            "loss: 0.635537  [    0/20380]\n",
            "Train Error: Accuracy: 82.2%\n",
            "loss: 0.308153  [ 6400/20380]\n",
            "Train Error: Accuracy: 82.4%\n",
            "loss: 0.403987  [12800/20380]\n",
            "Train Error: Accuracy: 82.8%\n",
            "loss: 0.487277  [19200/20380]\n",
            "Train Error: Accuracy: 84.0%\n",
            "Val Error: \n",
            " Accuracy: 83.2%, Avg loss: 0.494666 \n",
            "\n",
            "Epoch 80\n",
            "-------------------------------\n",
            "loss: 0.632297  [    0/20380]\n",
            "Train Error: Accuracy: 82.3%\n",
            "loss: 0.304673  [ 6400/20380]\n",
            "Train Error: Accuracy: 82.5%\n",
            "loss: 0.400815  [12800/20380]\n",
            "Train Error: Accuracy: 82.9%\n",
            "loss: 0.481813  [19200/20380]\n",
            "Train Error: Accuracy: 84.1%\n",
            "Val Error: \n",
            " Accuracy: 83.3%, Avg loss: 0.492659 \n",
            "\n",
            "Epoch 81\n",
            "-------------------------------\n",
            "loss: 0.629189  [    0/20380]\n",
            "Train Error: Accuracy: 82.4%\n",
            "loss: 0.301347  [ 6400/20380]\n",
            "Train Error: Accuracy: 82.6%\n",
            "loss: 0.397693  [12800/20380]\n",
            "Train Error: Accuracy: 83.0%\n",
            "loss: 0.476557  [19200/20380]\n",
            "Train Error: Accuracy: 84.2%\n",
            "Val Error: \n",
            " Accuracy: 83.3%, Avg loss: 0.490735 \n",
            "\n",
            "Epoch 82\n",
            "-------------------------------\n",
            "loss: 0.626290  [    0/20380]\n",
            "Train Error: Accuracy: 82.4%\n",
            "loss: 0.298166  [ 6400/20380]\n",
            "Train Error: Accuracy: 82.7%\n",
            "loss: 0.394638  [12800/20380]\n",
            "Train Error: Accuracy: 83.1%\n",
            "loss: 0.471452  [19200/20380]\n",
            "Train Error: Accuracy: 84.2%\n",
            "Val Error: \n",
            " Accuracy: 83.5%, Avg loss: 0.488880 \n",
            "\n",
            "Epoch 83\n",
            "-------------------------------\n",
            "loss: 0.623668  [    0/20380]\n",
            "Train Error: Accuracy: 82.5%\n",
            "loss: 0.295099  [ 6400/20380]\n",
            "Train Error: Accuracy: 82.7%\n",
            "loss: 0.391678  [12800/20380]\n",
            "Train Error: Accuracy: 83.1%\n",
            "loss: 0.466432  [19200/20380]\n",
            "Train Error: Accuracy: 84.2%\n",
            "Val Error: \n",
            " Accuracy: 83.5%, Avg loss: 0.487075 \n",
            "\n",
            "Epoch 84\n",
            "-------------------------------\n",
            "loss: 0.621253  [    0/20380]\n",
            "Train Error: Accuracy: 82.5%\n",
            "loss: 0.292155  [ 6400/20380]\n",
            "Train Error: Accuracy: 82.8%\n",
            "loss: 0.388853  [12800/20380]\n",
            "Train Error: Accuracy: 83.2%\n",
            "loss: 0.461459  [19200/20380]\n",
            "Train Error: Accuracy: 84.3%\n",
            "Val Error: \n",
            " Accuracy: 83.7%, Avg loss: 0.485322 \n",
            "\n",
            "Epoch 85\n",
            "-------------------------------\n",
            "loss: 0.618985  [    0/20380]\n",
            "Train Error: Accuracy: 82.6%\n",
            "loss: 0.289272  [ 6400/20380]\n",
            "Train Error: Accuracy: 82.8%\n",
            "loss: 0.386153  [12800/20380]\n",
            "Train Error: Accuracy: 83.3%\n",
            "loss: 0.456524  [19200/20380]\n",
            "Train Error: Accuracy: 84.4%\n",
            "Val Error: \n",
            " Accuracy: 83.7%, Avg loss: 0.483612 \n",
            "\n",
            "Epoch 86\n",
            "-------------------------------\n",
            "loss: 0.616787  [    0/20380]\n",
            "Train Error: Accuracy: 82.7%\n",
            "loss: 0.286444  [ 6400/20380]\n",
            "Train Error: Accuracy: 82.9%\n",
            "loss: 0.383586  [12800/20380]\n",
            "Train Error: Accuracy: 83.4%\n",
            "loss: 0.451621  [19200/20380]\n",
            "Train Error: Accuracy: 84.4%\n",
            "Val Error: \n",
            " Accuracy: 83.9%, Avg loss: 0.481948 \n",
            "\n",
            "Epoch 87\n",
            "-------------------------------\n",
            "loss: 0.614623  [    0/20380]\n",
            "Train Error: Accuracy: 82.8%\n",
            "loss: 0.283629  [ 6400/20380]\n",
            "Train Error: Accuracy: 83.0%\n",
            "loss: 0.381116  [12800/20380]\n",
            "Train Error: Accuracy: 83.6%\n",
            "loss: 0.446741  [19200/20380]\n",
            "Train Error: Accuracy: 84.5%\n",
            "Val Error: \n",
            " Accuracy: 84.0%, Avg loss: 0.480343 \n",
            "\n",
            "Epoch 88\n",
            "-------------------------------\n",
            "loss: 0.612474  [    0/20380]\n",
            "Train Error: Accuracy: 82.9%\n",
            "loss: 0.280885  [ 6400/20380]\n",
            "Train Error: Accuracy: 83.1%\n",
            "loss: 0.378728  [12800/20380]\n",
            "Train Error: Accuracy: 83.7%\n",
            "loss: 0.441917  [19200/20380]\n",
            "Train Error: Accuracy: 84.5%\n",
            "Val Error: \n",
            " Accuracy: 84.1%, Avg loss: 0.478796 \n",
            "\n",
            "Epoch 89\n",
            "-------------------------------\n",
            "loss: 0.610302  [    0/20380]\n",
            "Train Error: Accuracy: 83.0%\n",
            "loss: 0.278218  [ 6400/20380]\n",
            "Train Error: Accuracy: 83.2%\n",
            "loss: 0.376411  [12800/20380]\n",
            "Train Error: Accuracy: 83.8%\n",
            "loss: 0.437186  [19200/20380]\n",
            "Train Error: Accuracy: 84.6%\n",
            "Val Error: \n",
            " Accuracy: 84.2%, Avg loss: 0.477291 \n",
            "\n",
            "Epoch 90\n",
            "-------------------------------\n",
            "loss: 0.608230  [    0/20380]\n",
            "Train Error: Accuracy: 83.1%\n",
            "loss: 0.275618  [ 6400/20380]\n",
            "Train Error: Accuracy: 83.3%\n",
            "loss: 0.374183  [12800/20380]\n",
            "Train Error: Accuracy: 83.9%\n",
            "loss: 0.432556  [19200/20380]\n",
            "Train Error: Accuracy: 84.7%\n",
            "Val Error: \n",
            " Accuracy: 84.2%, Avg loss: 0.475848 \n",
            "\n",
            "Epoch 91\n",
            "-------------------------------\n",
            "loss: 0.606294  [    0/20380]\n",
            "Train Error: Accuracy: 83.1%\n",
            "loss: 0.273081  [ 6400/20380]\n",
            "Train Error: Accuracy: 83.4%\n",
            "loss: 0.372052  [12800/20380]\n",
            "Train Error: Accuracy: 84.0%\n",
            "loss: 0.428044  [19200/20380]\n",
            "Train Error: Accuracy: 84.7%\n",
            "Val Error: \n",
            " Accuracy: 84.3%, Avg loss: 0.474476 \n",
            "\n",
            "Epoch 92\n",
            "-------------------------------\n",
            "loss: 0.604459  [    0/20380]\n",
            "Train Error: Accuracy: 83.2%\n",
            "loss: 0.270608  [ 6400/20380]\n",
            "Train Error: Accuracy: 83.4%\n",
            "loss: 0.370005  [12800/20380]\n",
            "Train Error: Accuracy: 84.1%\n",
            "loss: 0.423639  [19200/20380]\n",
            "Train Error: Accuracy: 84.8%\n",
            "Val Error: \n",
            " Accuracy: 84.3%, Avg loss: 0.473166 \n",
            "\n",
            "Epoch 93\n",
            "-------------------------------\n",
            "loss: 0.602699  [    0/20380]\n",
            "Train Error: Accuracy: 83.2%\n",
            "loss: 0.268202  [ 6400/20380]\n",
            "Train Error: Accuracy: 83.5%\n",
            "loss: 0.368037  [12800/20380]\n",
            "Train Error: Accuracy: 84.1%\n",
            "loss: 0.419329  [19200/20380]\n",
            "Train Error: Accuracy: 84.9%\n",
            "Val Error: \n",
            " Accuracy: 84.3%, Avg loss: 0.471913 \n",
            "\n",
            "Validation no longer rising. Done!\n",
            "Max number of epochs hit or validation stop reached. Done!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Hyperparameter Study 2: Number of Layers"
      ],
      "metadata": {
        "id": "txD-LielSRmp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##2A: 3 Hidden Layers"
      ],
      "metadata": {
        "id": "hZ1y0vqxSV6T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#check what device we're using\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"Using {device} device\")\n",
        "torch.set_grad_enabled(True) "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bdfafa10-9f75-40b9-aace-bd53724bd222",
        "id": "oc71q0x4kj24"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using cpu device\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch.autograd.grad_mode.set_grad_enabled at 0x7f260f23b9d0>"
            ]
          },
          "metadata": {},
          "execution_count": 102
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class NeuralNetwork(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.linear_relu_stack = nn.Sequential(\n",
        "            nn.Linear(7, 256), #input layer \n",
        "            nn.ELU(), # relu for simplicity?\n",
        "            nn.Linear(256, 256), #hidden layer 1\n",
        "            nn.ELU(),\n",
        "            nn.Linear(256, 256), #hidden layer 2\n",
        "            nn.ELU(),\n",
        "            nn.Linear(256, 256),\n",
        "            nn.ELU(),\n",
        "            nn.Linear(256, 5), #output layer\n",
        "        )\n",
        "    # \"Every nn.Module subclass implements the operations \n",
        "    # on input data in the forward method.\"\n",
        "    # We already preprocessed the input data\n",
        "    # so we just return x\n",
        "    def forward(self, x): \n",
        "        logits = self.linear_relu_stack(x) #we already preprocessed the data, so we just pass it into the NN here\n",
        "        return logits"
      ],
      "metadata": {
        "id": "TMRwRPVhkj27"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Implement forward propagation (clearly describe the activation functions and other\n",
        "hyper-parameters you are using)."
      ],
      "metadata": {
        "id": "Gx2m9MWxkj28"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = NeuralNetwork().to(device)\n",
        "print(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d0fb0dfb-f023-4f8e-bd0f-9f684019bcae",
        "id": "66P7g1TPkj2-"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NeuralNetwork(\n",
            "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
            "  (linear_relu_stack): Sequential(\n",
            "    (0): Linear(in_features=7, out_features=256, bias=True)\n",
            "    (1): ELU(alpha=1.0)\n",
            "    (2): Linear(in_features=256, out_features=256, bias=True)\n",
            "    (3): ELU(alpha=1.0)\n",
            "    (4): Linear(in_features=256, out_features=256, bias=True)\n",
            "    (5): ELU(alpha=1.0)\n",
            "    (6): Linear(in_features=256, out_features=256, bias=True)\n",
            "    (7): ELU(alpha=1.0)\n",
            "    (8): Linear(in_features=256, out_features=5, bias=True)\n",
            "  )\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "4. Compute the final cost function.\n",
        "\n",
        "5. Implement gradient descent (any variant of gradient descent depending upon your\n",
        "data and project can be used) to train your model. In this step it is up to you as someone\n",
        "in charge of their project to improvise using optimization algorithms (Adams, RMSProp\n",
        "etc.) and/or regularization."
      ],
      "metadata": {
        "id": "tQtIHV-Rkj2-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=1e-2) "
      ],
      "metadata": {
        "id": "F1C-hmlNkj2_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Code snippet attribution:\n",
        "#https://pytorch.org/tutorials/beginner/basics/intro.html\n",
        "#was used as part of the research process\n",
        "#Modifications were made so that data could be processed as longs,\n",
        "#additional statitics could be reported, \n",
        "#and a separate validate process was created. \n",
        "\n",
        "def train(dataloader, model, loss_fn, optimizer):\n",
        "    size = len(dataloader.dataset)\n",
        "    model.train()\n",
        "    for batch, (X, y) in enumerate(dataloader):\n",
        "        X, y = X.to(device), y.to(device)\n",
        "\n",
        "        # Compute prediction error\n",
        "        pred = model(X)\n",
        "        loss = loss_fn(pred, y.long())\n",
        "\n",
        "        # Backpropagation\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if batch % 100 == 0:\n",
        "            loss, current = loss.item(), batch * len(X)\n",
        "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
        "            correct = 0\n",
        "            with torch.no_grad():\n",
        "                for X, y in dataloader:\n",
        "                    X, y = X.to(device), y.to(device)\n",
        "                    pred = model(X)\n",
        "                    correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
        "            correct /= size\n",
        "            print(f\"Train Error: Accuracy: {(100*correct):>0.1f}%\")\n",
        "\n",
        "\n",
        "def validate(dataloader, model, loss_fn):\n",
        "    size = len(dataloader.dataset)\n",
        "    num_batches = len(dataloader)\n",
        "    model.eval()\n",
        "    test_loss, correct = 0, 0\n",
        "    with torch.no_grad():\n",
        "        for X, y in dataloader:\n",
        "            X, y = X.to(device), y.to(device)\n",
        "            pred = model(X)\n",
        "            test_loss += loss_fn(pred, y.long()).item()\n",
        "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
        "    test_loss /= num_batches\n",
        "    correct /= size\n",
        "    print(f\"Val Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n",
        "    return correct\n",
        "\n",
        "def test(dataloader, model, loss_fn):\n",
        "    size = len(dataloader.dataset)\n",
        "    num_batches = len(dataloader)\n",
        "    model.eval()\n",
        "    test_loss, correct = 0, 0\n",
        "    with torch.no_grad():\n",
        "        for X, y in dataloader:\n",
        "            X, y = X.to(device), y.to(device)\n",
        "            pred = model(X)\n",
        "            test_loss += loss_fn(pred, y.long()).item()\n",
        "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
        "    test_loss /= num_batches\n",
        "    correct /= size\n",
        "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"
      ],
      "metadata": {
        "id": "tW-4Mv_9kj3A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 100\n",
        "bestval = 0\n",
        "decval = 0\n",
        "for t in range(epochs):\n",
        "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
        "    train(train_dataloader, model, loss_fn, optimizer)\n",
        "    tempval = validate(val_dataloader, model, loss_fn)\n",
        "    if(tempval < bestval):\n",
        "      decval+=1 \n",
        "      if(decval==3):\n",
        "        print(\"Validation no longer rising. Done!\")\n",
        "        break\n",
        "    else:\n",
        "      bestval=tempval\n",
        "print(\"Max number of epochs hit or validation stop reached. Done!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9a96404b-b49c-4e46-c4d9-468f8a9d4160",
        "id": "90hGtT0Zkj3B"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1\n",
            "-------------------------------\n",
            "loss: 1.620467  [    0/20380]\n",
            "Train Error: Accuracy: 21.3%\n",
            "loss: 1.505602  [ 6400/20380]\n",
            "Train Error: Accuracy: 47.4%\n",
            "loss: 1.401324  [12800/20380]\n",
            "Train Error: Accuracy: 45.2%\n",
            "loss: 1.161096  [19200/20380]\n",
            "Train Error: Accuracy: 46.3%\n",
            "Val Error: \n",
            " Accuracy: 46.6%, Avg loss: 1.299278 \n",
            "\n",
            "Epoch 2\n",
            "-------------------------------\n",
            "loss: 1.262424  [    0/20380]\n",
            "Train Error: Accuracy: 46.5%\n",
            "loss: 1.339059  [ 6400/20380]\n",
            "Train Error: Accuracy: 45.9%\n",
            "loss: 1.308021  [12800/20380]\n",
            "Train Error: Accuracy: 46.0%\n",
            "loss: 1.039704  [19200/20380]\n",
            "Train Error: Accuracy: 46.5%\n",
            "Val Error: \n",
            " Accuracy: 46.3%, Avg loss: 1.235762 \n",
            "\n",
            "Epoch 3\n",
            "-------------------------------\n",
            "loss: 1.201532  [    0/20380]\n",
            "Train Error: Accuracy: 46.6%\n",
            "loss: 1.285263  [ 6400/20380]\n",
            "Train Error: Accuracy: 46.2%\n",
            "loss: 1.276472  [12800/20380]\n",
            "Train Error: Accuracy: 46.7%\n",
            "loss: 0.984694  [19200/20380]\n",
            "Train Error: Accuracy: 46.6%\n",
            "Val Error: \n",
            " Accuracy: 45.8%, Avg loss: 1.199696 \n",
            "\n",
            "Epoch 4\n",
            "-------------------------------\n",
            "loss: 1.183826  [    0/20380]\n",
            "Train Error: Accuracy: 46.3%\n",
            "loss: 1.248020  [ 6400/20380]\n",
            "Train Error: Accuracy: 46.3%\n",
            "loss: 1.245000  [12800/20380]\n",
            "Train Error: Accuracy: 46.8%\n",
            "loss: 0.943083  [19200/20380]\n",
            "Train Error: Accuracy: 46.7%\n",
            "Val Error: \n",
            " Accuracy: 46.1%, Avg loss: 1.166209 \n",
            "\n",
            "Validation no longer rising. Done!\n",
            "Max number of epochs hit or validation stop reached. Done!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##2B: 5 Hidden Layers"
      ],
      "metadata": {
        "id": "D23j0ukJkply"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#check what device we're using\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"Using {device} device\")\n",
        "torch.set_grad_enabled(True) "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "506c3657-3d9b-4778-e60f-135c05d256ac",
        "id": "EwU7aXwZkplz"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using cpu device\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch.autograd.grad_mode.set_grad_enabled at 0x7f260f23fa50>"
            ]
          },
          "metadata": {},
          "execution_count": 109
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class NeuralNetwork(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.linear_relu_stack = nn.Sequential(\n",
        "            nn.Linear(7, 256), #input layer \n",
        "            nn.ELU(), # relu for simplicity?\n",
        "            nn.Linear(256, 256), #hidden layer 1\n",
        "            nn.ELU(),\n",
        "            nn.Linear(256, 256), #hidden layer 2\n",
        "            nn.ELU(),\n",
        "            nn.Linear(256, 256),\n",
        "            nn.ELU(),\n",
        "            nn.Linear(256, 256),\n",
        "            nn.ELU(),\n",
        "            nn.Linear(256, 256),\n",
        "            nn.ELU(),\n",
        "            nn.Linear(256, 5), #output layer\n",
        "        )\n",
        "    # \"Every nn.Module subclass implements the operations \n",
        "    # on input data in the forward method.\"\n",
        "    # We already preprocessed the input data\n",
        "    # so we just return x\n",
        "    def forward(self, x): \n",
        "        logits = self.linear_relu_stack(x) #we already preprocessed the data, so we just pass it into the NN here\n",
        "        return logits"
      ],
      "metadata": {
        "id": "b-Vwq1O1kpl2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Implement forward propagation (clearly describe the activation functions and other\n",
        "hyper-parameters you are using)."
      ],
      "metadata": {
        "id": "COC_7GHSkpl3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = NeuralNetwork().to(device)\n",
        "print(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "255612ad-4180-4f22-9502-0e86f73033bf",
        "id": "jTMX-0NBkpl6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NeuralNetwork(\n",
            "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
            "  (linear_relu_stack): Sequential(\n",
            "    (0): Linear(in_features=7, out_features=256, bias=True)\n",
            "    (1): ELU(alpha=1.0)\n",
            "    (2): Linear(in_features=256, out_features=256, bias=True)\n",
            "    (3): ELU(alpha=1.0)\n",
            "    (4): Linear(in_features=256, out_features=256, bias=True)\n",
            "    (5): ELU(alpha=1.0)\n",
            "    (6): Linear(in_features=256, out_features=256, bias=True)\n",
            "    (7): ELU(alpha=1.0)\n",
            "    (8): Linear(in_features=256, out_features=256, bias=True)\n",
            "    (9): ELU(alpha=1.0)\n",
            "    (10): Linear(in_features=256, out_features=256, bias=True)\n",
            "    (11): ELU(alpha=1.0)\n",
            "    (12): Linear(in_features=256, out_features=5, bias=True)\n",
            "  )\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "4. Compute the final cost function.\n",
        "\n",
        "5. Implement gradient descent (any variant of gradient descent depending upon your\n",
        "data and project can be used) to train your model. In this step it is up to you as someone\n",
        "in charge of their project to improvise using optimization algorithms (Adams, RMSProp\n",
        "etc.) and/or regularization."
      ],
      "metadata": {
        "id": "8evEGDRikpl7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=1e-2) "
      ],
      "metadata": {
        "id": "QYR95lLdkpl8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(dataloader, model, loss_fn, optimizer):\n",
        "    size = len(dataloader.dataset)\n",
        "    model.train()\n",
        "    for batch, (X, y) in enumerate(dataloader):\n",
        "        X, y = X.to(device), y.to(device)\n",
        "\n",
        "        # Compute prediction error\n",
        "        pred = model(X)\n",
        "        #print(pred.size())\n",
        "        #print(y.size())\n",
        "        #predtrans = torch.transpose(pred, 0, 1)\n",
        "        #print(predtrans.size())\n",
        "        #loss = loss_fn(predtrans, y)\n",
        "        loss = loss_fn(pred, y.long()) #fix 1\n",
        "        #loss.requires_grad = True #fix 2\n",
        "\n",
        "        # Backpropagation\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if batch % 100 == 0:\n",
        "            loss, current = loss.item(), batch * len(X)\n",
        "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
        "            correct = 0\n",
        "            with torch.no_grad():\n",
        "                for X, y in dataloader:\n",
        "                    X, y = X.to(device), y.to(device)\n",
        "                    pred = model(X)\n",
        "                    correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
        "            correct /= size\n",
        "            print(f\"Train Error: Accuracy: {(100*correct):>0.1f}%\")\n",
        "\n",
        "\n",
        "def validate(dataloader, model, loss_fn):\n",
        "    size = len(dataloader.dataset)\n",
        "    num_batches = len(dataloader)\n",
        "    model.eval()\n",
        "    test_loss, correct = 0, 0\n",
        "    with torch.no_grad():\n",
        "        for X, y in dataloader:\n",
        "            X, y = X.to(device), y.to(device)\n",
        "            pred = model(X)\n",
        "            test_loss += loss_fn(pred, y.long()).item()\n",
        "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
        "    test_loss /= num_batches\n",
        "    correct /= size\n",
        "    print(f\"Val Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n",
        "    return correct\n",
        "\n",
        "def test(dataloader, model, loss_fn):\n",
        "    size = len(dataloader.dataset)\n",
        "    num_batches = len(dataloader)\n",
        "    model.eval()\n",
        "    test_loss, correct = 0, 0\n",
        "    with torch.no_grad():\n",
        "        for X, y in dataloader:\n",
        "            X, y = X.to(device), y.to(device)\n",
        "            pred = model(X)\n",
        "            test_loss += loss_fn(pred, y.long()).item()\n",
        "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
        "    test_loss /= num_batches\n",
        "    correct /= size\n",
        "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"
      ],
      "metadata": {
        "id": "RILT1mqrkpl9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 100\n",
        "bestval = 0\n",
        "decval = 0\n",
        "for t in range(epochs):\n",
        "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
        "    train(train_dataloader, model, loss_fn, optimizer)\n",
        "    #TODO VALIDATE\n",
        "    tempval = validate(val_dataloader, model, loss_fn)\n",
        "    if(tempval < bestval):\n",
        "      decval+=1 \n",
        "      if(decval==3):\n",
        "        print(\"Validation no longer rising. Done!\")\n",
        "        break\n",
        "    else:\n",
        "      bestval=tempval\n",
        "    #TODO ESSAY\n",
        "    #TODO ACTIVATION JUSTIFICATION\n",
        "    #TODO ALT\n",
        "print(\"Max number of epochs hit or validation stop reached. Done!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c390955b-9f79-4cd7-cba3-ba211afabf29",
        "id": "XL0swhwckpl_"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1\n",
            "-------------------------------\n",
            "loss: 1.601836  [    0/20380]\n",
            "Train Error: Accuracy: 17.5%\n",
            "loss: 1.602398  [ 6400/20380]\n",
            "Train Error: Accuracy: 36.3%\n",
            "loss: 1.573842  [12800/20380]\n",
            "Train Error: Accuracy: 40.7%\n",
            "loss: 1.467060  [19200/20380]\n",
            "Train Error: Accuracy: 40.9%\n",
            "Val Error: \n",
            " Accuracy: 39.8%, Avg loss: 1.492200 \n",
            "\n",
            "Epoch 2\n",
            "-------------------------------\n",
            "loss: 1.473192  [    0/20380]\n",
            "Train Error: Accuracy: 38.9%\n",
            "loss: 1.479681  [ 6400/20380]\n",
            "Train Error: Accuracy: 37.4%\n",
            "loss: 1.411127  [12800/20380]\n",
            "Train Error: Accuracy: 41.0%\n",
            "loss: 1.186414  [19200/20380]\n",
            "Train Error: Accuracy: 43.9%\n",
            "Val Error: \n",
            " Accuracy: 44.2%, Avg loss: 1.314579 \n",
            "\n",
            "Epoch 3\n",
            "-------------------------------\n",
            "loss: 1.246724  [    0/20380]\n",
            "Train Error: Accuracy: 43.6%\n",
            "loss: 1.349205  [ 6400/20380]\n",
            "Train Error: Accuracy: 44.3%\n",
            "loss: 1.313642  [12800/20380]\n",
            "Train Error: Accuracy: 46.5%\n",
            "loss: 1.056104  [19200/20380]\n",
            "Train Error: Accuracy: 46.6%\n",
            "Val Error: \n",
            " Accuracy: 46.8%, Avg loss: 1.236637 \n",
            "\n",
            "Epoch 4\n",
            "-------------------------------\n",
            "loss: 1.188188  [    0/20380]\n",
            "Train Error: Accuracy: 46.6%\n",
            "loss: 1.274974  [ 6400/20380]\n",
            "Train Error: Accuracy: 46.5%\n",
            "loss: 1.252337  [12800/20380]\n",
            "Train Error: Accuracy: 47.4%\n",
            "loss: 0.976451  [19200/20380]\n",
            "Train Error: Accuracy: 47.2%\n",
            "Val Error: \n",
            " Accuracy: 47.1%, Avg loss: 1.179123 \n",
            "\n",
            "Epoch 5\n",
            "-------------------------------\n",
            "loss: 1.171626  [    0/20380]\n",
            "Train Error: Accuracy: 47.2%\n",
            "loss: 1.223415  [ 6400/20380]\n",
            "Train Error: Accuracy: 47.2%\n",
            "loss: 1.213056  [12800/20380]\n",
            "Train Error: Accuracy: 48.9%\n",
            "loss: 0.917327  [19200/20380]\n",
            "Train Error: Accuracy: 49.1%\n",
            "Val Error: \n",
            " Accuracy: 49.1%, Avg loss: 1.133821 \n",
            "\n",
            "Epoch 6\n",
            "-------------------------------\n",
            "loss: 1.159351  [    0/20380]\n",
            "Train Error: Accuracy: 48.9%\n",
            "loss: 1.200228  [ 6400/20380]\n",
            "Train Error: Accuracy: 49.9%\n",
            "loss: 1.184888  [12800/20380]\n",
            "Train Error: Accuracy: 50.7%\n",
            "loss: 0.895475  [19200/20380]\n",
            "Train Error: Accuracy: 50.1%\n",
            "Val Error: \n",
            " Accuracy: 50.5%, Avg loss: 1.107841 \n",
            "\n",
            "Epoch 7\n",
            "-------------------------------\n",
            "loss: 1.144352  [    0/20380]\n",
            "Train Error: Accuracy: 49.5%\n",
            "loss: 1.193761  [ 6400/20380]\n",
            "Train Error: Accuracy: 50.6%\n",
            "loss: 1.153171  [12800/20380]\n",
            "Train Error: Accuracy: 51.0%\n",
            "loss: 0.882515  [19200/20380]\n",
            "Train Error: Accuracy: 50.5%\n",
            "Val Error: \n",
            " Accuracy: 50.6%, Avg loss: 1.089732 \n",
            "\n",
            "Epoch 8\n",
            "-------------------------------\n",
            "loss: 1.128389  [    0/20380]\n",
            "Train Error: Accuracy: 49.9%\n",
            "loss: 1.181504  [ 6400/20380]\n",
            "Train Error: Accuracy: 50.2%\n",
            "loss: 1.117814  [12800/20380]\n",
            "Train Error: Accuracy: 50.9%\n",
            "loss: 0.869857  [19200/20380]\n",
            "Train Error: Accuracy: 51.1%\n",
            "Val Error: \n",
            " Accuracy: 50.9%, Avg loss: 1.073364 \n",
            "\n",
            "Epoch 9\n",
            "-------------------------------\n",
            "loss: 1.111578  [    0/20380]\n",
            "Train Error: Accuracy: 50.5%\n",
            "loss: 1.161417  [ 6400/20380]\n",
            "Train Error: Accuracy: 50.3%\n",
            "loss: 1.082739  [12800/20380]\n",
            "Train Error: Accuracy: 51.5%\n",
            "loss: 0.857271  [19200/20380]\n",
            "Train Error: Accuracy: 51.6%\n",
            "Val Error: \n",
            " Accuracy: 51.7%, Avg loss: 1.057603 \n",
            "\n",
            "Epoch 10\n",
            "-------------------------------\n",
            "loss: 1.094847  [    0/20380]\n",
            "Train Error: Accuracy: 50.9%\n",
            "loss: 1.136122  [ 6400/20380]\n",
            "Train Error: Accuracy: 50.7%\n",
            "loss: 1.050724  [12800/20380]\n",
            "Train Error: Accuracy: 51.9%\n",
            "loss: 0.845857  [19200/20380]\n",
            "Train Error: Accuracy: 52.0%\n",
            "Val Error: \n",
            " Accuracy: 52.4%, Avg loss: 1.041875 \n",
            "\n",
            "Epoch 11\n",
            "-------------------------------\n",
            "loss: 1.078789  [    0/20380]\n",
            "Train Error: Accuracy: 51.4%\n",
            "loss: 1.107157  [ 6400/20380]\n",
            "Train Error: Accuracy: 51.3%\n",
            "loss: 1.022064  [12800/20380]\n",
            "Train Error: Accuracy: 52.3%\n",
            "loss: 0.834814  [19200/20380]\n",
            "Train Error: Accuracy: 52.5%\n",
            "Val Error: \n",
            " Accuracy: 53.5%, Avg loss: 1.025005 \n",
            "\n",
            "Epoch 12\n",
            "-------------------------------\n",
            "loss: 1.062487  [    0/20380]\n",
            "Train Error: Accuracy: 52.4%\n",
            "loss: 1.073752  [ 6400/20380]\n",
            "Train Error: Accuracy: 51.8%\n",
            "loss: 0.995769  [12800/20380]\n",
            "Train Error: Accuracy: 52.8%\n",
            "loss: 0.823517  [19200/20380]\n",
            "Train Error: Accuracy: 53.2%\n",
            "Val Error: \n",
            " Accuracy: 55.7%, Avg loss: 1.005733 \n",
            "\n",
            "Epoch 13\n",
            "-------------------------------\n",
            "loss: 1.044612  [    0/20380]\n",
            "Train Error: Accuracy: 54.7%\n",
            "loss: 1.034738  [ 6400/20380]\n",
            "Train Error: Accuracy: 52.4%\n",
            "loss: 0.970610  [12800/20380]\n",
            "Train Error: Accuracy: 54.6%\n",
            "loss: 0.812916  [19200/20380]\n",
            "Train Error: Accuracy: 54.2%\n",
            "Val Error: \n",
            " Accuracy: 57.2%, Avg loss: 0.983995 \n",
            "\n",
            "Epoch 14\n",
            "-------------------------------\n",
            "loss: 1.024508  [    0/20380]\n",
            "Train Error: Accuracy: 56.3%\n",
            "loss: 0.990451  [ 6400/20380]\n",
            "Train Error: Accuracy: 55.1%\n",
            "loss: 0.946453  [12800/20380]\n",
            "Train Error: Accuracy: 56.9%\n",
            "loss: 0.804352  [19200/20380]\n",
            "Train Error: Accuracy: 57.1%\n",
            "Val Error: \n",
            " Accuracy: 58.8%, Avg loss: 0.961051 \n",
            "\n",
            "Epoch 15\n",
            "-------------------------------\n",
            "loss: 1.003212  [    0/20380]\n",
            "Train Error: Accuracy: 57.8%\n",
            "loss: 0.944646  [ 6400/20380]\n",
            "Train Error: Accuracy: 57.1%\n",
            "loss: 0.923806  [12800/20380]\n",
            "Train Error: Accuracy: 58.6%\n",
            "loss: 0.796728  [19200/20380]\n",
            "Train Error: Accuracy: 59.2%\n",
            "Val Error: \n",
            " Accuracy: 59.4%, Avg loss: 0.938659 \n",
            "\n",
            "Epoch 16\n",
            "-------------------------------\n",
            "loss: 0.983255  [    0/20380]\n",
            "Train Error: Accuracy: 59.2%\n",
            "loss: 0.903397  [ 6400/20380]\n",
            "Train Error: Accuracy: 59.1%\n",
            "loss: 0.903445  [12800/20380]\n",
            "Train Error: Accuracy: 60.5%\n",
            "loss: 0.789392  [19200/20380]\n",
            "Train Error: Accuracy: 61.0%\n",
            "Val Error: \n",
            " Accuracy: 60.2%, Avg loss: 0.918018 \n",
            "\n",
            "Epoch 17\n",
            "-------------------------------\n",
            "loss: 0.966638  [    0/20380]\n",
            "Train Error: Accuracy: 60.3%\n",
            "loss: 0.869201  [ 6400/20380]\n",
            "Train Error: Accuracy: 59.9%\n",
            "loss: 0.884953  [12800/20380]\n",
            "Train Error: Accuracy: 62.5%\n",
            "loss: 0.782637  [19200/20380]\n",
            "Train Error: Accuracy: 61.8%\n",
            "Val Error: \n",
            " Accuracy: 61.3%, Avg loss: 0.899276 \n",
            "\n",
            "Epoch 18\n",
            "-------------------------------\n",
            "loss: 0.953399  [    0/20380]\n",
            "Train Error: Accuracy: 61.4%\n",
            "loss: 0.840957  [ 6400/20380]\n",
            "Train Error: Accuracy: 60.3%\n",
            "loss: 0.866494  [12800/20380]\n",
            "Train Error: Accuracy: 63.9%\n",
            "loss: 0.776165  [19200/20380]\n",
            "Train Error: Accuracy: 62.7%\n",
            "Val Error: \n",
            " Accuracy: 62.7%, Avg loss: 0.882097 \n",
            "\n",
            "Epoch 19\n",
            "-------------------------------\n",
            "loss: 0.942438  [    0/20380]\n",
            "Train Error: Accuracy: 63.4%\n",
            "loss: 0.817023  [ 6400/20380]\n",
            "Train Error: Accuracy: 61.1%\n",
            "loss: 0.847461  [12800/20380]\n",
            "Train Error: Accuracy: 65.2%\n",
            "loss: 0.769449  [19200/20380]\n",
            "Train Error: Accuracy: 63.9%\n",
            "Val Error: \n",
            " Accuracy: 64.5%, Avg loss: 0.866043 \n",
            "\n",
            "Epoch 20\n",
            "-------------------------------\n",
            "loss: 0.933031  [    0/20380]\n",
            "Train Error: Accuracy: 64.5%\n",
            "loss: 0.796198  [ 6400/20380]\n",
            "Train Error: Accuracy: 62.3%\n",
            "loss: 0.828962  [12800/20380]\n",
            "Train Error: Accuracy: 66.7%\n",
            "loss: 0.762365  [19200/20380]\n",
            "Train Error: Accuracy: 65.3%\n",
            "Val Error: \n",
            " Accuracy: 66.2%, Avg loss: 0.850925 \n",
            "\n",
            "Epoch 21\n",
            "-------------------------------\n",
            "loss: 0.925605  [    0/20380]\n",
            "Train Error: Accuracy: 66.3%\n",
            "loss: 0.777545  [ 6400/20380]\n",
            "Train Error: Accuracy: 64.6%\n",
            "loss: 0.811707  [12800/20380]\n",
            "Train Error: Accuracy: 67.6%\n",
            "loss: 0.754882  [19200/20380]\n",
            "Train Error: Accuracy: 67.1%\n",
            "Val Error: \n",
            " Accuracy: 68.0%, Avg loss: 0.836762 \n",
            "\n",
            "Epoch 22\n",
            "-------------------------------\n",
            "loss: 0.920940  [    0/20380]\n",
            "Train Error: Accuracy: 68.0%\n",
            "loss: 0.760272  [ 6400/20380]\n",
            "Train Error: Accuracy: 66.3%\n",
            "loss: 0.795530  [12800/20380]\n",
            "Train Error: Accuracy: 68.6%\n",
            "loss: 0.747271  [19200/20380]\n",
            "Train Error: Accuracy: 68.2%\n",
            "Val Error: \n",
            " Accuracy: 68.5%, Avg loss: 0.823699 \n",
            "\n",
            "Epoch 23\n",
            "-------------------------------\n",
            "loss: 0.918790  [    0/20380]\n",
            "Train Error: Accuracy: 69.3%\n",
            "loss: 0.743467  [ 6400/20380]\n",
            "Train Error: Accuracy: 67.1%\n",
            "loss: 0.779889  [12800/20380]\n",
            "Train Error: Accuracy: 69.3%\n",
            "loss: 0.740216  [19200/20380]\n",
            "Train Error: Accuracy: 68.8%\n",
            "Val Error: \n",
            " Accuracy: 69.0%, Avg loss: 0.811968 \n",
            "\n",
            "Epoch 24\n",
            "-------------------------------\n",
            "loss: 0.918085  [    0/20380]\n",
            "Train Error: Accuracy: 70.0%\n",
            "loss: 0.726266  [ 6400/20380]\n",
            "Train Error: Accuracy: 67.9%\n",
            "loss: 0.764081  [12800/20380]\n",
            "Train Error: Accuracy: 69.9%\n",
            "loss: 0.734231  [19200/20380]\n",
            "Train Error: Accuracy: 69.4%\n",
            "Val Error: \n",
            " Accuracy: 69.3%, Avg loss: 0.801459 \n",
            "\n",
            "Epoch 25\n",
            "-------------------------------\n",
            "loss: 0.917501  [    0/20380]\n",
            "Train Error: Accuracy: 70.5%\n",
            "loss: 0.708247  [ 6400/20380]\n",
            "Train Error: Accuracy: 68.8%\n",
            "loss: 0.747958  [12800/20380]\n",
            "Train Error: Accuracy: 70.5%\n",
            "loss: 0.729561  [19200/20380]\n",
            "Train Error: Accuracy: 70.0%\n",
            "Val Error: \n",
            " Accuracy: 69.2%, Avg loss: 0.791265 \n",
            "\n",
            "Epoch 26\n",
            "-------------------------------\n",
            "loss: 0.915679  [    0/20380]\n",
            "Train Error: Accuracy: 70.9%\n",
            "loss: 0.689731  [ 6400/20380]\n",
            "Train Error: Accuracy: 69.5%\n",
            "loss: 0.731952  [12800/20380]\n",
            "Train Error: Accuracy: 71.0%\n",
            "loss: 0.726195  [19200/20380]\n",
            "Train Error: Accuracy: 70.5%\n",
            "Val Error: \n",
            " Accuracy: 69.3%, Avg loss: 0.781519 \n",
            "\n",
            "Epoch 27\n",
            "-------------------------------\n",
            "loss: 0.913140  [    0/20380]\n",
            "Train Error: Accuracy: 71.2%\n",
            "loss: 0.671172  [ 6400/20380]\n",
            "Train Error: Accuracy: 70.1%\n",
            "loss: 0.716493  [12800/20380]\n",
            "Train Error: Accuracy: 71.5%\n",
            "loss: 0.723755  [19200/20380]\n",
            "Train Error: Accuracy: 70.7%\n",
            "Val Error: \n",
            " Accuracy: 69.4%, Avg loss: 0.771831 \n",
            "\n",
            "Epoch 28\n",
            "-------------------------------\n",
            "loss: 0.909927  [    0/20380]\n",
            "Train Error: Accuracy: 71.7%\n",
            "loss: 0.652854  [ 6400/20380]\n",
            "Train Error: Accuracy: 70.6%\n",
            "loss: 0.701950  [12800/20380]\n",
            "Train Error: Accuracy: 72.1%\n",
            "loss: 0.721588  [19200/20380]\n",
            "Train Error: Accuracy: 70.7%\n",
            "Val Error: \n",
            " Accuracy: 69.5%, Avg loss: 0.761757 \n",
            "\n",
            "Epoch 29\n",
            "-------------------------------\n",
            "loss: 0.905971  [    0/20380]\n",
            "Train Error: Accuracy: 72.4%\n",
            "loss: 0.634934  [ 6400/20380]\n",
            "Train Error: Accuracy: 71.2%\n",
            "loss: 0.688303  [12800/20380]\n",
            "Train Error: Accuracy: 72.7%\n",
            "loss: 0.719848  [19200/20380]\n",
            "Train Error: Accuracy: 70.8%\n",
            "Val Error: \n",
            " Accuracy: 69.8%, Avg loss: 0.750809 \n",
            "\n",
            "Epoch 30\n",
            "-------------------------------\n",
            "loss: 0.901312  [    0/20380]\n",
            "Train Error: Accuracy: 73.2%\n",
            "loss: 0.617424  [ 6400/20380]\n",
            "Train Error: Accuracy: 71.8%\n",
            "loss: 0.675394  [12800/20380]\n",
            "Train Error: Accuracy: 73.2%\n",
            "loss: 0.718286  [19200/20380]\n",
            "Train Error: Accuracy: 71.1%\n",
            "Val Error: \n",
            " Accuracy: 70.0%, Avg loss: 0.738671 \n",
            "\n",
            "Epoch 31\n",
            "-------------------------------\n",
            "loss: 0.896085  [    0/20380]\n",
            "Train Error: Accuracy: 74.0%\n",
            "loss: 0.600607  [ 6400/20380]\n",
            "Train Error: Accuracy: 72.5%\n",
            "loss: 0.662969  [12800/20380]\n",
            "Train Error: Accuracy: 74.0%\n",
            "loss: 0.716281  [19200/20380]\n",
            "Train Error: Accuracy: 71.3%\n",
            "Val Error: \n",
            " Accuracy: 70.4%, Avg loss: 0.725517 \n",
            "\n",
            "Epoch 32\n",
            "-------------------------------\n",
            "loss: 0.890486  [    0/20380]\n",
            "Train Error: Accuracy: 74.7%\n",
            "loss: 0.584527  [ 6400/20380]\n",
            "Train Error: Accuracy: 73.0%\n",
            "loss: 0.650716  [12800/20380]\n",
            "Train Error: Accuracy: 74.7%\n",
            "loss: 0.713712  [19200/20380]\n",
            "Train Error: Accuracy: 71.9%\n",
            "Val Error: \n",
            " Accuracy: 71.0%, Avg loss: 0.712280 \n",
            "\n",
            "Epoch 33\n",
            "-------------------------------\n",
            "loss: 0.885282  [    0/20380]\n",
            "Train Error: Accuracy: 75.2%\n",
            "loss: 0.569394  [ 6400/20380]\n",
            "Train Error: Accuracy: 73.7%\n",
            "loss: 0.638509  [12800/20380]\n",
            "Train Error: Accuracy: 75.2%\n",
            "loss: 0.710133  [19200/20380]\n",
            "Train Error: Accuracy: 72.3%\n",
            "Val Error: \n",
            " Accuracy: 71.6%, Avg loss: 0.699846 \n",
            "\n",
            "Epoch 34\n",
            "-------------------------------\n",
            "loss: 0.880938  [    0/20380]\n",
            "Train Error: Accuracy: 75.9%\n",
            "loss: 0.555100  [ 6400/20380]\n",
            "Train Error: Accuracy: 74.5%\n",
            "loss: 0.626249  [12800/20380]\n",
            "Train Error: Accuracy: 75.7%\n",
            "loss: 0.705310  [19200/20380]\n",
            "Train Error: Accuracy: 72.8%\n",
            "Val Error: \n",
            " Accuracy: 72.2%, Avg loss: 0.688806 \n",
            "\n",
            "Epoch 35\n",
            "-------------------------------\n",
            "loss: 0.877455  [    0/20380]\n",
            "Train Error: Accuracy: 76.5%\n",
            "loss: 0.541669  [ 6400/20380]\n",
            "Train Error: Accuracy: 75.1%\n",
            "loss: 0.613777  [12800/20380]\n",
            "Train Error: Accuracy: 76.2%\n",
            "loss: 0.699237  [19200/20380]\n",
            "Train Error: Accuracy: 73.3%\n",
            "Val Error: \n",
            " Accuracy: 72.6%, Avg loss: 0.679343 \n",
            "\n",
            "Epoch 36\n",
            "-------------------------------\n",
            "loss: 0.874534  [    0/20380]\n",
            "Train Error: Accuracy: 77.1%\n",
            "loss: 0.528957  [ 6400/20380]\n",
            "Train Error: Accuracy: 75.6%\n",
            "loss: 0.600995  [12800/20380]\n",
            "Train Error: Accuracy: 76.7%\n",
            "loss: 0.691871  [19200/20380]\n",
            "Train Error: Accuracy: 73.9%\n",
            "Val Error: \n",
            " Accuracy: 73.0%, Avg loss: 0.671408 \n",
            "\n",
            "Epoch 37\n",
            "-------------------------------\n",
            "loss: 0.871933  [    0/20380]\n",
            "Train Error: Accuracy: 77.6%\n",
            "loss: 0.516714  [ 6400/20380]\n",
            "Train Error: Accuracy: 76.0%\n",
            "loss: 0.588133  [12800/20380]\n",
            "Train Error: Accuracy: 77.0%\n",
            "loss: 0.683430  [19200/20380]\n",
            "Train Error: Accuracy: 74.4%\n",
            "Val Error: \n",
            " Accuracy: 73.4%, Avg loss: 0.664768 \n",
            "\n",
            "Epoch 38\n",
            "-------------------------------\n",
            "loss: 0.869289  [    0/20380]\n",
            "Train Error: Accuracy: 77.9%\n",
            "loss: 0.504722  [ 6400/20380]\n",
            "Train Error: Accuracy: 76.2%\n",
            "loss: 0.575395  [12800/20380]\n",
            "Train Error: Accuracy: 77.4%\n",
            "loss: 0.673929  [19200/20380]\n",
            "Train Error: Accuracy: 75.0%\n",
            "Val Error: \n",
            " Accuracy: 74.0%, Avg loss: 0.659248 \n",
            "\n",
            "Epoch 39\n",
            "-------------------------------\n",
            "loss: 0.866671  [    0/20380]\n",
            "Train Error: Accuracy: 78.4%\n",
            "loss: 0.492995  [ 6400/20380]\n",
            "Train Error: Accuracy: 76.6%\n",
            "loss: 0.563046  [12800/20380]\n",
            "Train Error: Accuracy: 77.9%\n",
            "loss: 0.663397  [19200/20380]\n",
            "Train Error: Accuracy: 75.8%\n",
            "Val Error: \n",
            " Accuracy: 74.5%, Avg loss: 0.654635 \n",
            "\n",
            "Epoch 40\n",
            "-------------------------------\n",
            "loss: 0.864119  [    0/20380]\n",
            "Train Error: Accuracy: 78.8%\n",
            "loss: 0.481410  [ 6400/20380]\n",
            "Train Error: Accuracy: 77.0%\n",
            "loss: 0.551365  [12800/20380]\n",
            "Train Error: Accuracy: 78.3%\n",
            "loss: 0.652063  [19200/20380]\n",
            "Train Error: Accuracy: 76.2%\n",
            "Val Error: \n",
            " Accuracy: 74.8%, Avg loss: 0.650831 \n",
            "\n",
            "Epoch 41\n",
            "-------------------------------\n",
            "loss: 0.861743  [    0/20380]\n",
            "Train Error: Accuracy: 79.1%\n",
            "loss: 0.469847  [ 6400/20380]\n",
            "Train Error: Accuracy: 77.5%\n",
            "loss: 0.540467  [12800/20380]\n",
            "Train Error: Accuracy: 78.7%\n",
            "loss: 0.640107  [19200/20380]\n",
            "Train Error: Accuracy: 76.8%\n",
            "Val Error: \n",
            " Accuracy: 75.2%, Avg loss: 0.647635 \n",
            "\n",
            "Epoch 42\n",
            "-------------------------------\n",
            "loss: 0.859526  [    0/20380]\n",
            "Train Error: Accuracy: 79.4%\n",
            "loss: 0.458420  [ 6400/20380]\n",
            "Train Error: Accuracy: 77.9%\n",
            "loss: 0.530288  [12800/20380]\n",
            "Train Error: Accuracy: 79.1%\n",
            "loss: 0.627785  [19200/20380]\n",
            "Train Error: Accuracy: 77.2%\n",
            "Val Error: \n",
            " Accuracy: 75.2%, Avg loss: 0.644851 \n",
            "\n",
            "Epoch 43\n",
            "-------------------------------\n",
            "loss: 0.857152  [    0/20380]\n",
            "Train Error: Accuracy: 79.5%\n",
            "loss: 0.447221  [ 6400/20380]\n",
            "Train Error: Accuracy: 78.3%\n",
            "loss: 0.520851  [12800/20380]\n",
            "Train Error: Accuracy: 79.4%\n",
            "loss: 0.615387  [19200/20380]\n",
            "Train Error: Accuracy: 77.7%\n",
            "Val Error: \n",
            " Accuracy: 75.3%, Avg loss: 0.642225 \n",
            "\n",
            "Epoch 44\n",
            "-------------------------------\n",
            "loss: 0.854242  [    0/20380]\n",
            "Train Error: Accuracy: 79.7%\n",
            "loss: 0.436453  [ 6400/20380]\n",
            "Train Error: Accuracy: 78.6%\n",
            "loss: 0.512257  [12800/20380]\n",
            "Train Error: Accuracy: 79.8%\n",
            "loss: 0.602973  [19200/20380]\n",
            "Train Error: Accuracy: 78.1%\n",
            "Val Error: \n",
            " Accuracy: 75.4%, Avg loss: 0.639775 \n",
            "\n",
            "Epoch 45\n",
            "-------------------------------\n",
            "loss: 0.850946  [    0/20380]\n",
            "Train Error: Accuracy: 79.8%\n",
            "loss: 0.426176  [ 6400/20380]\n",
            "Train Error: Accuracy: 79.0%\n",
            "loss: 0.504514  [12800/20380]\n",
            "Train Error: Accuracy: 80.2%\n",
            "loss: 0.590701  [19200/20380]\n",
            "Train Error: Accuracy: 78.4%\n",
            "Val Error: \n",
            " Accuracy: 75.5%, Avg loss: 0.637589 \n",
            "\n",
            "Epoch 46\n",
            "-------------------------------\n",
            "loss: 0.847443  [    0/20380]\n",
            "Train Error: Accuracy: 79.8%\n",
            "loss: 0.416581  [ 6400/20380]\n",
            "Train Error: Accuracy: 79.4%\n",
            "loss: 0.497534  [12800/20380]\n",
            "Train Error: Accuracy: 80.5%\n",
            "loss: 0.579017  [19200/20380]\n",
            "Train Error: Accuracy: 78.7%\n",
            "Val Error: \n",
            " Accuracy: 75.6%, Avg loss: 0.635736 \n",
            "\n",
            "Epoch 47\n",
            "-------------------------------\n",
            "loss: 0.843863  [    0/20380]\n",
            "Train Error: Accuracy: 79.9%\n",
            "loss: 0.407663  [ 6400/20380]\n",
            "Train Error: Accuracy: 79.8%\n",
            "loss: 0.491119  [12800/20380]\n",
            "Train Error: Accuracy: 80.8%\n",
            "loss: 0.567791  [19200/20380]\n",
            "Train Error: Accuracy: 79.1%\n",
            "Val Error: \n",
            " Accuracy: 76.0%, Avg loss: 0.634134 \n",
            "\n",
            "Epoch 48\n",
            "-------------------------------\n",
            "loss: 0.840011  [    0/20380]\n",
            "Train Error: Accuracy: 80.0%\n",
            "loss: 0.399357  [ 6400/20380]\n",
            "Train Error: Accuracy: 80.2%\n",
            "loss: 0.485313  [12800/20380]\n",
            "Train Error: Accuracy: 81.2%\n",
            "loss: 0.556995  [19200/20380]\n",
            "Train Error: Accuracy: 79.3%\n",
            "Val Error: \n",
            " Accuracy: 76.2%, Avg loss: 0.632643 \n",
            "\n",
            "Epoch 49\n",
            "-------------------------------\n",
            "loss: 0.835691  [    0/20380]\n",
            "Train Error: Accuracy: 80.3%\n",
            "loss: 0.391626  [ 6400/20380]\n",
            "Train Error: Accuracy: 80.4%\n",
            "loss: 0.480062  [12800/20380]\n",
            "Train Error: Accuracy: 81.5%\n",
            "loss: 0.546563  [19200/20380]\n",
            "Train Error: Accuracy: 79.4%\n",
            "Val Error: \n",
            " Accuracy: 76.5%, Avg loss: 0.631282 \n",
            "\n",
            "Epoch 50\n",
            "-------------------------------\n",
            "loss: 0.831009  [    0/20380]\n",
            "Train Error: Accuracy: 80.3%\n",
            "loss: 0.384347  [ 6400/20380]\n",
            "Train Error: Accuracy: 80.7%\n",
            "loss: 0.475206  [12800/20380]\n",
            "Train Error: Accuracy: 81.8%\n",
            "loss: 0.536400  [19200/20380]\n",
            "Train Error: Accuracy: 79.7%\n",
            "Val Error: \n",
            " Accuracy: 76.7%, Avg loss: 0.629891 \n",
            "\n",
            "Epoch 51\n",
            "-------------------------------\n",
            "loss: 0.825806  [    0/20380]\n",
            "Train Error: Accuracy: 80.5%\n",
            "loss: 0.377497  [ 6400/20380]\n",
            "Train Error: Accuracy: 81.0%\n",
            "loss: 0.470667  [12800/20380]\n",
            "Train Error: Accuracy: 82.1%\n",
            "loss: 0.526569  [19200/20380]\n",
            "Train Error: Accuracy: 79.9%\n",
            "Val Error: \n",
            " Accuracy: 76.9%, Avg loss: 0.628445 \n",
            "\n",
            "Epoch 52\n",
            "-------------------------------\n",
            "loss: 0.820100  [    0/20380]\n",
            "Train Error: Accuracy: 80.6%\n",
            "loss: 0.371031  [ 6400/20380]\n",
            "Train Error: Accuracy: 81.1%\n",
            "loss: 0.466296  [12800/20380]\n",
            "Train Error: Accuracy: 82.3%\n",
            "loss: 0.516986  [19200/20380]\n",
            "Train Error: Accuracy: 80.1%\n",
            "Val Error: \n",
            " Accuracy: 77.0%, Avg loss: 0.626911 \n",
            "\n",
            "Epoch 53\n",
            "-------------------------------\n",
            "loss: 0.813969  [    0/20380]\n",
            "Train Error: Accuracy: 80.8%\n",
            "loss: 0.364737  [ 6400/20380]\n",
            "Train Error: Accuracy: 81.3%\n",
            "loss: 0.462033  [12800/20380]\n",
            "Train Error: Accuracy: 82.5%\n",
            "loss: 0.507765  [19200/20380]\n",
            "Train Error: Accuracy: 80.3%\n",
            "Val Error: \n",
            " Accuracy: 77.2%, Avg loss: 0.625247 \n",
            "\n",
            "Epoch 54\n",
            "-------------------------------\n",
            "loss: 0.807432  [    0/20380]\n",
            "Train Error: Accuracy: 81.0%\n",
            "loss: 0.358647  [ 6400/20380]\n",
            "Train Error: Accuracy: 81.5%\n",
            "loss: 0.457837  [12800/20380]\n",
            "Train Error: Accuracy: 82.7%\n",
            "loss: 0.499068  [19200/20380]\n",
            "Train Error: Accuracy: 80.5%\n",
            "Val Error: \n",
            " Accuracy: 77.3%, Avg loss: 0.623531 \n",
            "\n",
            "Epoch 55\n",
            "-------------------------------\n",
            "loss: 0.800640  [    0/20380]\n",
            "Train Error: Accuracy: 81.1%\n",
            "loss: 0.352984  [ 6400/20380]\n",
            "Train Error: Accuracy: 81.7%\n",
            "loss: 0.453542  [12800/20380]\n",
            "Train Error: Accuracy: 82.8%\n",
            "loss: 0.490813  [19200/20380]\n",
            "Train Error: Accuracy: 80.6%\n",
            "Val Error: \n",
            " Accuracy: 77.4%, Avg loss: 0.621843 \n",
            "\n",
            "Epoch 56\n",
            "-------------------------------\n",
            "loss: 0.793731  [    0/20380]\n",
            "Train Error: Accuracy: 81.2%\n",
            "loss: 0.347702  [ 6400/20380]\n",
            "Train Error: Accuracy: 81.9%\n",
            "loss: 0.449202  [12800/20380]\n",
            "Train Error: Accuracy: 82.9%\n",
            "loss: 0.483125  [19200/20380]\n",
            "Train Error: Accuracy: 80.6%\n",
            "Val Error: \n",
            " Accuracy: 77.4%, Avg loss: 0.620327 \n",
            "\n",
            "Epoch 57\n",
            "-------------------------------\n",
            "loss: 0.786832  [    0/20380]\n",
            "Train Error: Accuracy: 81.2%\n",
            "loss: 0.342687  [ 6400/20380]\n",
            "Train Error: Accuracy: 82.0%\n",
            "loss: 0.444893  [12800/20380]\n",
            "Train Error: Accuracy: 83.0%\n",
            "loss: 0.476036  [19200/20380]\n",
            "Train Error: Accuracy: 80.8%\n",
            "Val Error: \n",
            " Accuracy: 77.6%, Avg loss: 0.618884 \n",
            "\n",
            "Epoch 58\n",
            "-------------------------------\n",
            "loss: 0.779938  [    0/20380]\n",
            "Train Error: Accuracy: 81.3%\n",
            "loss: 0.337820  [ 6400/20380]\n",
            "Train Error: Accuracy: 82.3%\n",
            "loss: 0.440639  [12800/20380]\n",
            "Train Error: Accuracy: 83.1%\n",
            "loss: 0.469547  [19200/20380]\n",
            "Train Error: Accuracy: 81.0%\n",
            "Val Error: \n",
            " Accuracy: 77.7%, Avg loss: 0.617624 \n",
            "\n",
            "Epoch 59\n",
            "-------------------------------\n",
            "loss: 0.773226  [    0/20380]\n",
            "Train Error: Accuracy: 81.4%\n",
            "loss: 0.333053  [ 6400/20380]\n",
            "Train Error: Accuracy: 82.5%\n",
            "loss: 0.436469  [12800/20380]\n",
            "Train Error: Accuracy: 83.2%\n",
            "loss: 0.463573  [19200/20380]\n",
            "Train Error: Accuracy: 81.1%\n",
            "Val Error: \n",
            " Accuracy: 77.9%, Avg loss: 0.616525 \n",
            "\n",
            "Epoch 60\n",
            "-------------------------------\n",
            "loss: 0.766712  [    0/20380]\n",
            "Train Error: Accuracy: 81.6%\n",
            "loss: 0.328403  [ 6400/20380]\n",
            "Train Error: Accuracy: 82.6%\n",
            "loss: 0.432350  [12800/20380]\n",
            "Train Error: Accuracy: 83.3%\n",
            "loss: 0.458120  [19200/20380]\n",
            "Train Error: Accuracy: 81.2%\n",
            "Val Error: \n",
            " Accuracy: 78.1%, Avg loss: 0.615552 \n",
            "\n",
            "Epoch 61\n",
            "-------------------------------\n",
            "loss: 0.760435  [    0/20380]\n",
            "Train Error: Accuracy: 81.7%\n",
            "loss: 0.323892  [ 6400/20380]\n",
            "Train Error: Accuracy: 82.8%\n",
            "loss: 0.428299  [12800/20380]\n",
            "Train Error: Accuracy: 83.4%\n",
            "loss: 0.453104  [19200/20380]\n",
            "Train Error: Accuracy: 81.3%\n",
            "Val Error: \n",
            " Accuracy: 78.0%, Avg loss: 0.614725 \n",
            "\n",
            "Epoch 62\n",
            "-------------------------------\n",
            "loss: 0.754353  [    0/20380]\n",
            "Train Error: Accuracy: 81.8%\n",
            "loss: 0.319622  [ 6400/20380]\n",
            "Train Error: Accuracy: 82.9%\n",
            "loss: 0.424316  [12800/20380]\n",
            "Train Error: Accuracy: 83.6%\n",
            "loss: 0.448432  [19200/20380]\n",
            "Train Error: Accuracy: 81.4%\n",
            "Val Error: \n",
            " Accuracy: 78.1%, Avg loss: 0.613801 \n",
            "\n",
            "Epoch 63\n",
            "-------------------------------\n",
            "loss: 0.748409  [    0/20380]\n",
            "Train Error: Accuracy: 82.0%\n",
            "loss: 0.315562  [ 6400/20380]\n",
            "Train Error: Accuracy: 83.0%\n",
            "loss: 0.420387  [12800/20380]\n",
            "Train Error: Accuracy: 83.6%\n",
            "loss: 0.444032  [19200/20380]\n",
            "Train Error: Accuracy: 81.5%\n",
            "Val Error: \n",
            " Accuracy: 78.2%, Avg loss: 0.613074 \n",
            "\n",
            "Epoch 64\n",
            "-------------------------------\n",
            "loss: 0.742761  [    0/20380]\n",
            "Train Error: Accuracy: 82.1%\n",
            "loss: 0.311656  [ 6400/20380]\n",
            "Train Error: Accuracy: 83.1%\n",
            "loss: 0.416569  [12800/20380]\n",
            "Train Error: Accuracy: 83.7%\n",
            "loss: 0.439888  [19200/20380]\n",
            "Train Error: Accuracy: 81.6%\n",
            "Val Error: \n",
            " Accuracy: 78.2%, Avg loss: 0.612563 \n",
            "\n",
            "Epoch 65\n",
            "-------------------------------\n",
            "loss: 0.737365  [    0/20380]\n",
            "Train Error: Accuracy: 82.3%\n",
            "loss: 0.307892  [ 6400/20380]\n",
            "Train Error: Accuracy: 83.2%\n",
            "loss: 0.412816  [12800/20380]\n",
            "Train Error: Accuracy: 83.9%\n",
            "loss: 0.435968  [19200/20380]\n",
            "Train Error: Accuracy: 81.7%\n",
            "Val Error: \n",
            " Accuracy: 78.4%, Avg loss: 0.612194 \n",
            "\n",
            "Epoch 66\n",
            "-------------------------------\n",
            "loss: 0.732228  [    0/20380]\n",
            "Train Error: Accuracy: 82.4%\n",
            "loss: 0.304363  [ 6400/20380]\n",
            "Train Error: Accuracy: 83.4%\n",
            "loss: 0.409130  [12800/20380]\n",
            "Train Error: Accuracy: 83.9%\n",
            "loss: 0.432253  [19200/20380]\n",
            "Train Error: Accuracy: 81.8%\n",
            "Val Error: \n",
            " Accuracy: 78.4%, Avg loss: 0.611889 \n",
            "\n",
            "Epoch 67\n",
            "-------------------------------\n",
            "loss: 0.727252  [    0/20380]\n",
            "Train Error: Accuracy: 82.6%\n",
            "loss: 0.301042  [ 6400/20380]\n",
            "Train Error: Accuracy: 83.5%\n",
            "loss: 0.405525  [12800/20380]\n",
            "Train Error: Accuracy: 84.0%\n",
            "loss: 0.428726  [19200/20380]\n",
            "Train Error: Accuracy: 81.9%\n",
            "Val Error: \n",
            " Accuracy: 78.4%, Avg loss: 0.611707 \n",
            "\n",
            "Epoch 68\n",
            "-------------------------------\n",
            "loss: 0.722499  [    0/20380]\n",
            "Train Error: Accuracy: 82.7%\n",
            "loss: 0.297914  [ 6400/20380]\n",
            "Train Error: Accuracy: 83.7%\n",
            "loss: 0.401998  [12800/20380]\n",
            "Train Error: Accuracy: 84.1%\n",
            "loss: 0.425371  [19200/20380]\n",
            "Train Error: Accuracy: 81.9%\n",
            "Val Error: \n",
            " Accuracy: 78.6%, Avg loss: 0.611579 \n",
            "\n",
            "Epoch 69\n",
            "-------------------------------\n",
            "loss: 0.717986  [    0/20380]\n",
            "Train Error: Accuracy: 82.7%\n",
            "loss: 0.294997  [ 6400/20380]\n",
            "Train Error: Accuracy: 83.8%\n",
            "loss: 0.398539  [12800/20380]\n",
            "Train Error: Accuracy: 84.1%\n",
            "loss: 0.422161  [19200/20380]\n",
            "Train Error: Accuracy: 82.0%\n",
            "Val Error: \n",
            " Accuracy: 78.6%, Avg loss: 0.611405 \n",
            "\n",
            "Epoch 70\n",
            "-------------------------------\n",
            "loss: 0.713664  [    0/20380]\n",
            "Train Error: Accuracy: 82.8%\n",
            "loss: 0.292368  [ 6400/20380]\n",
            "Train Error: Accuracy: 84.0%\n",
            "loss: 0.395142  [12800/20380]\n",
            "Train Error: Accuracy: 84.2%\n",
            "loss: 0.419034  [19200/20380]\n",
            "Train Error: Accuracy: 82.0%\n",
            "Val Error: \n",
            " Accuracy: 78.7%, Avg loss: 0.611132 \n",
            "\n",
            "Epoch 71\n",
            "-------------------------------\n",
            "loss: 0.709365  [    0/20380]\n",
            "Train Error: Accuracy: 82.8%\n",
            "loss: 0.289955  [ 6400/20380]\n",
            "Train Error: Accuracy: 84.1%\n",
            "loss: 0.391801  [12800/20380]\n",
            "Train Error: Accuracy: 84.3%\n",
            "loss: 0.415943  [19200/20380]\n",
            "Train Error: Accuracy: 82.0%\n",
            "Val Error: \n",
            " Accuracy: 78.7%, Avg loss: 0.610780 \n",
            "\n",
            "Epoch 72\n",
            "-------------------------------\n",
            "loss: 0.705055  [    0/20380]\n",
            "Train Error: Accuracy: 82.9%\n",
            "loss: 0.287735  [ 6400/20380]\n",
            "Train Error: Accuracy: 84.2%\n",
            "loss: 0.388500  [12800/20380]\n",
            "Train Error: Accuracy: 84.4%\n",
            "loss: 0.412876  [19200/20380]\n",
            "Train Error: Accuracy: 82.1%\n",
            "Val Error: \n",
            " Accuracy: 78.8%, Avg loss: 0.610373 \n",
            "\n",
            "Epoch 73\n",
            "-------------------------------\n",
            "loss: 0.700734  [    0/20380]\n",
            "Train Error: Accuracy: 82.9%\n",
            "loss: 0.285638  [ 6400/20380]\n",
            "Train Error: Accuracy: 84.3%\n",
            "loss: 0.385240  [12800/20380]\n",
            "Train Error: Accuracy: 84.4%\n",
            "loss: 0.409816  [19200/20380]\n",
            "Train Error: Accuracy: 82.2%\n",
            "Val Error: \n",
            " Accuracy: 78.8%, Avg loss: 0.609822 \n",
            "\n",
            "Epoch 74\n",
            "-------------------------------\n",
            "loss: 0.696413  [    0/20380]\n",
            "Train Error: Accuracy: 83.0%\n",
            "loss: 0.283729  [ 6400/20380]\n",
            "Train Error: Accuracy: 84.4%\n",
            "loss: 0.382041  [12800/20380]\n",
            "Train Error: Accuracy: 84.5%\n",
            "loss: 0.406786  [19200/20380]\n",
            "Train Error: Accuracy: 82.3%\n",
            "Val Error: \n",
            " Accuracy: 78.7%, Avg loss: 0.609268 \n",
            "\n",
            "Validation no longer rising. Done!\n",
            "Max number of epochs hit or validation stop reached. Done!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Hyperparameter Study 3:Num Nodes Per Layer Size"
      ],
      "metadata": {
        "id": "lNfzAZG3Tsbc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##3A:  Hidden Layers have 128 Nodes"
      ],
      "metadata": {
        "id": "8XA9FEMRTsbe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#check what device we're using\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"Using {device} device\")\n",
        "torch.set_grad_enabled(True) "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c7afd6e4-744c-4d32-9a66-005bcd7673c1",
        "id": "osEu40PUoPRs"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using cpu device\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch.autograd.grad_mode.set_grad_enabled at 0x7f260f307950>"
            ]
          },
          "metadata": {},
          "execution_count": 116
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#try at 256 neurons double if there are issues\n",
        "class NeuralNetwork(nn.Module):\n",
        "    def __init__(self):\n",
        "        #super(NeuralNetwork, self).__init__()\n",
        "        super().__init__()\n",
        "        self.flatten = nn.Flatten() #might be vestigial...remove if no impact\n",
        "        self.linear_relu_stack = nn.Sequential(\n",
        "            nn.Linear(7, 128), #input layer \n",
        "            nn.ELU(), # relu for simplicity?\n",
        "            nn.Linear(128, 128), #hidden layer 1\n",
        "            nn.ELU(),\n",
        "            nn.Linear(128, 128), #hidden layer 2\n",
        "            nn.ELU(),\n",
        "            #nn.Softmax(dim=1), #softmax for ideal classification?\n",
        "            nn.Linear(128, 5), #output layer\n",
        "        )\n",
        "    # \"Every nn.Module subclass implements the operations \n",
        "    # on input data in the forward method.\"\n",
        "    # We already preprocessed the input data\n",
        "    # so we just return x\n",
        "    def forward(self, x): \n",
        "        #x.requires_grad_(True) #not needed - should be already setup with this assumption\n",
        "        #return x\n",
        "        #x = self.flatten(x)\n",
        "        logits = self.linear_relu_stack(x) #we already preprocessed the data, so we just pass it into the NN here\n",
        "        return logits"
      ],
      "metadata": {
        "id": "EDzB9GKJoPRu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Implement forward propagation (clearly describe the activation functions and other\n",
        "hyper-parameters you are using)."
      ],
      "metadata": {
        "id": "WNFTKX-EoPRv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "m = nn.Softmax(dim=1)\n",
        "input = torch.randn(2, 3)\n",
        "output = m(input)\n",
        "output"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3970cfb6-453e-48df-fd64-4cfeab77d978",
        "id": "KwFx78kpoPRx"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.3494, 0.1873, 0.4633],\n",
              "        [0.3742, 0.3069, 0.3189]])"
            ]
          },
          "metadata": {},
          "execution_count": 118
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = NeuralNetwork().to(device)\n",
        "print(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0d5a2a1a-9c5e-4779-a173-42ac2d542318",
        "id": "2o3rG44OoPRy"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NeuralNetwork(\n",
            "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
            "  (linear_relu_stack): Sequential(\n",
            "    (0): Linear(in_features=7, out_features=128, bias=True)\n",
            "    (1): ELU(alpha=1.0)\n",
            "    (2): Linear(in_features=128, out_features=128, bias=True)\n",
            "    (3): ELU(alpha=1.0)\n",
            "    (4): Linear(in_features=128, out_features=128, bias=True)\n",
            "    (5): ELU(alpha=1.0)\n",
            "    (6): Linear(in_features=128, out_features=5, bias=True)\n",
            "  )\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "4. Compute the final cost function.\n",
        "\n",
        "5. Implement gradient descent (any variant of gradient descent depending upon your\n",
        "data and project can be used) to train your model. In this step it is up to you as someone\n",
        "in charge of their project to improvise using optimization algorithms (Adams, RMSProp\n",
        "etc.) and/or regularization."
      ],
      "metadata": {
        "id": "OstF9KUOoPRz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=1e-2) "
      ],
      "metadata": {
        "id": "h4wfd_3voPR0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(dataloader, model, loss_fn, optimizer):\n",
        "    size = len(dataloader.dataset)\n",
        "    model.train()\n",
        "    for batch, (X, y) in enumerate(dataloader):\n",
        "        X, y = X.to(device), y.to(device)\n",
        "\n",
        "        # Compute prediction error\n",
        "        pred = model(X)\n",
        "        #print(pred.size())\n",
        "        #print(y.size())\n",
        "        #predtrans = torch.transpose(pred, 0, 1)\n",
        "        #print(predtrans.size())\n",
        "        #loss = loss_fn(predtrans, y)\n",
        "        loss = loss_fn(pred, y.long()) #fix 1\n",
        "        #loss.requires_grad = True #fix 2\n",
        "\n",
        "        # Backpropagation\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if batch % 100 == 0:\n",
        "            loss, current = loss.item(), batch * len(X)\n",
        "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
        "            correct = 0\n",
        "            with torch.no_grad():\n",
        "                for X, y in dataloader:\n",
        "                    X, y = X.to(device), y.to(device)\n",
        "                    pred = model(X)\n",
        "                    correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
        "            correct /= size\n",
        "            print(f\"Train Error: Accuracy: {(100*correct):>0.1f}%\")\n",
        "\n",
        "\n",
        "def validate(dataloader, model, loss_fn):\n",
        "    size = len(dataloader.dataset)\n",
        "    num_batches = len(dataloader)\n",
        "    model.eval()\n",
        "    test_loss, correct = 0, 0\n",
        "    with torch.no_grad():\n",
        "        for X, y in dataloader:\n",
        "            X, y = X.to(device), y.to(device)\n",
        "            pred = model(X)\n",
        "            test_loss += loss_fn(pred, y.long()).item()\n",
        "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
        "    test_loss /= num_batches\n",
        "    correct /= size\n",
        "    print(f\"Val Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n",
        "    return correct\n",
        "\n",
        "def test(dataloader, model, loss_fn):\n",
        "    size = len(dataloader.dataset)\n",
        "    num_batches = len(dataloader)\n",
        "    model.eval()\n",
        "    test_loss, correct = 0, 0\n",
        "    with torch.no_grad():\n",
        "        for X, y in dataloader:\n",
        "            X, y = X.to(device), y.to(device)\n",
        "            pred = model(X)\n",
        "            test_loss += loss_fn(pred, y.long()).item()\n",
        "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
        "    test_loss /= num_batches\n",
        "    correct /= size\n",
        "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"
      ],
      "metadata": {
        "id": "Xf9EDgmRoPR1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 100\n",
        "bestval = 0\n",
        "decval = 0\n",
        "for t in range(epochs):\n",
        "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
        "    train(train_dataloader, model, loss_fn, optimizer)\n",
        "    #TODO VALIDATE\n",
        "    tempval = validate(val_dataloader, model, loss_fn)\n",
        "    if(tempval < bestval):\n",
        "      decval+=1 \n",
        "      if(decval==3):\n",
        "        print(\"Validation no longer rising. Done!\")\n",
        "        break\n",
        "    else:\n",
        "      bestval=tempval\n",
        "    #TODO ESSAY\n",
        "    #TODO ACTIVATION JUSTIFICATION\n",
        "    #TODO ALT\n",
        "print(\"Max number of epochs hit or validation stop reached. Done!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "defb8c9b-1508-4021-9076-66f95c80f5e7",
        "id": "pk8fnKPgoPR2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1\n",
            "-------------------------------\n",
            "loss: 1.631810  [    0/20380]\n",
            "Train Error: Accuracy: 19.7%\n",
            "loss: 1.527414  [ 6400/20380]\n",
            "Train Error: Accuracy: 37.9%\n",
            "loss: 1.444305  [12800/20380]\n",
            "Train Error: Accuracy: 41.7%\n",
            "loss: 1.206050  [19200/20380]\n",
            "Train Error: Accuracy: 45.4%\n",
            "Val Error: \n",
            " Accuracy: 45.3%, Avg loss: 1.336324 \n",
            "\n",
            "Epoch 2\n",
            "-------------------------------\n",
            "loss: 1.323048  [    0/20380]\n",
            "Train Error: Accuracy: 45.1%\n",
            "loss: 1.370135  [ 6400/20380]\n",
            "Train Error: Accuracy: 44.7%\n",
            "loss: 1.337392  [12800/20380]\n",
            "Train Error: Accuracy: 46.3%\n",
            "loss: 1.060523  [19200/20380]\n",
            "Train Error: Accuracy: 46.3%\n",
            "Val Error: \n",
            " Accuracy: 46.6%, Avg loss: 1.256334 \n",
            "\n",
            "Epoch 3\n",
            "-------------------------------\n",
            "loss: 1.230130  [    0/20380]\n",
            "Train Error: Accuracy: 46.6%\n",
            "loss: 1.298939  [ 6400/20380]\n",
            "Train Error: Accuracy: 46.6%\n",
            "loss: 1.291863  [12800/20380]\n",
            "Train Error: Accuracy: 46.5%\n",
            "loss: 1.003802  [19200/20380]\n",
            "Train Error: Accuracy: 46.4%\n",
            "Val Error: \n",
            " Accuracy: 46.2%, Avg loss: 1.219719 \n",
            "\n",
            "Epoch 4\n",
            "-------------------------------\n",
            "loss: 1.199014  [    0/20380]\n",
            "Train Error: Accuracy: 46.8%\n",
            "loss: 1.258928  [ 6400/20380]\n",
            "Train Error: Accuracy: 46.8%\n",
            "loss: 1.258234  [12800/20380]\n",
            "Train Error: Accuracy: 46.8%\n",
            "loss: 0.966064  [19200/20380]\n",
            "Train Error: Accuracy: 46.3%\n",
            "Val Error: \n",
            " Accuracy: 46.0%, Avg loss: 1.191403 \n",
            "\n",
            "Epoch 5\n",
            "-------------------------------\n",
            "loss: 1.181698  [    0/20380]\n",
            "Train Error: Accuracy: 46.3%\n",
            "loss: 1.228974  [ 6400/20380]\n",
            "Train Error: Accuracy: 46.5%\n",
            "loss: 1.229741  [12800/20380]\n",
            "Train Error: Accuracy: 46.4%\n",
            "loss: 0.939178  [19200/20380]\n",
            "Train Error: Accuracy: 46.4%\n",
            "Val Error: \n",
            " Accuracy: 46.4%, Avg loss: 1.165756 \n",
            "\n",
            "Validation no longer rising. Done!\n",
            "Max number of epochs hit or validation stop reached. Done!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##3B: Hidden Layers Have  have 512 Nodes"
      ],
      "metadata": {
        "id": "_yKm2YNrTsbh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#check what device we're using\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"Using {device} device\")\n",
        "torch.set_grad_enabled(True) "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e5c0aade-1288-4fe6-e791-0b6c9bfd5e9c",
        "id": "4DfZ7kudoUir"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using cpu device\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch.autograd.grad_mode.set_grad_enabled at 0x7f260f2c6790>"
            ]
          },
          "metadata": {},
          "execution_count": 123
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class NeuralNetwork(nn.Module):\n",
        "    def __init__(self):\n",
        "        #super(NeuralNetwork, self).__init__()\n",
        "        super().__init__()\n",
        "        self.linear_relu_stack = nn.Sequential(\n",
        "            nn.Linear(7, 512), #input layer \n",
        "            nn.ELU(), \n",
        "            nn.Linear(512, 512), #hidden layer 1\n",
        "            nn.ELU(),\n",
        "            nn.Linear(512, 512), #hidden layer 2\n",
        "            nn.ELU(),\n",
        "            nn.Linear(512, 5), #output layer\n",
        "        )\n",
        "    # \"Every nn.Module subclass implements the operations \n",
        "    # on input data in the forward method.\"\n",
        "    # We already preprocessed the input data\n",
        "    # so we just return x\n",
        "    def forward(self, x): \n",
        "        logits = self.linear_relu_stack(x) #we already preprocessed the data, so we just pass it into the NN here\n",
        "        return logits"
      ],
      "metadata": {
        "id": "Q1fbSAv1oUis"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Implement forward propagation (clearly describe the activation functions and other\n",
        "hyper-parameters you are using)."
      ],
      "metadata": {
        "id": "KMbOpNU3oUit"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = NeuralNetwork().to(device)\n",
        "print(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9b2c638f-9fbd-4cc5-c587-ee7f68f3a330",
        "id": "sC5G9_T2oUiv"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NeuralNetwork(\n",
            "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
            "  (linear_relu_stack): Sequential(\n",
            "    (0): Linear(in_features=7, out_features=512, bias=True)\n",
            "    (1): ELU(alpha=1.0)\n",
            "    (2): Linear(in_features=512, out_features=512, bias=True)\n",
            "    (3): ELU(alpha=1.0)\n",
            "    (4): Linear(in_features=512, out_features=512, bias=True)\n",
            "    (5): ELU(alpha=1.0)\n",
            "    (6): Linear(in_features=512, out_features=5, bias=True)\n",
            "  )\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "4. Compute the final cost function.\n",
        "\n",
        "5. Implement gradient descent (any variant of gradient descent depending upon your\n",
        "data and project can be used) to train your model. In this step it is up to you as someone\n",
        "in charge of their project to improvise using optimization algorithms (Adams, RMSProp\n",
        "etc.) and/or regularization."
      ],
      "metadata": {
        "id": "feW72vnmoUiw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=1e-2) "
      ],
      "metadata": {
        "id": "zCj6mmQ0oUix"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Code snippet attribution:\n",
        "#https://pytorch.org/tutorials/beginner/basics/intro.html\n",
        "#was used as part of the research process\n",
        "#Modifications were made so that data could be processed as longs,\n",
        "#additional statitics could be reported, \n",
        "#and a separate validate process was created. \n",
        "def train(dataloader, model, loss_fn, optimizer):\n",
        "    size = len(dataloader.dataset)\n",
        "    model.train()\n",
        "    for batch, (X, y) in enumerate(dataloader):\n",
        "        X, y = X.to(device), y.to(device)\n",
        "\n",
        "        # Compute prediction error\n",
        "        pred = model(X)\n",
        "        loss = loss_fn(pred, y.long()) \n",
        "\n",
        "        # Backpropagation\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if batch % 100 == 0:\n",
        "            loss, current = loss.item(), batch * len(X)\n",
        "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
        "            correct = 0\n",
        "            with torch.no_grad():\n",
        "                for X, y in dataloader:\n",
        "                    X, y = X.to(device), y.to(device)\n",
        "                    pred = model(X)\n",
        "                    correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
        "            correct /= size\n",
        "            print(f\"Train Error: Accuracy: {(100*correct):>0.1f}%\")\n",
        "\n",
        "\n",
        "def validate(dataloader, model, loss_fn):\n",
        "    size = len(dataloader.dataset)\n",
        "    num_batches = len(dataloader)\n",
        "    model.eval()\n",
        "    test_loss, correct = 0, 0\n",
        "    with torch.no_grad():\n",
        "        for X, y in dataloader:\n",
        "            X, y = X.to(device), y.to(device)\n",
        "            pred = model(X)\n",
        "            test_loss += loss_fn(pred, y.long()).item()\n",
        "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
        "    test_loss /= num_batches\n",
        "    correct /= size\n",
        "    print(f\"Val Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n",
        "    return correct\n",
        "\n",
        "def test(dataloader, model, loss_fn):\n",
        "    size = len(dataloader.dataset)\n",
        "    num_batches = len(dataloader)\n",
        "    model.eval()\n",
        "    test_loss, correct = 0, 0\n",
        "    with torch.no_grad():\n",
        "        for X, y in dataloader:\n",
        "            X, y = X.to(device), y.to(device)\n",
        "            pred = model(X)\n",
        "            test_loss += loss_fn(pred, y.long()).item()\n",
        "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
        "    test_loss /= num_batches\n",
        "    correct /= size\n",
        "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"
      ],
      "metadata": {
        "id": "Mbccpc6IoUix"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 100\n",
        "bestval = 0\n",
        "decval = 0\n",
        "for t in range(epochs):\n",
        "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
        "    train(train_dataloader, model, loss_fn, optimizer)\n",
        "    tempval = validate(val_dataloader, model, loss_fn)\n",
        "    if(tempval < bestval):\n",
        "      decval+=1 \n",
        "      if(decval==3):\n",
        "        print(\"Validation no longer rising. Done!\")\n",
        "        break\n",
        "    else:\n",
        "      bestval=tempval\n",
        "print(\"Max number of epochs hit or validation stop reached. Done!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4370260e-d9c2-4be7-e258-cc9a4bfd5487",
        "id": "OF0Q-OI0oUiy"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1\n",
            "-------------------------------\n",
            "loss: 1.613340  [    0/20380]\n",
            "Train Error: Accuracy: 30.3%\n",
            "loss: 1.395846  [ 6400/20380]\n",
            "Train Error: Accuracy: 45.2%\n",
            "loss: 1.324866  [12800/20380]\n",
            "Train Error: Accuracy: 46.1%\n",
            "loss: 1.052245  [19200/20380]\n",
            "Train Error: Accuracy: 45.9%\n",
            "Val Error: \n",
            " Accuracy: 45.5%, Avg loss: 1.238783 \n",
            "\n",
            "Epoch 2\n",
            "-------------------------------\n",
            "loss: 1.208584  [    0/20380]\n",
            "Train Error: Accuracy: 46.0%\n",
            "loss: 1.273260  [ 6400/20380]\n",
            "Train Error: Accuracy: 46.3%\n",
            "loss: 1.269467  [12800/20380]\n",
            "Train Error: Accuracy: 46.3%\n",
            "loss: 0.979881  [19200/20380]\n",
            "Train Error: Accuracy: 45.9%\n",
            "Val Error: \n",
            " Accuracy: 46.0%, Avg loss: 1.190588 \n",
            "\n",
            "Epoch 3\n",
            "-------------------------------\n",
            "loss: 1.175752  [    0/20380]\n",
            "Train Error: Accuracy: 45.8%\n",
            "loss: 1.221062  [ 6400/20380]\n",
            "Train Error: Accuracy: 46.1%\n",
            "loss: 1.223329  [12800/20380]\n",
            "Train Error: Accuracy: 46.5%\n",
            "loss: 0.934941  [19200/20380]\n",
            "Train Error: Accuracy: 46.8%\n",
            "Val Error: \n",
            " Accuracy: 46.7%, Avg loss: 1.150913 \n",
            "\n",
            "Epoch 4\n",
            "-------------------------------\n",
            "loss: 1.147950  [    0/20380]\n",
            "Train Error: Accuracy: 46.1%\n",
            "loss: 1.180093  [ 6400/20380]\n",
            "Train Error: Accuracy: 46.7%\n",
            "loss: 1.185228  [12800/20380]\n",
            "Train Error: Accuracy: 47.1%\n",
            "loss: 0.904211  [19200/20380]\n",
            "Train Error: Accuracy: 47.5%\n",
            "Val Error: \n",
            " Accuracy: 47.6%, Avg loss: 1.117338 \n",
            "\n",
            "Epoch 5\n",
            "-------------------------------\n",
            "loss: 1.123998  [    0/20380]\n",
            "Train Error: Accuracy: 46.6%\n",
            "loss: 1.149919  [ 6400/20380]\n",
            "Train Error: Accuracy: 47.8%\n",
            "loss: 1.156543  [12800/20380]\n",
            "Train Error: Accuracy: 48.1%\n",
            "loss: 0.881273  [19200/20380]\n",
            "Train Error: Accuracy: 48.8%\n",
            "Val Error: \n",
            " Accuracy: 50.5%, Avg loss: 1.090128 \n",
            "\n",
            "Epoch 6\n",
            "-------------------------------\n",
            "loss: 1.104039  [    0/20380]\n",
            "Train Error: Accuracy: 48.5%\n",
            "loss: 1.124784  [ 6400/20380]\n",
            "Train Error: Accuracy: 49.5%\n",
            "loss: 1.128543  [12800/20380]\n",
            "Train Error: Accuracy: 50.1%\n",
            "loss: 0.863359  [19200/20380]\n",
            "Train Error: Accuracy: 50.2%\n",
            "Val Error: \n",
            " Accuracy: 53.2%, Avg loss: 1.067156 \n",
            "\n",
            "Epoch 7\n",
            "-------------------------------\n",
            "loss: 1.087327  [    0/20380]\n",
            "Train Error: Accuracy: 51.6%\n",
            "loss: 1.098812  [ 6400/20380]\n",
            "Train Error: Accuracy: 50.8%\n",
            "loss: 1.098564  [12800/20380]\n",
            "Train Error: Accuracy: 52.0%\n",
            "loss: 0.847115  [19200/20380]\n",
            "Train Error: Accuracy: 52.3%\n",
            "Val Error: \n",
            " Accuracy: 55.6%, Avg loss: 1.046218 \n",
            "\n",
            "Epoch 8\n",
            "-------------------------------\n",
            "loss: 1.073264  [    0/20380]\n",
            "Train Error: Accuracy: 53.7%\n",
            "loss: 1.070521  [ 6400/20380]\n",
            "Train Error: Accuracy: 52.3%\n",
            "loss: 1.067408  [12800/20380]\n",
            "Train Error: Accuracy: 54.4%\n",
            "loss: 0.830564  [19200/20380]\n",
            "Train Error: Accuracy: 54.4%\n",
            "Val Error: \n",
            " Accuracy: 56.8%, Avg loss: 1.026201 \n",
            "\n",
            "Epoch 9\n",
            "-------------------------------\n",
            "loss: 1.060152  [    0/20380]\n",
            "Train Error: Accuracy: 55.0%\n",
            "loss: 1.040526  [ 6400/20380]\n",
            "Train Error: Accuracy: 54.0%\n",
            "loss: 1.035651  [12800/20380]\n",
            "Train Error: Accuracy: 56.0%\n",
            "loss: 0.813429  [19200/20380]\n",
            "Train Error: Accuracy: 56.1%\n",
            "Val Error: \n",
            " Accuracy: 57.4%, Avg loss: 1.006643 \n",
            "\n",
            "Epoch 10\n",
            "-------------------------------\n",
            "loss: 1.045942  [    0/20380]\n",
            "Train Error: Accuracy: 56.2%\n",
            "loss: 1.009691  [ 6400/20380]\n",
            "Train Error: Accuracy: 55.7%\n",
            "loss: 1.003600  [12800/20380]\n",
            "Train Error: Accuracy: 57.3%\n",
            "loss: 0.796154  [19200/20380]\n",
            "Train Error: Accuracy: 57.3%\n",
            "Val Error: \n",
            " Accuracy: 58.3%, Avg loss: 0.987449 \n",
            "\n",
            "Epoch 11\n",
            "-------------------------------\n",
            "loss: 1.029416  [    0/20380]\n",
            "Train Error: Accuracy: 57.5%\n",
            "loss: 0.979117  [ 6400/20380]\n",
            "Train Error: Accuracy: 57.1%\n",
            "loss: 0.971771  [12800/20380]\n",
            "Train Error: Accuracy: 58.3%\n",
            "loss: 0.779258  [19200/20380]\n",
            "Train Error: Accuracy: 58.6%\n",
            "Val Error: \n",
            " Accuracy: 58.9%, Avg loss: 0.968756 \n",
            "\n",
            "Epoch 12\n",
            "-------------------------------\n",
            "loss: 1.010055  [    0/20380]\n",
            "Train Error: Accuracy: 58.6%\n",
            "loss: 0.949820  [ 6400/20380]\n",
            "Train Error: Accuracy: 58.3%\n",
            "loss: 0.941118  [12800/20380]\n",
            "Train Error: Accuracy: 59.3%\n",
            "loss: 0.763422  [19200/20380]\n",
            "Train Error: Accuracy: 60.0%\n",
            "Val Error: \n",
            " Accuracy: 59.8%, Avg loss: 0.950750 \n",
            "\n",
            "Epoch 13\n",
            "-------------------------------\n",
            "loss: 0.988377  [    0/20380]\n",
            "Train Error: Accuracy: 59.6%\n",
            "loss: 0.921685  [ 6400/20380]\n",
            "Train Error: Accuracy: 59.2%\n",
            "loss: 0.912782  [12800/20380]\n",
            "Train Error: Accuracy: 60.4%\n",
            "loss: 0.749408  [19200/20380]\n",
            "Train Error: Accuracy: 61.1%\n",
            "Val Error: \n",
            " Accuracy: 60.5%, Avg loss: 0.933504 \n",
            "\n",
            "Epoch 14\n",
            "-------------------------------\n",
            "loss: 0.966186  [    0/20380]\n",
            "Train Error: Accuracy: 60.8%\n",
            "loss: 0.894208  [ 6400/20380]\n",
            "Train Error: Accuracy: 60.0%\n",
            "loss: 0.887230  [12800/20380]\n",
            "Train Error: Accuracy: 61.8%\n",
            "loss: 0.737184  [19200/20380]\n",
            "Train Error: Accuracy: 62.1%\n",
            "Val Error: \n",
            " Accuracy: 61.4%, Avg loss: 0.916917 \n",
            "\n",
            "Epoch 15\n",
            "-------------------------------\n",
            "loss: 0.944826  [    0/20380]\n",
            "Train Error: Accuracy: 61.7%\n",
            "loss: 0.867478  [ 6400/20380]\n",
            "Train Error: Accuracy: 61.1%\n",
            "loss: 0.864134  [12800/20380]\n",
            "Train Error: Accuracy: 62.9%\n",
            "loss: 0.726259  [19200/20380]\n",
            "Train Error: Accuracy: 62.7%\n",
            "Val Error: \n",
            " Accuracy: 62.0%, Avg loss: 0.900733 \n",
            "\n",
            "Epoch 16\n",
            "-------------------------------\n",
            "loss: 0.924676  [    0/20380]\n",
            "Train Error: Accuracy: 62.4%\n",
            "loss: 0.841582  [ 6400/20380]\n",
            "Train Error: Accuracy: 61.9%\n",
            "loss: 0.842747  [12800/20380]\n",
            "Train Error: Accuracy: 64.0%\n",
            "loss: 0.716079  [19200/20380]\n",
            "Train Error: Accuracy: 63.5%\n",
            "Val Error: \n",
            " Accuracy: 62.8%, Avg loss: 0.884622 \n",
            "\n",
            "Epoch 17\n",
            "-------------------------------\n",
            "loss: 0.905624  [    0/20380]\n",
            "Train Error: Accuracy: 63.2%\n",
            "loss: 0.816498  [ 6400/20380]\n",
            "Train Error: Accuracy: 62.5%\n",
            "loss: 0.822467  [12800/20380]\n",
            "Train Error: Accuracy: 65.2%\n",
            "loss: 0.706323  [19200/20380]\n",
            "Train Error: Accuracy: 64.4%\n",
            "Val Error: \n",
            " Accuracy: 63.4%, Avg loss: 0.868425 \n",
            "\n",
            "Epoch 18\n",
            "-------------------------------\n",
            "loss: 0.887518  [    0/20380]\n",
            "Train Error: Accuracy: 63.8%\n",
            "loss: 0.792113  [ 6400/20380]\n",
            "Train Error: Accuracy: 63.5%\n",
            "loss: 0.803129  [12800/20380]\n",
            "Train Error: Accuracy: 66.4%\n",
            "loss: 0.696970  [19200/20380]\n",
            "Train Error: Accuracy: 65.7%\n",
            "Val Error: \n",
            " Accuracy: 64.5%, Avg loss: 0.852227 \n",
            "\n",
            "Epoch 19\n",
            "-------------------------------\n",
            "loss: 0.870359  [    0/20380]\n",
            "Train Error: Accuracy: 64.6%\n",
            "loss: 0.768344  [ 6400/20380]\n",
            "Train Error: Accuracy: 64.5%\n",
            "loss: 0.785094  [12800/20380]\n",
            "Train Error: Accuracy: 67.2%\n",
            "loss: 0.688266  [19200/20380]\n",
            "Train Error: Accuracy: 67.0%\n",
            "Val Error: \n",
            " Accuracy: 65.7%, Avg loss: 0.836428 \n",
            "\n",
            "Epoch 20\n",
            "-------------------------------\n",
            "loss: 0.854539  [    0/20380]\n",
            "Train Error: Accuracy: 66.0%\n",
            "loss: 0.745570  [ 6400/20380]\n",
            "Train Error: Accuracy: 65.7%\n",
            "loss: 0.769019  [12800/20380]\n",
            "Train Error: Accuracy: 67.8%\n",
            "loss: 0.680593  [19200/20380]\n",
            "Train Error: Accuracy: 68.0%\n",
            "Val Error: \n",
            " Accuracy: 66.8%, Avg loss: 0.821659 \n",
            "\n",
            "Epoch 21\n",
            "-------------------------------\n",
            "loss: 0.840594  [    0/20380]\n",
            "Train Error: Accuracy: 67.2%\n",
            "loss: 0.724472  [ 6400/20380]\n",
            "Train Error: Accuracy: 66.5%\n",
            "loss: 0.755695  [12800/20380]\n",
            "Train Error: Accuracy: 68.4%\n",
            "loss: 0.674104  [19200/20380]\n",
            "Train Error: Accuracy: 68.4%\n",
            "Val Error: \n",
            " Accuracy: 67.7%, Avg loss: 0.808315 \n",
            "\n",
            "Epoch 22\n",
            "-------------------------------\n",
            "loss: 0.828783  [    0/20380]\n",
            "Train Error: Accuracy: 68.1%\n",
            "loss: 0.705292  [ 6400/20380]\n",
            "Train Error: Accuracy: 67.3%\n",
            "loss: 0.745265  [12800/20380]\n",
            "Train Error: Accuracy: 69.1%\n",
            "loss: 0.668667  [19200/20380]\n",
            "Train Error: Accuracy: 69.0%\n",
            "Val Error: \n",
            " Accuracy: 68.3%, Avg loss: 0.796429 \n",
            "\n",
            "Epoch 23\n",
            "-------------------------------\n",
            "loss: 0.819080  [    0/20380]\n",
            "Train Error: Accuracy: 68.9%\n",
            "loss: 0.687903  [ 6400/20380]\n",
            "Train Error: Accuracy: 68.0%\n",
            "loss: 0.737016  [12800/20380]\n",
            "Train Error: Accuracy: 69.5%\n",
            "loss: 0.664004  [19200/20380]\n",
            "Train Error: Accuracy: 69.5%\n",
            "Val Error: \n",
            " Accuracy: 68.7%, Avg loss: 0.785733 \n",
            "\n",
            "Epoch 24\n",
            "-------------------------------\n",
            "loss: 0.811245  [    0/20380]\n",
            "Train Error: Accuracy: 69.3%\n",
            "loss: 0.672066  [ 6400/20380]\n",
            "Train Error: Accuracy: 68.6%\n",
            "loss: 0.729834  [12800/20380]\n",
            "Train Error: Accuracy: 69.9%\n",
            "loss: 0.659830  [19200/20380]\n",
            "Train Error: Accuracy: 70.0%\n",
            "Val Error: \n",
            " Accuracy: 69.2%, Avg loss: 0.775843 \n",
            "\n",
            "Epoch 25\n",
            "-------------------------------\n",
            "loss: 0.804884  [    0/20380]\n",
            "Train Error: Accuracy: 70.0%\n",
            "loss: 0.657479  [ 6400/20380]\n",
            "Train Error: Accuracy: 69.1%\n",
            "loss: 0.722822  [12800/20380]\n",
            "Train Error: Accuracy: 70.3%\n",
            "loss: 0.655956  [19200/20380]\n",
            "Train Error: Accuracy: 70.5%\n",
            "Val Error: \n",
            " Accuracy: 69.3%, Avg loss: 0.766445 \n",
            "\n",
            "Epoch 26\n",
            "-------------------------------\n",
            "loss: 0.799552  [    0/20380]\n",
            "Train Error: Accuracy: 70.8%\n",
            "loss: 0.643954  [ 6400/20380]\n",
            "Train Error: Accuracy: 69.7%\n",
            "loss: 0.715542  [12800/20380]\n",
            "Train Error: Accuracy: 70.9%\n",
            "loss: 0.652333  [19200/20380]\n",
            "Train Error: Accuracy: 71.0%\n",
            "Val Error: \n",
            " Accuracy: 69.9%, Avg loss: 0.757361 \n",
            "\n",
            "Epoch 27\n",
            "-------------------------------\n",
            "loss: 0.794828  [    0/20380]\n",
            "Train Error: Accuracy: 71.5%\n",
            "loss: 0.631442  [ 6400/20380]\n",
            "Train Error: Accuracy: 70.2%\n",
            "loss: 0.708093  [12800/20380]\n",
            "Train Error: Accuracy: 71.3%\n",
            "loss: 0.648744  [19200/20380]\n",
            "Train Error: Accuracy: 71.5%\n",
            "Val Error: \n",
            " Accuracy: 70.4%, Avg loss: 0.748503 \n",
            "\n",
            "Epoch 28\n",
            "-------------------------------\n",
            "loss: 0.790362  [    0/20380]\n",
            "Train Error: Accuracy: 72.1%\n",
            "loss: 0.619919  [ 6400/20380]\n",
            "Train Error: Accuracy: 70.8%\n",
            "loss: 0.700638  [12800/20380]\n",
            "Train Error: Accuracy: 71.8%\n",
            "loss: 0.645237  [19200/20380]\n",
            "Train Error: Accuracy: 71.9%\n",
            "Val Error: \n",
            " Accuracy: 70.9%, Avg loss: 0.739810 \n",
            "\n",
            "Epoch 29\n",
            "-------------------------------\n",
            "loss: 0.786184  [    0/20380]\n",
            "Train Error: Accuracy: 72.5%\n",
            "loss: 0.609212  [ 6400/20380]\n",
            "Train Error: Accuracy: 71.3%\n",
            "loss: 0.693136  [12800/20380]\n",
            "Train Error: Accuracy: 72.4%\n",
            "loss: 0.641826  [19200/20380]\n",
            "Train Error: Accuracy: 72.4%\n",
            "Val Error: \n",
            " Accuracy: 71.4%, Avg loss: 0.731327 \n",
            "\n",
            "Epoch 30\n",
            "-------------------------------\n",
            "loss: 0.782542  [    0/20380]\n",
            "Train Error: Accuracy: 73.0%\n",
            "loss: 0.599158  [ 6400/20380]\n",
            "Train Error: Accuracy: 71.8%\n",
            "loss: 0.685682  [12800/20380]\n",
            "Train Error: Accuracy: 72.9%\n",
            "loss: 0.638499  [19200/20380]\n",
            "Train Error: Accuracy: 72.7%\n",
            "Val Error: \n",
            " Accuracy: 71.5%, Avg loss: 0.723037 \n",
            "\n",
            "Epoch 31\n",
            "-------------------------------\n",
            "loss: 0.779524  [    0/20380]\n",
            "Train Error: Accuracy: 73.3%\n",
            "loss: 0.589593  [ 6400/20380]\n",
            "Train Error: Accuracy: 72.3%\n",
            "loss: 0.678213  [12800/20380]\n",
            "Train Error: Accuracy: 73.3%\n",
            "loss: 0.635435  [19200/20380]\n",
            "Train Error: Accuracy: 73.1%\n",
            "Val Error: \n",
            " Accuracy: 72.0%, Avg loss: 0.714695 \n",
            "\n",
            "Epoch 32\n",
            "-------------------------------\n",
            "loss: 0.776969  [    0/20380]\n",
            "Train Error: Accuracy: 73.6%\n",
            "loss: 0.580376  [ 6400/20380]\n",
            "Train Error: Accuracy: 72.9%\n",
            "loss: 0.670711  [12800/20380]\n",
            "Train Error: Accuracy: 73.9%\n",
            "loss: 0.632693  [19200/20380]\n",
            "Train Error: Accuracy: 73.5%\n",
            "Val Error: \n",
            " Accuracy: 72.1%, Avg loss: 0.706228 \n",
            "\n",
            "Epoch 33\n",
            "-------------------------------\n",
            "loss: 0.774711  [    0/20380]\n",
            "Train Error: Accuracy: 74.0%\n",
            "loss: 0.571363  [ 6400/20380]\n",
            "Train Error: Accuracy: 73.3%\n",
            "loss: 0.663182  [12800/20380]\n",
            "Train Error: Accuracy: 74.4%\n",
            "loss: 0.630076  [19200/20380]\n",
            "Train Error: Accuracy: 73.9%\n",
            "Val Error: \n",
            " Accuracy: 72.4%, Avg loss: 0.697650 \n",
            "\n",
            "Epoch 34\n",
            "-------------------------------\n",
            "loss: 0.772767  [    0/20380]\n",
            "Train Error: Accuracy: 74.4%\n",
            "loss: 0.562530  [ 6400/20380]\n",
            "Train Error: Accuracy: 73.7%\n",
            "loss: 0.655586  [12800/20380]\n",
            "Train Error: Accuracy: 74.8%\n",
            "loss: 0.627556  [19200/20380]\n",
            "Train Error: Accuracy: 74.4%\n",
            "Val Error: \n",
            " Accuracy: 72.8%, Avg loss: 0.689078 \n",
            "\n",
            "Epoch 35\n",
            "-------------------------------\n",
            "loss: 0.771117  [    0/20380]\n",
            "Train Error: Accuracy: 74.8%\n",
            "loss: 0.553797  [ 6400/20380]\n",
            "Train Error: Accuracy: 74.2%\n",
            "loss: 0.647967  [12800/20380]\n",
            "Train Error: Accuracy: 75.1%\n",
            "loss: 0.625145  [19200/20380]\n",
            "Train Error: Accuracy: 74.8%\n",
            "Val Error: \n",
            " Accuracy: 73.3%, Avg loss: 0.680641 \n",
            "\n",
            "Epoch 36\n",
            "-------------------------------\n",
            "loss: 0.769611  [    0/20380]\n",
            "Train Error: Accuracy: 75.2%\n",
            "loss: 0.545154  [ 6400/20380]\n",
            "Train Error: Accuracy: 74.8%\n",
            "loss: 0.640390  [12800/20380]\n",
            "Train Error: Accuracy: 75.6%\n",
            "loss: 0.622824  [19200/20380]\n",
            "Train Error: Accuracy: 75.1%\n",
            "Val Error: \n",
            " Accuracy: 73.6%, Avg loss: 0.672447 \n",
            "\n",
            "Epoch 37\n",
            "-------------------------------\n",
            "loss: 0.768088  [    0/20380]\n",
            "Train Error: Accuracy: 75.5%\n",
            "loss: 0.536511  [ 6400/20380]\n",
            "Train Error: Accuracy: 75.1%\n",
            "loss: 0.632790  [12800/20380]\n",
            "Train Error: Accuracy: 75.9%\n",
            "loss: 0.620600  [19200/20380]\n",
            "Train Error: Accuracy: 75.5%\n",
            "Val Error: \n",
            " Accuracy: 73.9%, Avg loss: 0.664530 \n",
            "\n",
            "Epoch 38\n",
            "-------------------------------\n",
            "loss: 0.766454  [    0/20380]\n",
            "Train Error: Accuracy: 75.9%\n",
            "loss: 0.527716  [ 6400/20380]\n",
            "Train Error: Accuracy: 75.4%\n",
            "loss: 0.625209  [12800/20380]\n",
            "Train Error: Accuracy: 76.2%\n",
            "loss: 0.618432  [19200/20380]\n",
            "Train Error: Accuracy: 75.8%\n",
            "Val Error: \n",
            " Accuracy: 74.5%, Avg loss: 0.656900 \n",
            "\n",
            "Epoch 39\n",
            "-------------------------------\n",
            "loss: 0.764638  [    0/20380]\n",
            "Train Error: Accuracy: 76.3%\n",
            "loss: 0.518699  [ 6400/20380]\n",
            "Train Error: Accuracy: 75.7%\n",
            "loss: 0.617588  [12800/20380]\n",
            "Train Error: Accuracy: 76.5%\n",
            "loss: 0.616294  [19200/20380]\n",
            "Train Error: Accuracy: 76.1%\n",
            "Val Error: \n",
            " Accuracy: 74.8%, Avg loss: 0.649514 \n",
            "\n",
            "Epoch 40\n",
            "-------------------------------\n",
            "loss: 0.762587  [    0/20380]\n",
            "Train Error: Accuracy: 76.8%\n",
            "loss: 0.509447  [ 6400/20380]\n",
            "Train Error: Accuracy: 76.1%\n",
            "loss: 0.609933  [12800/20380]\n",
            "Train Error: Accuracy: 76.7%\n",
            "loss: 0.614164  [19200/20380]\n",
            "Train Error: Accuracy: 76.3%\n",
            "Val Error: \n",
            " Accuracy: 75.2%, Avg loss: 0.642359 \n",
            "\n",
            "Epoch 41\n",
            "-------------------------------\n",
            "loss: 0.760219  [    0/20380]\n",
            "Train Error: Accuracy: 77.1%\n",
            "loss: 0.500070  [ 6400/20380]\n",
            "Train Error: Accuracy: 76.5%\n",
            "loss: 0.602234  [12800/20380]\n",
            "Train Error: Accuracy: 77.1%\n",
            "loss: 0.612078  [19200/20380]\n",
            "Train Error: Accuracy: 76.6%\n",
            "Val Error: \n",
            " Accuracy: 75.6%, Avg loss: 0.635449 \n",
            "\n",
            "Epoch 42\n",
            "-------------------------------\n",
            "loss: 0.757483  [    0/20380]\n",
            "Train Error: Accuracy: 77.5%\n",
            "loss: 0.490765  [ 6400/20380]\n",
            "Train Error: Accuracy: 76.9%\n",
            "loss: 0.594641  [12800/20380]\n",
            "Train Error: Accuracy: 77.3%\n",
            "loss: 0.610012  [19200/20380]\n",
            "Train Error: Accuracy: 76.9%\n",
            "Val Error: \n",
            " Accuracy: 75.9%, Avg loss: 0.628797 \n",
            "\n",
            "Epoch 43\n",
            "-------------------------------\n",
            "loss: 0.754450  [    0/20380]\n",
            "Train Error: Accuracy: 77.7%\n",
            "loss: 0.481600  [ 6400/20380]\n",
            "Train Error: Accuracy: 77.2%\n",
            "loss: 0.587178  [12800/20380]\n",
            "Train Error: Accuracy: 77.6%\n",
            "loss: 0.607893  [19200/20380]\n",
            "Train Error: Accuracy: 77.3%\n",
            "Val Error: \n",
            " Accuracy: 76.4%, Avg loss: 0.622433 \n",
            "\n",
            "Epoch 44\n",
            "-------------------------------\n",
            "loss: 0.751196  [    0/20380]\n",
            "Train Error: Accuracy: 78.0%\n",
            "loss: 0.472627  [ 6400/20380]\n",
            "Train Error: Accuracy: 77.6%\n",
            "loss: 0.579885  [12800/20380]\n",
            "Train Error: Accuracy: 77.9%\n",
            "loss: 0.605657  [19200/20380]\n",
            "Train Error: Accuracy: 77.7%\n",
            "Val Error: \n",
            " Accuracy: 77.0%, Avg loss: 0.616344 \n",
            "\n",
            "Epoch 45\n",
            "-------------------------------\n",
            "loss: 0.747719  [    0/20380]\n",
            "Train Error: Accuracy: 78.3%\n",
            "loss: 0.463873  [ 6400/20380]\n",
            "Train Error: Accuracy: 77.9%\n",
            "loss: 0.572691  [12800/20380]\n",
            "Train Error: Accuracy: 78.1%\n",
            "loss: 0.603201  [19200/20380]\n",
            "Train Error: Accuracy: 78.0%\n",
            "Val Error: \n",
            " Accuracy: 77.5%, Avg loss: 0.610491 \n",
            "\n",
            "Epoch 46\n",
            "-------------------------------\n",
            "loss: 0.744013  [    0/20380]\n",
            "Train Error: Accuracy: 78.4%\n",
            "loss: 0.455321  [ 6400/20380]\n",
            "Train Error: Accuracy: 78.1%\n",
            "loss: 0.565526  [12800/20380]\n",
            "Train Error: Accuracy: 78.3%\n",
            "loss: 0.600597  [19200/20380]\n",
            "Train Error: Accuracy: 78.3%\n",
            "Val Error: \n",
            " Accuracy: 78.0%, Avg loss: 0.604845 \n",
            "\n",
            "Epoch 47\n",
            "-------------------------------\n",
            "loss: 0.740126  [    0/20380]\n",
            "Train Error: Accuracy: 78.6%\n",
            "loss: 0.446953  [ 6400/20380]\n",
            "Train Error: Accuracy: 78.3%\n",
            "loss: 0.558358  [12800/20380]\n",
            "Train Error: Accuracy: 78.5%\n",
            "loss: 0.597947  [19200/20380]\n",
            "Train Error: Accuracy: 78.6%\n",
            "Val Error: \n",
            " Accuracy: 78.3%, Avg loss: 0.599408 \n",
            "\n",
            "Epoch 48\n",
            "-------------------------------\n",
            "loss: 0.736108  [    0/20380]\n",
            "Train Error: Accuracy: 78.8%\n",
            "loss: 0.438797  [ 6400/20380]\n",
            "Train Error: Accuracy: 78.5%\n",
            "loss: 0.551190  [12800/20380]\n",
            "Train Error: Accuracy: 78.8%\n",
            "loss: 0.595273  [19200/20380]\n",
            "Train Error: Accuracy: 78.8%\n",
            "Val Error: \n",
            " Accuracy: 78.5%, Avg loss: 0.594174 \n",
            "\n",
            "Epoch 49\n",
            "-------------------------------\n",
            "loss: 0.731989  [    0/20380]\n",
            "Train Error: Accuracy: 78.9%\n",
            "loss: 0.430913  [ 6400/20380]\n",
            "Train Error: Accuracy: 78.7%\n",
            "loss: 0.544054  [12800/20380]\n",
            "Train Error: Accuracy: 79.0%\n",
            "loss: 0.592508  [19200/20380]\n",
            "Train Error: Accuracy: 79.0%\n",
            "Val Error: \n",
            " Accuracy: 78.8%, Avg loss: 0.589137 \n",
            "\n",
            "Epoch 50\n",
            "-------------------------------\n",
            "loss: 0.727807  [    0/20380]\n",
            "Train Error: Accuracy: 79.1%\n",
            "loss: 0.423338  [ 6400/20380]\n",
            "Train Error: Accuracy: 78.9%\n",
            "loss: 0.536970  [12800/20380]\n",
            "Train Error: Accuracy: 79.3%\n",
            "loss: 0.589684  [19200/20380]\n",
            "Train Error: Accuracy: 79.3%\n",
            "Val Error: \n",
            " Accuracy: 79.0%, Avg loss: 0.584285 \n",
            "\n",
            "Epoch 51\n",
            "-------------------------------\n",
            "loss: 0.723558  [    0/20380]\n",
            "Train Error: Accuracy: 79.3%\n",
            "loss: 0.416120  [ 6400/20380]\n",
            "Train Error: Accuracy: 79.1%\n",
            "loss: 0.529963  [12800/20380]\n",
            "Train Error: Accuracy: 79.5%\n",
            "loss: 0.586859  [19200/20380]\n",
            "Train Error: Accuracy: 79.4%\n",
            "Val Error: \n",
            " Accuracy: 79.1%, Avg loss: 0.579586 \n",
            "\n",
            "Epoch 52\n",
            "-------------------------------\n",
            "loss: 0.719294  [    0/20380]\n",
            "Train Error: Accuracy: 79.5%\n",
            "loss: 0.409279  [ 6400/20380]\n",
            "Train Error: Accuracy: 79.3%\n",
            "loss: 0.523108  [12800/20380]\n",
            "Train Error: Accuracy: 79.6%\n",
            "loss: 0.584019  [19200/20380]\n",
            "Train Error: Accuracy: 79.6%\n",
            "Val Error: \n",
            " Accuracy: 79.2%, Avg loss: 0.575038 \n",
            "\n",
            "Epoch 53\n",
            "-------------------------------\n",
            "loss: 0.715173  [    0/20380]\n",
            "Train Error: Accuracy: 79.6%\n",
            "loss: 0.402676  [ 6400/20380]\n",
            "Train Error: Accuracy: 79.4%\n",
            "loss: 0.516604  [12800/20380]\n",
            "Train Error: Accuracy: 79.8%\n",
            "loss: 0.581151  [19200/20380]\n",
            "Train Error: Accuracy: 79.7%\n",
            "Val Error: \n",
            " Accuracy: 79.3%, Avg loss: 0.570664 \n",
            "\n",
            "Epoch 54\n",
            "-------------------------------\n",
            "loss: 0.711164  [    0/20380]\n",
            "Train Error: Accuracy: 79.7%\n",
            "loss: 0.396395  [ 6400/20380]\n",
            "Train Error: Accuracy: 79.5%\n",
            "loss: 0.510385  [12800/20380]\n",
            "Train Error: Accuracy: 80.0%\n",
            "loss: 0.578258  [19200/20380]\n",
            "Train Error: Accuracy: 79.8%\n",
            "Val Error: \n",
            " Accuracy: 79.5%, Avg loss: 0.566504 \n",
            "\n",
            "Epoch 55\n",
            "-------------------------------\n",
            "loss: 0.707257  [    0/20380]\n",
            "Train Error: Accuracy: 79.8%\n",
            "loss: 0.390335  [ 6400/20380]\n",
            "Train Error: Accuracy: 79.7%\n",
            "loss: 0.504392  [12800/20380]\n",
            "Train Error: Accuracy: 80.2%\n",
            "loss: 0.575272  [19200/20380]\n",
            "Train Error: Accuracy: 80.0%\n",
            "Val Error: \n",
            " Accuracy: 79.7%, Avg loss: 0.562568 \n",
            "\n",
            "Epoch 56\n",
            "-------------------------------\n",
            "loss: 0.703424  [    0/20380]\n",
            "Train Error: Accuracy: 80.0%\n",
            "loss: 0.384374  [ 6400/20380]\n",
            "Train Error: Accuracy: 79.8%\n",
            "loss: 0.498654  [12800/20380]\n",
            "Train Error: Accuracy: 80.3%\n",
            "loss: 0.572112  [19200/20380]\n",
            "Train Error: Accuracy: 80.1%\n",
            "Val Error: \n",
            " Accuracy: 79.9%, Avg loss: 0.558849 \n",
            "\n",
            "Epoch 57\n",
            "-------------------------------\n",
            "loss: 0.699724  [    0/20380]\n",
            "Train Error: Accuracy: 80.1%\n",
            "loss: 0.378499  [ 6400/20380]\n",
            "Train Error: Accuracy: 79.9%\n",
            "loss: 0.493149  [12800/20380]\n",
            "Train Error: Accuracy: 80.4%\n",
            "loss: 0.568796  [19200/20380]\n",
            "Train Error: Accuracy: 80.2%\n",
            "Val Error: \n",
            " Accuracy: 80.1%, Avg loss: 0.555343 \n",
            "\n",
            "Epoch 58\n",
            "-------------------------------\n",
            "loss: 0.696202  [    0/20380]\n",
            "Train Error: Accuracy: 80.2%\n",
            "loss: 0.372836  [ 6400/20380]\n",
            "Train Error: Accuracy: 80.0%\n",
            "loss: 0.487826  [12800/20380]\n",
            "Train Error: Accuracy: 80.6%\n",
            "loss: 0.565369  [19200/20380]\n",
            "Train Error: Accuracy: 80.3%\n",
            "Val Error: \n",
            " Accuracy: 80.2%, Avg loss: 0.552053 \n",
            "\n",
            "Epoch 59\n",
            "-------------------------------\n",
            "loss: 0.692831  [    0/20380]\n",
            "Train Error: Accuracy: 80.3%\n",
            "loss: 0.367441  [ 6400/20380]\n",
            "Train Error: Accuracy: 80.2%\n",
            "loss: 0.482646  [12800/20380]\n",
            "Train Error: Accuracy: 80.7%\n",
            "loss: 0.561807  [19200/20380]\n",
            "Train Error: Accuracy: 80.4%\n",
            "Val Error: \n",
            " Accuracy: 80.2%, Avg loss: 0.548985 \n",
            "\n",
            "Epoch 60\n",
            "-------------------------------\n",
            "loss: 0.689613  [    0/20380]\n",
            "Train Error: Accuracy: 80.3%\n",
            "loss: 0.362326  [ 6400/20380]\n",
            "Train Error: Accuracy: 80.4%\n",
            "loss: 0.477604  [12800/20380]\n",
            "Train Error: Accuracy: 80.9%\n",
            "loss: 0.558113  [19200/20380]\n",
            "Train Error: Accuracy: 80.4%\n",
            "Val Error: \n",
            " Accuracy: 80.3%, Avg loss: 0.546109 \n",
            "\n",
            "Epoch 61\n",
            "-------------------------------\n",
            "loss: 0.686496  [    0/20380]\n",
            "Train Error: Accuracy: 80.4%\n",
            "loss: 0.357462  [ 6400/20380]\n",
            "Train Error: Accuracy: 80.5%\n",
            "loss: 0.472695  [12800/20380]\n",
            "Train Error: Accuracy: 81.0%\n",
            "loss: 0.554215  [19200/20380]\n",
            "Train Error: Accuracy: 80.5%\n",
            "Val Error: \n",
            " Accuracy: 80.3%, Avg loss: 0.543418 \n",
            "\n",
            "Epoch 62\n",
            "-------------------------------\n",
            "loss: 0.683459  [    0/20380]\n",
            "Train Error: Accuracy: 80.5%\n",
            "loss: 0.352817  [ 6400/20380]\n",
            "Train Error: Accuracy: 80.7%\n",
            "loss: 0.467931  [12800/20380]\n",
            "Train Error: Accuracy: 81.2%\n",
            "loss: 0.550073  [19200/20380]\n",
            "Train Error: Accuracy: 80.5%\n",
            "Val Error: \n",
            " Accuracy: 80.5%, Avg loss: 0.540899 \n",
            "\n",
            "Epoch 63\n",
            "-------------------------------\n",
            "loss: 0.680516  [    0/20380]\n",
            "Train Error: Accuracy: 80.6%\n",
            "loss: 0.348336  [ 6400/20380]\n",
            "Train Error: Accuracy: 80.8%\n",
            "loss: 0.463275  [12800/20380]\n",
            "Train Error: Accuracy: 81.3%\n",
            "loss: 0.545799  [19200/20380]\n",
            "Train Error: Accuracy: 80.6%\n",
            "Val Error: \n",
            " Accuracy: 80.7%, Avg loss: 0.538540 \n",
            "\n",
            "Epoch 64\n",
            "-------------------------------\n",
            "loss: 0.677567  [    0/20380]\n",
            "Train Error: Accuracy: 80.7%\n",
            "loss: 0.343981  [ 6400/20380]\n",
            "Train Error: Accuracy: 80.9%\n",
            "loss: 0.458678  [12800/20380]\n",
            "Train Error: Accuracy: 81.5%\n",
            "loss: 0.541323  [19200/20380]\n",
            "Train Error: Accuracy: 80.7%\n",
            "Val Error: \n",
            " Accuracy: 80.9%, Avg loss: 0.536314 \n",
            "\n",
            "Epoch 65\n",
            "-------------------------------\n",
            "loss: 0.674507  [    0/20380]\n",
            "Train Error: Accuracy: 80.8%\n",
            "loss: 0.339719  [ 6400/20380]\n",
            "Train Error: Accuracy: 81.1%\n",
            "loss: 0.454098  [12800/20380]\n",
            "Train Error: Accuracy: 81.6%\n",
            "loss: 0.536525  [19200/20380]\n",
            "Train Error: Accuracy: 80.8%\n",
            "Val Error: \n",
            " Accuracy: 81.0%, Avg loss: 0.534222 \n",
            "\n",
            "Epoch 66\n",
            "-------------------------------\n",
            "loss: 0.671315  [    0/20380]\n",
            "Train Error: Accuracy: 80.8%\n",
            "loss: 0.335533  [ 6400/20380]\n",
            "Train Error: Accuracy: 81.2%\n",
            "loss: 0.449564  [12800/20380]\n",
            "Train Error: Accuracy: 81.7%\n",
            "loss: 0.531424  [19200/20380]\n",
            "Train Error: Accuracy: 80.9%\n",
            "Val Error: \n",
            " Accuracy: 81.2%, Avg loss: 0.532254 \n",
            "\n",
            "Epoch 67\n",
            "-------------------------------\n",
            "loss: 0.668024  [    0/20380]\n",
            "Train Error: Accuracy: 80.9%\n",
            "loss: 0.331446  [ 6400/20380]\n",
            "Train Error: Accuracy: 81.3%\n",
            "loss: 0.445208  [12800/20380]\n",
            "Train Error: Accuracy: 81.8%\n",
            "loss: 0.526124  [19200/20380]\n",
            "Train Error: Accuracy: 81.1%\n",
            "Val Error: \n",
            " Accuracy: 81.2%, Avg loss: 0.530391 \n",
            "\n",
            "Epoch 68\n",
            "-------------------------------\n",
            "loss: 0.664824  [    0/20380]\n",
            "Train Error: Accuracy: 81.1%\n",
            "loss: 0.327462  [ 6400/20380]\n",
            "Train Error: Accuracy: 81.4%\n",
            "loss: 0.441106  [12800/20380]\n",
            "Train Error: Accuracy: 81.9%\n",
            "loss: 0.520768  [19200/20380]\n",
            "Train Error: Accuracy: 81.2%\n",
            "Val Error: \n",
            " Accuracy: 81.3%, Avg loss: 0.528616 \n",
            "\n",
            "Epoch 69\n",
            "-------------------------------\n",
            "loss: 0.661772  [    0/20380]\n",
            "Train Error: Accuracy: 81.1%\n",
            "loss: 0.323612  [ 6400/20380]\n",
            "Train Error: Accuracy: 81.4%\n",
            "loss: 0.437232  [12800/20380]\n",
            "Train Error: Accuracy: 82.0%\n",
            "loss: 0.515526  [19200/20380]\n",
            "Train Error: Accuracy: 81.3%\n",
            "Val Error: \n",
            " Accuracy: 81.5%, Avg loss: 0.526921 \n",
            "\n",
            "Epoch 70\n",
            "-------------------------------\n",
            "loss: 0.658858  [    0/20380]\n",
            "Train Error: Accuracy: 81.2%\n",
            "loss: 0.320052  [ 6400/20380]\n",
            "Train Error: Accuracy: 81.6%\n",
            "loss: 0.433585  [12800/20380]\n",
            "Train Error: Accuracy: 82.1%\n",
            "loss: 0.510337  [19200/20380]\n",
            "Train Error: Accuracy: 81.4%\n",
            "Val Error: \n",
            " Accuracy: 81.5%, Avg loss: 0.525277 \n",
            "\n",
            "Epoch 71\n",
            "-------------------------------\n",
            "loss: 0.656069  [    0/20380]\n",
            "Train Error: Accuracy: 81.3%\n",
            "loss: 0.316729  [ 6400/20380]\n",
            "Train Error: Accuracy: 81.6%\n",
            "loss: 0.430105  [12800/20380]\n",
            "Train Error: Accuracy: 82.2%\n",
            "loss: 0.505187  [19200/20380]\n",
            "Train Error: Accuracy: 81.5%\n",
            "Val Error: \n",
            " Accuracy: 81.4%, Avg loss: 0.523694 \n",
            "\n",
            "Epoch 72\n",
            "-------------------------------\n",
            "loss: 0.653389  [    0/20380]\n",
            "Train Error: Accuracy: 81.3%\n",
            "loss: 0.313584  [ 6400/20380]\n",
            "Train Error: Accuracy: 81.7%\n",
            "loss: 0.426764  [12800/20380]\n",
            "Train Error: Accuracy: 82.3%\n",
            "loss: 0.500034  [19200/20380]\n",
            "Train Error: Accuracy: 81.6%\n",
            "Val Error: \n",
            " Accuracy: 81.4%, Avg loss: 0.522179 \n",
            "\n",
            "Epoch 73\n",
            "-------------------------------\n",
            "loss: 0.650812  [    0/20380]\n",
            "Train Error: Accuracy: 81.5%\n",
            "loss: 0.310615  [ 6400/20380]\n",
            "Train Error: Accuracy: 81.8%\n",
            "loss: 0.423546  [12800/20380]\n",
            "Train Error: Accuracy: 82.4%\n",
            "loss: 0.494837  [19200/20380]\n",
            "Train Error: Accuracy: 81.7%\n",
            "Val Error: \n",
            " Accuracy: 81.5%, Avg loss: 0.520717 \n",
            "\n",
            "Validation no longer rising. Done!\n",
            "Max number of epochs hit or validation stop reached. Done!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Final Configuration - ELU, 2 Layer, 256 Nodes"
      ],
      "metadata": {
        "id": "Tepo7Jtsrfyd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#check what device we're using\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"Using {device} device\")\n",
        "torch.set_grad_enabled(True) "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "24c0f695-83ef-42b0-cd8b-32828186a134",
        "id": "CNvqO7Gtrm9J"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using cpu device\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch.autograd.grad_mode.set_grad_enabled at 0x7f260f23bcd0>"
            ]
          },
          "metadata": {},
          "execution_count": 130
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class NeuralNetwork(nn.Module):\n",
        "    def __init__(self):\n",
        "        #super(NeuralNetwork, self).__init__()\n",
        "        super().__init__()\n",
        "        self.linear_relu_stack = nn.Sequential(\n",
        "            nn.Linear(7, 256), #input layer \n",
        "            nn.ELU(), # relu for simplicity?\n",
        "            nn.Linear(256, 256), #hidden layer 1\n",
        "            nn.ELU(),\n",
        "            nn.Linear(256, 256), #hidden layer 2\n",
        "            nn.ELU(),\n",
        "            nn.Linear(256, 5), #output layer\n",
        "        )\n",
        "    # \"Every nn.Module subclass implements the operations \n",
        "    # on input data in the forward method.\"\n",
        "    # We already preprocessed the input data\n",
        "    # so we just return x\n",
        "    def forward(self, x): \n",
        "        logits = self.linear_relu_stack(x) #we already preprocessed the data, so we just pass it into the NN here\n",
        "        return logits"
      ],
      "metadata": {
        "id": "P0gPOQZyrm9K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Implement forward propagation (clearly describe the activation functions and other\n",
        "hyper-parameters you are using)."
      ],
      "metadata": {
        "id": "o8ZEZlkMrm9K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = NeuralNetwork().to(device)\n",
        "print(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f7de6b09-18c7-499b-e1f7-4112a4c97a0a",
        "id": "TMpZHtE8rm9M"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NeuralNetwork(\n",
            "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
            "  (linear_relu_stack): Sequential(\n",
            "    (0): Linear(in_features=7, out_features=256, bias=True)\n",
            "    (1): ELU(alpha=1.0)\n",
            "    (2): Linear(in_features=256, out_features=256, bias=True)\n",
            "    (3): ELU(alpha=1.0)\n",
            "    (4): Linear(in_features=256, out_features=256, bias=True)\n",
            "    (5): ELU(alpha=1.0)\n",
            "    (6): Linear(in_features=256, out_features=5, bias=True)\n",
            "  )\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "4. Compute the final cost function.\n",
        "\n",
        "5. Implement gradient descent (any variant of gradient descent depending upon your\n",
        "data and project can be used) to train your model. In this step it is up to you as someone\n",
        "in charge of their project to improvise using optimization algorithms (Adams, RMSProp\n",
        "etc.) and/or regularization."
      ],
      "metadata": {
        "id": "pV2tUAP2rm9M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=1e-2) #from 1e-3"
      ],
      "metadata": {
        "id": "3rSdwxKHrm9N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Code snippet attribution:\n",
        "#https://pytorch.org/tutorials/beginner/basics/intro.html\n",
        "#was used as part of the research process\n",
        "#Modifications were made so that data could be processed as longs,\n",
        "#additional statitics could be reported, \n",
        "#and a separate validate process was created. \n",
        "def train(dataloader, model, loss_fn, optimizer):\n",
        "    size = len(dataloader.dataset)\n",
        "    model.train()\n",
        "    for batch, (X, y) in enumerate(dataloader):\n",
        "        X, y = X.to(device), y.to(device)\n",
        "\n",
        "        # Compute prediction error\n",
        "        pred = model(X)\n",
        "        loss = loss_fn(pred, y.long()) \n",
        "\n",
        "\n",
        "        # Backpropagation\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if batch % 100 == 0:\n",
        "            loss, current = loss.item(), batch * len(X)\n",
        "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
        "            correct = 0\n",
        "            with torch.no_grad():\n",
        "                for X, y in dataloader:\n",
        "                    X, y = X.to(device), y.to(device)\n",
        "                    pred = model(X)\n",
        "                    correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
        "            correct /= size\n",
        "            print(f\"Train Error: Accuracy: {(100*correct):>0.1f}%\")\n",
        "\n",
        "\n",
        "def validate(dataloader, model, loss_fn):\n",
        "    size = len(dataloader.dataset)\n",
        "    num_batches = len(dataloader)\n",
        "    model.eval()\n",
        "    test_loss, correct = 0, 0\n",
        "    with torch.no_grad():\n",
        "        for X, y in dataloader:\n",
        "            X, y = X.to(device), y.to(device)\n",
        "            pred = model(X)\n",
        "            test_loss += loss_fn(pred, y.long()).item()\n",
        "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
        "    test_loss /= num_batches\n",
        "    correct /= size\n",
        "    print(f\"Val Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n",
        "    return correct\n",
        "\n",
        "def test(dataloader, model, loss_fn):\n",
        "    size = len(dataloader.dataset)\n",
        "    num_batches = len(dataloader)\n",
        "    model.eval()\n",
        "    test_loss, correct = 0, 0\n",
        "    with torch.no_grad():\n",
        "        for X, y in dataloader:\n",
        "            X, y = X.to(device), y.to(device)\n",
        "            pred = model(X)\n",
        "            test_loss += loss_fn(pred, y.long()).item()\n",
        "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
        "    test_loss /= num_batches\n",
        "    correct /= size\n",
        "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"
      ],
      "metadata": {
        "id": "Otl4Xn_7rm9N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 100\n",
        "bestval = 0\n",
        "decval = 0\n",
        "for t in range(epochs):\n",
        "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
        "    train(train_dataloader, model, loss_fn, optimizer)\n",
        "    #TODO VALIDATE\n",
        "    tempval = validate(val_dataloader, model, loss_fn)\n",
        "    if(tempval < bestval):\n",
        "      decval+=1 \n",
        "      if(decval==3):\n",
        "        print(\"Validation no longer rising. Done!\")\n",
        "        break\n",
        "    else:\n",
        "      bestval=tempval\n",
        "print(\"Max number of epochs hit or validation stop reached. Done!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "09d43e5b-1055-437e-8024-f7f8d8abb6c0",
        "id": "dM3JNmktrm9P"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1\n",
            "-------------------------------\n",
            "loss: 1.600311  [    0/20380]\n",
            "Train Error: Accuracy: 24.5%\n",
            "loss: 1.429459  [ 6400/20380]\n",
            "Train Error: Accuracy: 42.3%\n",
            "loss: 1.358910  [12800/20380]\n",
            "Train Error: Accuracy: 45.8%\n",
            "loss: 1.093532  [19200/20380]\n",
            "Train Error: Accuracy: 45.4%\n",
            "Val Error: \n",
            " Accuracy: 45.4%, Avg loss: 1.266747 \n",
            "\n",
            "Epoch 2\n",
            "-------------------------------\n",
            "loss: 1.230521  [    0/20380]\n",
            "Train Error: Accuracy: 45.9%\n",
            "loss: 1.301010  [ 6400/20380]\n",
            "Train Error: Accuracy: 45.9%\n",
            "loss: 1.300360  [12800/20380]\n",
            "Train Error: Accuracy: 46.1%\n",
            "loss: 1.010393  [19200/20380]\n",
            "Train Error: Accuracy: 46.1%\n",
            "Val Error: \n",
            " Accuracy: 45.4%, Avg loss: 1.221017 \n",
            "\n",
            "Epoch 3\n",
            "-------------------------------\n",
            "loss: 1.195560  [    0/20380]\n",
            "Train Error: Accuracy: 46.4%\n",
            "loss: 1.258553  [ 6400/20380]\n",
            "Train Error: Accuracy: 46.5%\n",
            "loss: 1.267096  [12800/20380]\n",
            "Train Error: Accuracy: 46.5%\n",
            "loss: 0.966060  [19200/20380]\n",
            "Train Error: Accuracy: 46.2%\n",
            "Val Error: \n",
            " Accuracy: 45.8%, Avg loss: 1.188846 \n",
            "\n",
            "Epoch 4\n",
            "-------------------------------\n",
            "loss: 1.178552  [    0/20380]\n",
            "Train Error: Accuracy: 46.0%\n",
            "loss: 1.222952  [ 6400/20380]\n",
            "Train Error: Accuracy: 46.6%\n",
            "loss: 1.235240  [12800/20380]\n",
            "Train Error: Accuracy: 46.4%\n",
            "loss: 0.935745  [19200/20380]\n",
            "Train Error: Accuracy: 46.7%\n",
            "Val Error: \n",
            " Accuracy: 46.5%, Avg loss: 1.159179 \n",
            "\n",
            "Epoch 5\n",
            "-------------------------------\n",
            "loss: 1.160651  [    0/20380]\n",
            "Train Error: Accuracy: 46.2%\n",
            "loss: 1.190811  [ 6400/20380]\n",
            "Train Error: Accuracy: 46.6%\n",
            "loss: 1.206961  [12800/20380]\n",
            "Train Error: Accuracy: 47.0%\n",
            "loss: 0.912333  [19200/20380]\n",
            "Train Error: Accuracy: 47.2%\n",
            "Val Error: \n",
            " Accuracy: 47.2%, Avg loss: 1.131704 \n",
            "\n",
            "Epoch 6\n",
            "-------------------------------\n",
            "loss: 1.143054  [    0/20380]\n",
            "Train Error: Accuracy: 46.6%\n",
            "loss: 1.164982  [ 6400/20380]\n",
            "Train Error: Accuracy: 47.2%\n",
            "loss: 1.183598  [12800/20380]\n",
            "Train Error: Accuracy: 47.9%\n",
            "loss: 0.893062  [19200/20380]\n",
            "Train Error: Accuracy: 48.2%\n",
            "Val Error: \n",
            " Accuracy: 48.3%, Avg loss: 1.107701 \n",
            "\n",
            "Epoch 7\n",
            "-------------------------------\n",
            "loss: 1.126361  [    0/20380]\n",
            "Train Error: Accuracy: 47.7%\n",
            "loss: 1.145937  [ 6400/20380]\n",
            "Train Error: Accuracy: 48.5%\n",
            "loss: 1.162936  [12800/20380]\n",
            "Train Error: Accuracy: 49.0%\n",
            "loss: 0.877281  [19200/20380]\n",
            "Train Error: Accuracy: 49.3%\n",
            "Val Error: \n",
            " Accuracy: 49.4%, Avg loss: 1.087258 \n",
            "\n",
            "Epoch 8\n",
            "-------------------------------\n",
            "loss: 1.110714  [    0/20380]\n",
            "Train Error: Accuracy: 48.7%\n",
            "loss: 1.130198  [ 6400/20380]\n",
            "Train Error: Accuracy: 49.6%\n",
            "loss: 1.141799  [12800/20380]\n",
            "Train Error: Accuracy: 50.3%\n",
            "loss: 0.864282  [19200/20380]\n",
            "Train Error: Accuracy: 50.1%\n",
            "Val Error: \n",
            " Accuracy: 51.8%, Avg loss: 1.069081 \n",
            "\n",
            "Epoch 9\n",
            "-------------------------------\n",
            "loss: 1.096395  [    0/20380]\n",
            "Train Error: Accuracy: 49.7%\n",
            "loss: 1.114092  [ 6400/20380]\n",
            "Train Error: Accuracy: 50.7%\n",
            "loss: 1.118804  [12800/20380]\n",
            "Train Error: Accuracy: 51.2%\n",
            "loss: 0.852690  [19200/20380]\n",
            "Train Error: Accuracy: 50.8%\n",
            "Val Error: \n",
            " Accuracy: 53.6%, Avg loss: 1.051843 \n",
            "\n",
            "Epoch 10\n",
            "-------------------------------\n",
            "loss: 1.083225  [    0/20380]\n",
            "Train Error: Accuracy: 51.8%\n",
            "loss: 1.095471  [ 6400/20380]\n",
            "Train Error: Accuracy: 51.7%\n",
            "loss: 1.094087  [12800/20380]\n",
            "Train Error: Accuracy: 52.4%\n",
            "loss: 0.841325  [19200/20380]\n",
            "Train Error: Accuracy: 52.0%\n",
            "Val Error: \n",
            " Accuracy: 55.4%, Avg loss: 1.034723 \n",
            "\n",
            "Epoch 11\n",
            "-------------------------------\n",
            "loss: 1.070647  [    0/20380]\n",
            "Train Error: Accuracy: 53.8%\n",
            "loss: 1.073748  [ 6400/20380]\n",
            "Train Error: Accuracy: 52.6%\n",
            "loss: 1.068273  [12800/20380]\n",
            "Train Error: Accuracy: 54.4%\n",
            "loss: 0.829634  [19200/20380]\n",
            "Train Error: Accuracy: 53.9%\n",
            "Val Error: \n",
            " Accuracy: 56.9%, Avg loss: 1.017375 \n",
            "\n",
            "Epoch 12\n",
            "-------------------------------\n",
            "loss: 1.057859  [    0/20380]\n",
            "Train Error: Accuracy: 55.3%\n",
            "loss: 1.049281  [ 6400/20380]\n",
            "Train Error: Accuracy: 54.2%\n",
            "loss: 1.041809  [12800/20380]\n",
            "Train Error: Accuracy: 56.0%\n",
            "loss: 0.817506  [19200/20380]\n",
            "Train Error: Accuracy: 55.7%\n",
            "Val Error: \n",
            " Accuracy: 57.7%, Avg loss: 0.999785 \n",
            "\n",
            "Epoch 13\n",
            "-------------------------------\n",
            "loss: 1.043940  [    0/20380]\n",
            "Train Error: Accuracy: 56.5%\n",
            "loss: 1.022840  [ 6400/20380]\n",
            "Train Error: Accuracy: 55.7%\n",
            "loss: 1.014961  [12800/20380]\n",
            "Train Error: Accuracy: 57.3%\n",
            "loss: 0.804856  [19200/20380]\n",
            "Train Error: Accuracy: 57.0%\n",
            "Val Error: \n",
            " Accuracy: 58.4%, Avg loss: 0.982228 \n",
            "\n",
            "Epoch 14\n",
            "-------------------------------\n",
            "loss: 1.028289  [    0/20380]\n",
            "Train Error: Accuracy: 57.5%\n",
            "loss: 0.995577  [ 6400/20380]\n",
            "Train Error: Accuracy: 56.9%\n",
            "loss: 0.988102  [12800/20380]\n",
            "Train Error: Accuracy: 58.3%\n",
            "loss: 0.792039  [19200/20380]\n",
            "Train Error: Accuracy: 58.1%\n",
            "Val Error: \n",
            " Accuracy: 59.0%, Avg loss: 0.965102 \n",
            "\n",
            "Epoch 15\n",
            "-------------------------------\n",
            "loss: 1.010703  [    0/20380]\n",
            "Train Error: Accuracy: 58.4%\n",
            "loss: 0.968352  [ 6400/20380]\n",
            "Train Error: Accuracy: 57.8%\n",
            "loss: 0.962086  [12800/20380]\n",
            "Train Error: Accuracy: 59.2%\n",
            "loss: 0.780011  [19200/20380]\n",
            "Train Error: Accuracy: 59.0%\n",
            "Val Error: \n",
            " Accuracy: 59.9%, Avg loss: 0.948594 \n",
            "\n",
            "Epoch 16\n",
            "-------------------------------\n",
            "loss: 0.991939  [    0/20380]\n",
            "Train Error: Accuracy: 59.3%\n",
            "loss: 0.940910  [ 6400/20380]\n",
            "Train Error: Accuracy: 58.6%\n",
            "loss: 0.937646  [12800/20380]\n",
            "Train Error: Accuracy: 60.5%\n",
            "loss: 0.768988  [19200/20380]\n",
            "Train Error: Accuracy: 60.1%\n",
            "Val Error: \n",
            " Accuracy: 60.6%, Avg loss: 0.932846 \n",
            "\n",
            "Epoch 17\n",
            "-------------------------------\n",
            "loss: 0.973480  [    0/20380]\n",
            "Train Error: Accuracy: 60.2%\n",
            "loss: 0.913512  [ 6400/20380]\n",
            "Train Error: Accuracy: 59.5%\n",
            "loss: 0.915080  [12800/20380]\n",
            "Train Error: Accuracy: 61.4%\n",
            "loss: 0.758990  [19200/20380]\n",
            "Train Error: Accuracy: 61.1%\n",
            "Val Error: \n",
            " Accuracy: 61.2%, Avg loss: 0.917921 \n",
            "\n",
            "Epoch 18\n",
            "-------------------------------\n",
            "loss: 0.956027  [    0/20380]\n",
            "Train Error: Accuracy: 61.1%\n",
            "loss: 0.886869  [ 6400/20380]\n",
            "Train Error: Accuracy: 60.6%\n",
            "loss: 0.894288  [12800/20380]\n",
            "Train Error: Accuracy: 62.4%\n",
            "loss: 0.749882  [19200/20380]\n",
            "Train Error: Accuracy: 61.6%\n",
            "Val Error: \n",
            " Accuracy: 62.1%, Avg loss: 0.903723 \n",
            "\n",
            "Epoch 19\n",
            "-------------------------------\n",
            "loss: 0.939786  [    0/20380]\n",
            "Train Error: Accuracy: 61.9%\n",
            "loss: 0.861511  [ 6400/20380]\n",
            "Train Error: Accuracy: 61.5%\n",
            "loss: 0.874929  [12800/20380]\n",
            "Train Error: Accuracy: 63.1%\n",
            "loss: 0.741304  [19200/20380]\n",
            "Train Error: Accuracy: 62.3%\n",
            "Val Error: \n",
            " Accuracy: 62.8%, Avg loss: 0.890039 \n",
            "\n",
            "Epoch 20\n",
            "-------------------------------\n",
            "loss: 0.924665  [    0/20380]\n",
            "Train Error: Accuracy: 62.6%\n",
            "loss: 0.837658  [ 6400/20380]\n",
            "Train Error: Accuracy: 62.1%\n",
            "loss: 0.856646  [12800/20380]\n",
            "Train Error: Accuracy: 63.6%\n",
            "loss: 0.732825  [19200/20380]\n",
            "Train Error: Accuracy: 63.1%\n",
            "Val Error: \n",
            " Accuracy: 63.3%, Avg loss: 0.876607 \n",
            "\n",
            "Epoch 21\n",
            "-------------------------------\n",
            "loss: 0.910478  [    0/20380]\n",
            "Train Error: Accuracy: 63.4%\n",
            "loss: 0.815243  [ 6400/20380]\n",
            "Train Error: Accuracy: 62.6%\n",
            "loss: 0.839095  [12800/20380]\n",
            "Train Error: Accuracy: 64.7%\n",
            "loss: 0.724144  [19200/20380]\n",
            "Train Error: Accuracy: 63.9%\n",
            "Val Error: \n",
            " Accuracy: 64.2%, Avg loss: 0.863195 \n",
            "\n",
            "Epoch 22\n",
            "-------------------------------\n",
            "loss: 0.897053  [    0/20380]\n",
            "Train Error: Accuracy: 64.2%\n",
            "loss: 0.794031  [ 6400/20380]\n",
            "Train Error: Accuracy: 63.3%\n",
            "loss: 0.822020  [12800/20380]\n",
            "Train Error: Accuracy: 65.9%\n",
            "loss: 0.715109  [19200/20380]\n",
            "Train Error: Accuracy: 65.1%\n",
            "Val Error: \n",
            " Accuracy: 65.6%, Avg loss: 0.849691 \n",
            "\n",
            "Epoch 23\n",
            "-------------------------------\n",
            "loss: 0.884336  [    0/20380]\n",
            "Train Error: Accuracy: 65.5%\n",
            "loss: 0.773784  [ 6400/20380]\n",
            "Train Error: Accuracy: 64.1%\n",
            "loss: 0.805431  [12800/20380]\n",
            "Train Error: Accuracy: 66.8%\n",
            "loss: 0.705821  [19200/20380]\n",
            "Train Error: Accuracy: 66.3%\n",
            "Val Error: \n",
            " Accuracy: 66.9%, Avg loss: 0.836159 \n",
            "\n",
            "Epoch 24\n",
            "-------------------------------\n",
            "loss: 0.872421  [    0/20380]\n",
            "Train Error: Accuracy: 67.2%\n",
            "loss: 0.754338  [ 6400/20380]\n",
            "Train Error: Accuracy: 65.2%\n",
            "loss: 0.789641  [12800/20380]\n",
            "Train Error: Accuracy: 67.2%\n",
            "loss: 0.696552  [19200/20380]\n",
            "Train Error: Accuracy: 66.9%\n",
            "Val Error: \n",
            " Accuracy: 67.8%, Avg loss: 0.822835 \n",
            "\n",
            "Epoch 25\n",
            "-------------------------------\n",
            "loss: 0.861419  [    0/20380]\n",
            "Train Error: Accuracy: 68.2%\n",
            "loss: 0.735794  [ 6400/20380]\n",
            "Train Error: Accuracy: 66.1%\n",
            "loss: 0.775116  [12800/20380]\n",
            "Train Error: Accuracy: 67.7%\n",
            "loss: 0.687656  [19200/20380]\n",
            "Train Error: Accuracy: 67.6%\n",
            "Val Error: \n",
            " Accuracy: 68.8%, Avg loss: 0.810056 \n",
            "\n",
            "Epoch 26\n",
            "-------------------------------\n",
            "loss: 0.851412  [    0/20380]\n",
            "Train Error: Accuracy: 68.8%\n",
            "loss: 0.718593  [ 6400/20380]\n",
            "Train Error: Accuracy: 67.0%\n",
            "loss: 0.762350  [12800/20380]\n",
            "Train Error: Accuracy: 68.4%\n",
            "loss: 0.679410  [19200/20380]\n",
            "Train Error: Accuracy: 68.3%\n",
            "Val Error: \n",
            " Accuracy: 69.3%, Avg loss: 0.798133 \n",
            "\n",
            "Epoch 27\n",
            "-------------------------------\n",
            "loss: 0.842480  [    0/20380]\n",
            "Train Error: Accuracy: 69.6%\n",
            "loss: 0.703058  [ 6400/20380]\n",
            "Train Error: Accuracy: 67.6%\n",
            "loss: 0.751649  [12800/20380]\n",
            "Train Error: Accuracy: 68.8%\n",
            "loss: 0.671970  [19200/20380]\n",
            "Train Error: Accuracy: 68.7%\n",
            "Val Error: \n",
            " Accuracy: 69.6%, Avg loss: 0.787185 \n",
            "\n",
            "Epoch 28\n",
            "-------------------------------\n",
            "loss: 0.834621  [    0/20380]\n",
            "Train Error: Accuracy: 70.1%\n",
            "loss: 0.689199  [ 6400/20380]\n",
            "Train Error: Accuracy: 68.2%\n",
            "loss: 0.743012  [12800/20380]\n",
            "Train Error: Accuracy: 69.3%\n",
            "loss: 0.665380  [19200/20380]\n",
            "Train Error: Accuracy: 69.1%\n",
            "Val Error: \n",
            " Accuracy: 69.9%, Avg loss: 0.777144 \n",
            "\n",
            "Epoch 29\n",
            "-------------------------------\n",
            "loss: 0.827718  [    0/20380]\n",
            "Train Error: Accuracy: 70.4%\n",
            "loss: 0.676863  [ 6400/20380]\n",
            "Train Error: Accuracy: 68.8%\n",
            "loss: 0.735972  [12800/20380]\n",
            "Train Error: Accuracy: 69.6%\n",
            "loss: 0.659476  [19200/20380]\n",
            "Train Error: Accuracy: 69.6%\n",
            "Val Error: \n",
            " Accuracy: 70.3%, Avg loss: 0.767846 \n",
            "\n",
            "Epoch 30\n",
            "-------------------------------\n",
            "loss: 0.821571  [    0/20380]\n",
            "Train Error: Accuracy: 70.7%\n",
            "loss: 0.665821  [ 6400/20380]\n",
            "Train Error: Accuracy: 69.3%\n",
            "loss: 0.730039  [12800/20380]\n",
            "Train Error: Accuracy: 70.0%\n",
            "loss: 0.654169  [19200/20380]\n",
            "Train Error: Accuracy: 70.0%\n",
            "Val Error: \n",
            " Accuracy: 70.7%, Avg loss: 0.759065 \n",
            "\n",
            "Epoch 31\n",
            "-------------------------------\n",
            "loss: 0.816028  [    0/20380]\n",
            "Train Error: Accuracy: 71.0%\n",
            "loss: 0.655769  [ 6400/20380]\n",
            "Train Error: Accuracy: 69.8%\n",
            "loss: 0.724697  [12800/20380]\n",
            "Train Error: Accuracy: 70.5%\n",
            "loss: 0.649315  [19200/20380]\n",
            "Train Error: Accuracy: 70.5%\n",
            "Val Error: \n",
            " Accuracy: 71.2%, Avg loss: 0.750629 \n",
            "\n",
            "Epoch 32\n",
            "-------------------------------\n",
            "loss: 0.810921  [    0/20380]\n",
            "Train Error: Accuracy: 71.4%\n",
            "loss: 0.646319  [ 6400/20380]\n",
            "Train Error: Accuracy: 70.3%\n",
            "loss: 0.719520  [12800/20380]\n",
            "Train Error: Accuracy: 71.0%\n",
            "loss: 0.644847  [19200/20380]\n",
            "Train Error: Accuracy: 70.9%\n",
            "Val Error: \n",
            " Accuracy: 71.8%, Avg loss: 0.742441 \n",
            "\n",
            "Epoch 33\n",
            "-------------------------------\n",
            "loss: 0.806111  [    0/20380]\n",
            "Train Error: Accuracy: 71.8%\n",
            "loss: 0.637186  [ 6400/20380]\n",
            "Train Error: Accuracy: 70.8%\n",
            "loss: 0.714137  [12800/20380]\n",
            "Train Error: Accuracy: 71.4%\n",
            "loss: 0.640779  [19200/20380]\n",
            "Train Error: Accuracy: 71.3%\n",
            "Val Error: \n",
            " Accuracy: 72.1%, Avg loss: 0.734454 \n",
            "\n",
            "Epoch 34\n",
            "-------------------------------\n",
            "loss: 0.801491  [    0/20380]\n",
            "Train Error: Accuracy: 72.3%\n",
            "loss: 0.628220  [ 6400/20380]\n",
            "Train Error: Accuracy: 71.4%\n",
            "loss: 0.708304  [12800/20380]\n",
            "Train Error: Accuracy: 71.9%\n",
            "loss: 0.637122  [19200/20380]\n",
            "Train Error: Accuracy: 71.8%\n",
            "Val Error: \n",
            " Accuracy: 72.3%, Avg loss: 0.726621 \n",
            "\n",
            "Epoch 35\n",
            "-------------------------------\n",
            "loss: 0.797137  [    0/20380]\n",
            "Train Error: Accuracy: 72.6%\n",
            "loss: 0.619397  [ 6400/20380]\n",
            "Train Error: Accuracy: 72.0%\n",
            "loss: 0.701918  [12800/20380]\n",
            "Train Error: Accuracy: 72.3%\n",
            "loss: 0.633909  [19200/20380]\n",
            "Train Error: Accuracy: 72.3%\n",
            "Val Error: \n",
            " Accuracy: 72.5%, Avg loss: 0.718895 \n",
            "\n",
            "Epoch 36\n",
            "-------------------------------\n",
            "loss: 0.793065  [    0/20380]\n",
            "Train Error: Accuracy: 73.1%\n",
            "loss: 0.610709  [ 6400/20380]\n",
            "Train Error: Accuracy: 72.3%\n",
            "loss: 0.694936  [12800/20380]\n",
            "Train Error: Accuracy: 72.8%\n",
            "loss: 0.631119  [19200/20380]\n",
            "Train Error: Accuracy: 72.8%\n",
            "Val Error: \n",
            " Accuracy: 72.8%, Avg loss: 0.711242 \n",
            "\n",
            "Epoch 37\n",
            "-------------------------------\n",
            "loss: 0.789361  [    0/20380]\n",
            "Train Error: Accuracy: 73.5%\n",
            "loss: 0.602199  [ 6400/20380]\n",
            "Train Error: Accuracy: 72.8%\n",
            "loss: 0.687381  [12800/20380]\n",
            "Train Error: Accuracy: 73.4%\n",
            "loss: 0.628653  [19200/20380]\n",
            "Train Error: Accuracy: 73.2%\n",
            "Val Error: \n",
            " Accuracy: 73.1%, Avg loss: 0.703694 \n",
            "\n",
            "Epoch 38\n",
            "-------------------------------\n",
            "loss: 0.785964  [    0/20380]\n",
            "Train Error: Accuracy: 73.9%\n",
            "loss: 0.593930  [ 6400/20380]\n",
            "Train Error: Accuracy: 73.3%\n",
            "loss: 0.679395  [12800/20380]\n",
            "Train Error: Accuracy: 73.8%\n",
            "loss: 0.626482  [19200/20380]\n",
            "Train Error: Accuracy: 73.5%\n",
            "Val Error: \n",
            " Accuracy: 73.4%, Avg loss: 0.696259 \n",
            "\n",
            "Epoch 39\n",
            "-------------------------------\n",
            "loss: 0.782758  [    0/20380]\n",
            "Train Error: Accuracy: 74.3%\n",
            "loss: 0.585866  [ 6400/20380]\n",
            "Train Error: Accuracy: 73.7%\n",
            "loss: 0.671004  [12800/20380]\n",
            "Train Error: Accuracy: 74.3%\n",
            "loss: 0.624592  [19200/20380]\n",
            "Train Error: Accuracy: 73.9%\n",
            "Val Error: \n",
            " Accuracy: 73.6%, Avg loss: 0.688924 \n",
            "\n",
            "Epoch 40\n",
            "-------------------------------\n",
            "loss: 0.779711  [    0/20380]\n",
            "Train Error: Accuracy: 74.6%\n",
            "loss: 0.577954  [ 6400/20380]\n",
            "Train Error: Accuracy: 74.1%\n",
            "loss: 0.662280  [12800/20380]\n",
            "Train Error: Accuracy: 74.7%\n",
            "loss: 0.622957  [19200/20380]\n",
            "Train Error: Accuracy: 74.3%\n",
            "Val Error: \n",
            " Accuracy: 73.9%, Avg loss: 0.681659 \n",
            "\n",
            "Epoch 41\n",
            "-------------------------------\n",
            "loss: 0.776831  [    0/20380]\n",
            "Train Error: Accuracy: 75.0%\n",
            "loss: 0.570181  [ 6400/20380]\n",
            "Train Error: Accuracy: 74.5%\n",
            "loss: 0.653316  [12800/20380]\n",
            "Train Error: Accuracy: 75.2%\n",
            "loss: 0.621429  [19200/20380]\n",
            "Train Error: Accuracy: 74.7%\n",
            "Val Error: \n",
            " Accuracy: 74.0%, Avg loss: 0.674448 \n",
            "\n",
            "Epoch 42\n",
            "-------------------------------\n",
            "loss: 0.774100  [    0/20380]\n",
            "Train Error: Accuracy: 75.4%\n",
            "loss: 0.562443  [ 6400/20380]\n",
            "Train Error: Accuracy: 75.0%\n",
            "loss: 0.644129  [12800/20380]\n",
            "Train Error: Accuracy: 75.6%\n",
            "loss: 0.619873  [19200/20380]\n",
            "Train Error: Accuracy: 75.0%\n",
            "Val Error: \n",
            " Accuracy: 74.5%, Avg loss: 0.667274 \n",
            "\n",
            "Epoch 43\n",
            "-------------------------------\n",
            "loss: 0.771562  [    0/20380]\n",
            "Train Error: Accuracy: 75.8%\n",
            "loss: 0.554613  [ 6400/20380]\n",
            "Train Error: Accuracy: 75.6%\n",
            "loss: 0.634716  [12800/20380]\n",
            "Train Error: Accuracy: 76.0%\n",
            "loss: 0.618267  [19200/20380]\n",
            "Train Error: Accuracy: 75.4%\n",
            "Val Error: \n",
            " Accuracy: 74.7%, Avg loss: 0.660127 \n",
            "\n",
            "Epoch 44\n",
            "-------------------------------\n",
            "loss: 0.769126  [    0/20380]\n",
            "Train Error: Accuracy: 76.2%\n",
            "loss: 0.546643  [ 6400/20380]\n",
            "Train Error: Accuracy: 75.9%\n",
            "loss: 0.625149  [12800/20380]\n",
            "Train Error: Accuracy: 76.4%\n",
            "loss: 0.616654  [19200/20380]\n",
            "Train Error: Accuracy: 75.8%\n",
            "Val Error: \n",
            " Accuracy: 75.3%, Avg loss: 0.653017 \n",
            "\n",
            "Epoch 45\n",
            "-------------------------------\n",
            "loss: 0.766719  [    0/20380]\n",
            "Train Error: Accuracy: 76.7%\n",
            "loss: 0.538528  [ 6400/20380]\n",
            "Train Error: Accuracy: 76.3%\n",
            "loss: 0.615547  [12800/20380]\n",
            "Train Error: Accuracy: 76.9%\n",
            "loss: 0.614964  [19200/20380]\n",
            "Train Error: Accuracy: 76.2%\n",
            "Val Error: \n",
            " Accuracy: 75.6%, Avg loss: 0.645941 \n",
            "\n",
            "Epoch 46\n",
            "-------------------------------\n",
            "loss: 0.764209  [    0/20380]\n",
            "Train Error: Accuracy: 77.1%\n",
            "loss: 0.530233  [ 6400/20380]\n",
            "Train Error: Accuracy: 76.6%\n",
            "loss: 0.606016  [12800/20380]\n",
            "Train Error: Accuracy: 77.2%\n",
            "loss: 0.613130  [19200/20380]\n",
            "Train Error: Accuracy: 76.6%\n",
            "Val Error: \n",
            " Accuracy: 76.0%, Avg loss: 0.638907 \n",
            "\n",
            "Epoch 47\n",
            "-------------------------------\n",
            "loss: 0.761544  [    0/20380]\n",
            "Train Error: Accuracy: 77.5%\n",
            "loss: 0.521749  [ 6400/20380]\n",
            "Train Error: Accuracy: 76.9%\n",
            "loss: 0.596529  [12800/20380]\n",
            "Train Error: Accuracy: 77.6%\n",
            "loss: 0.611177  [19200/20380]\n",
            "Train Error: Accuracy: 76.9%\n",
            "Val Error: \n",
            " Accuracy: 76.5%, Avg loss: 0.631944 \n",
            "\n",
            "Epoch 48\n",
            "-------------------------------\n",
            "loss: 0.758727  [    0/20380]\n",
            "Train Error: Accuracy: 77.9%\n",
            "loss: 0.513095  [ 6400/20380]\n",
            "Train Error: Accuracy: 77.1%\n",
            "loss: 0.587083  [12800/20380]\n",
            "Train Error: Accuracy: 77.9%\n",
            "loss: 0.609044  [19200/20380]\n",
            "Train Error: Accuracy: 77.3%\n",
            "Val Error: \n",
            " Accuracy: 77.2%, Avg loss: 0.625068 \n",
            "\n",
            "Epoch 49\n",
            "-------------------------------\n",
            "loss: 0.755727  [    0/20380]\n",
            "Train Error: Accuracy: 78.3%\n",
            "loss: 0.504309  [ 6400/20380]\n",
            "Train Error: Accuracy: 77.4%\n",
            "loss: 0.577624  [12800/20380]\n",
            "Train Error: Accuracy: 78.2%\n",
            "loss: 0.606693  [19200/20380]\n",
            "Train Error: Accuracy: 77.6%\n",
            "Val Error: \n",
            " Accuracy: 77.8%, Avg loss: 0.618312 \n",
            "\n",
            "Epoch 50\n",
            "-------------------------------\n",
            "loss: 0.752498  [    0/20380]\n",
            "Train Error: Accuracy: 78.4%\n",
            "loss: 0.495422  [ 6400/20380]\n",
            "Train Error: Accuracy: 77.6%\n",
            "loss: 0.568116  [12800/20380]\n",
            "Train Error: Accuracy: 78.4%\n",
            "loss: 0.604045  [19200/20380]\n",
            "Train Error: Accuracy: 78.0%\n",
            "Val Error: \n",
            " Accuracy: 78.3%, Avg loss: 0.611696 \n",
            "\n",
            "Epoch 51\n",
            "-------------------------------\n",
            "loss: 0.749119  [    0/20380]\n",
            "Train Error: Accuracy: 78.7%\n",
            "loss: 0.486495  [ 6400/20380]\n",
            "Train Error: Accuracy: 77.7%\n",
            "loss: 0.558787  [12800/20380]\n",
            "Train Error: Accuracy: 78.6%\n",
            "loss: 0.601060  [19200/20380]\n",
            "Train Error: Accuracy: 78.4%\n",
            "Val Error: \n",
            " Accuracy: 78.6%, Avg loss: 0.605213 \n",
            "\n",
            "Epoch 52\n",
            "-------------------------------\n",
            "loss: 0.745670  [    0/20380]\n",
            "Train Error: Accuracy: 79.0%\n",
            "loss: 0.477619  [ 6400/20380]\n",
            "Train Error: Accuracy: 78.0%\n",
            "loss: 0.549807  [12800/20380]\n",
            "Train Error: Accuracy: 78.9%\n",
            "loss: 0.597679  [19200/20380]\n",
            "Train Error: Accuracy: 78.8%\n",
            "Val Error: \n",
            " Accuracy: 79.0%, Avg loss: 0.598863 \n",
            "\n",
            "Epoch 53\n",
            "-------------------------------\n",
            "loss: 0.742136  [    0/20380]\n",
            "Train Error: Accuracy: 79.2%\n",
            "loss: 0.468848  [ 6400/20380]\n",
            "Train Error: Accuracy: 78.2%\n",
            "loss: 0.541287  [12800/20380]\n",
            "Train Error: Accuracy: 79.1%\n",
            "loss: 0.593994  [19200/20380]\n",
            "Train Error: Accuracy: 79.2%\n",
            "Val Error: \n",
            " Accuracy: 79.3%, Avg loss: 0.592667 \n",
            "\n",
            "Epoch 54\n",
            "-------------------------------\n",
            "loss: 0.738522  [    0/20380]\n",
            "Train Error: Accuracy: 79.4%\n",
            "loss: 0.460215  [ 6400/20380]\n",
            "Train Error: Accuracy: 78.4%\n",
            "loss: 0.533240  [12800/20380]\n",
            "Train Error: Accuracy: 79.4%\n",
            "loss: 0.590102  [19200/20380]\n",
            "Train Error: Accuracy: 79.5%\n",
            "Val Error: \n",
            " Accuracy: 79.6%, Avg loss: 0.586643 \n",
            "\n",
            "Epoch 55\n",
            "-------------------------------\n",
            "loss: 0.734847  [    0/20380]\n",
            "Train Error: Accuracy: 79.6%\n",
            "loss: 0.451811  [ 6400/20380]\n",
            "Train Error: Accuracy: 78.6%\n",
            "loss: 0.525690  [12800/20380]\n",
            "Train Error: Accuracy: 79.5%\n",
            "loss: 0.585968  [19200/20380]\n",
            "Train Error: Accuracy: 79.8%\n",
            "Val Error: \n",
            " Accuracy: 80.0%, Avg loss: 0.580821 \n",
            "\n",
            "Epoch 56\n",
            "-------------------------------\n",
            "loss: 0.731122  [    0/20380]\n",
            "Train Error: Accuracy: 79.8%\n",
            "loss: 0.443740  [ 6400/20380]\n",
            "Train Error: Accuracy: 78.9%\n",
            "loss: 0.518652  [12800/20380]\n",
            "Train Error: Accuracy: 79.8%\n",
            "loss: 0.581635  [19200/20380]\n",
            "Train Error: Accuracy: 80.2%\n",
            "Val Error: \n",
            " Accuracy: 80.3%, Avg loss: 0.575230 \n",
            "\n",
            "Epoch 57\n",
            "-------------------------------\n",
            "loss: 0.727411  [    0/20380]\n",
            "Train Error: Accuracy: 80.0%\n",
            "loss: 0.435965  [ 6400/20380]\n",
            "Train Error: Accuracy: 79.2%\n",
            "loss: 0.512015  [12800/20380]\n",
            "Train Error: Accuracy: 80.0%\n",
            "loss: 0.577136  [19200/20380]\n",
            "Train Error: Accuracy: 80.6%\n",
            "Val Error: \n",
            " Accuracy: 80.6%, Avg loss: 0.569909 \n",
            "\n",
            "Epoch 58\n",
            "-------------------------------\n",
            "loss: 0.723805  [    0/20380]\n",
            "Train Error: Accuracy: 80.2%\n",
            "loss: 0.428313  [ 6400/20380]\n",
            "Train Error: Accuracy: 79.5%\n",
            "loss: 0.505673  [12800/20380]\n",
            "Train Error: Accuracy: 80.2%\n",
            "loss: 0.572437  [19200/20380]\n",
            "Train Error: Accuracy: 80.9%\n",
            "Val Error: \n",
            " Accuracy: 80.7%, Avg loss: 0.564903 \n",
            "\n",
            "Epoch 59\n",
            "-------------------------------\n",
            "loss: 0.720329  [    0/20380]\n",
            "Train Error: Accuracy: 80.3%\n",
            "loss: 0.420788  [ 6400/20380]\n",
            "Train Error: Accuracy: 79.6%\n",
            "loss: 0.499671  [12800/20380]\n",
            "Train Error: Accuracy: 80.4%\n",
            "loss: 0.567527  [19200/20380]\n",
            "Train Error: Accuracy: 81.2%\n",
            "Val Error: \n",
            " Accuracy: 80.8%, Avg loss: 0.560218 \n",
            "\n",
            "Epoch 60\n",
            "-------------------------------\n",
            "loss: 0.716976  [    0/20380]\n",
            "Train Error: Accuracy: 80.5%\n",
            "loss: 0.413538  [ 6400/20380]\n",
            "Train Error: Accuracy: 79.9%\n",
            "loss: 0.494022  [12800/20380]\n",
            "Train Error: Accuracy: 80.7%\n",
            "loss: 0.562487  [19200/20380]\n",
            "Train Error: Accuracy: 81.4%\n",
            "Val Error: \n",
            " Accuracy: 81.1%, Avg loss: 0.555830 \n",
            "\n",
            "Epoch 61\n",
            "-------------------------------\n",
            "loss: 0.713743  [    0/20380]\n",
            "Train Error: Accuracy: 80.6%\n",
            "loss: 0.406586  [ 6400/20380]\n",
            "Train Error: Accuracy: 80.0%\n",
            "loss: 0.488654  [12800/20380]\n",
            "Train Error: Accuracy: 80.8%\n",
            "loss: 0.557367  [19200/20380]\n",
            "Train Error: Accuracy: 81.6%\n",
            "Val Error: \n",
            " Accuracy: 81.2%, Avg loss: 0.551703 \n",
            "\n",
            "Epoch 62\n",
            "-------------------------------\n",
            "loss: 0.710592  [    0/20380]\n",
            "Train Error: Accuracy: 80.7%\n",
            "loss: 0.399918  [ 6400/20380]\n",
            "Train Error: Accuracy: 80.2%\n",
            "loss: 0.483516  [12800/20380]\n",
            "Train Error: Accuracy: 81.0%\n",
            "loss: 0.552203  [19200/20380]\n",
            "Train Error: Accuracy: 81.8%\n",
            "Val Error: \n",
            " Accuracy: 81.3%, Avg loss: 0.547806 \n",
            "\n",
            "Epoch 63\n",
            "-------------------------------\n",
            "loss: 0.707487  [    0/20380]\n",
            "Train Error: Accuracy: 80.9%\n",
            "loss: 0.393520  [ 6400/20380]\n",
            "Train Error: Accuracy: 80.4%\n",
            "loss: 0.478582  [12800/20380]\n",
            "Train Error: Accuracy: 81.1%\n",
            "loss: 0.546985  [19200/20380]\n",
            "Train Error: Accuracy: 81.9%\n",
            "Val Error: \n",
            " Accuracy: 81.6%, Avg loss: 0.544121 \n",
            "\n",
            "Epoch 64\n",
            "-------------------------------\n",
            "loss: 0.704379  [    0/20380]\n",
            "Train Error: Accuracy: 81.0%\n",
            "loss: 0.387375  [ 6400/20380]\n",
            "Train Error: Accuracy: 80.6%\n",
            "loss: 0.473842  [12800/20380]\n",
            "Train Error: Accuracy: 81.3%\n",
            "loss: 0.541737  [19200/20380]\n",
            "Train Error: Accuracy: 82.1%\n",
            "Val Error: \n",
            " Accuracy: 81.8%, Avg loss: 0.540620 \n",
            "\n",
            "Epoch 65\n",
            "-------------------------------\n",
            "loss: 0.701226  [    0/20380]\n",
            "Train Error: Accuracy: 81.2%\n",
            "loss: 0.381462  [ 6400/20380]\n",
            "Train Error: Accuracy: 80.8%\n",
            "loss: 0.469300  [12800/20380]\n",
            "Train Error: Accuracy: 81.5%\n",
            "loss: 0.536422  [19200/20380]\n",
            "Train Error: Accuracy: 82.2%\n",
            "Val Error: \n",
            " Accuracy: 81.9%, Avg loss: 0.537313 \n",
            "\n",
            "Epoch 66\n",
            "-------------------------------\n",
            "loss: 0.698032  [    0/20380]\n",
            "Train Error: Accuracy: 81.3%\n",
            "loss: 0.375764  [ 6400/20380]\n",
            "Train Error: Accuracy: 80.9%\n",
            "loss: 0.464974  [12800/20380]\n",
            "Train Error: Accuracy: 81.5%\n",
            "loss: 0.531044  [19200/20380]\n",
            "Train Error: Accuracy: 82.3%\n",
            "Val Error: \n",
            " Accuracy: 82.0%, Avg loss: 0.534200 \n",
            "\n",
            "Epoch 67\n",
            "-------------------------------\n",
            "loss: 0.694785  [    0/20380]\n",
            "Train Error: Accuracy: 81.5%\n",
            "loss: 0.370249  [ 6400/20380]\n",
            "Train Error: Accuracy: 81.1%\n",
            "loss: 0.460850  [12800/20380]\n",
            "Train Error: Accuracy: 81.7%\n",
            "loss: 0.525642  [19200/20380]\n",
            "Train Error: Accuracy: 82.5%\n",
            "Val Error: \n",
            " Accuracy: 82.0%, Avg loss: 0.531269 \n",
            "\n",
            "Epoch 68\n",
            "-------------------------------\n",
            "loss: 0.691544  [    0/20380]\n",
            "Train Error: Accuracy: 81.6%\n",
            "loss: 0.364857  [ 6400/20380]\n",
            "Train Error: Accuracy: 81.2%\n",
            "loss: 0.456912  [12800/20380]\n",
            "Train Error: Accuracy: 81.8%\n",
            "loss: 0.520231  [19200/20380]\n",
            "Train Error: Accuracy: 82.7%\n",
            "Val Error: \n",
            " Accuracy: 82.1%, Avg loss: 0.528507 \n",
            "\n",
            "Epoch 69\n",
            "-------------------------------\n",
            "loss: 0.688357  [    0/20380]\n",
            "Train Error: Accuracy: 81.7%\n",
            "loss: 0.359616  [ 6400/20380]\n",
            "Train Error: Accuracy: 81.3%\n",
            "loss: 0.453163  [12800/20380]\n",
            "Train Error: Accuracy: 81.9%\n",
            "loss: 0.514894  [19200/20380]\n",
            "Train Error: Accuracy: 82.8%\n",
            "Val Error: \n",
            " Accuracy: 82.3%, Avg loss: 0.525908 \n",
            "\n",
            "Epoch 70\n",
            "-------------------------------\n",
            "loss: 0.685243  [    0/20380]\n",
            "Train Error: Accuracy: 81.8%\n",
            "loss: 0.354555  [ 6400/20380]\n",
            "Train Error: Accuracy: 81.4%\n",
            "loss: 0.449606  [12800/20380]\n",
            "Train Error: Accuracy: 82.0%\n",
            "loss: 0.509670  [19200/20380]\n",
            "Train Error: Accuracy: 82.9%\n",
            "Val Error: \n",
            " Accuracy: 82.4%, Avg loss: 0.523453 \n",
            "\n",
            "Epoch 71\n",
            "-------------------------------\n",
            "loss: 0.682237  [    0/20380]\n",
            "Train Error: Accuracy: 81.8%\n",
            "loss: 0.349661  [ 6400/20380]\n",
            "Train Error: Accuracy: 81.5%\n",
            "loss: 0.446195  [12800/20380]\n",
            "Train Error: Accuracy: 82.1%\n",
            "loss: 0.504534  [19200/20380]\n",
            "Train Error: Accuracy: 83.0%\n",
            "Val Error: \n",
            " Accuracy: 82.6%, Avg loss: 0.521123 \n",
            "\n",
            "Epoch 72\n",
            "-------------------------------\n",
            "loss: 0.679289  [    0/20380]\n",
            "Train Error: Accuracy: 82.0%\n",
            "loss: 0.344915  [ 6400/20380]\n",
            "Train Error: Accuracy: 81.6%\n",
            "loss: 0.442907  [12800/20380]\n",
            "Train Error: Accuracy: 82.3%\n",
            "loss: 0.499504  [19200/20380]\n",
            "Train Error: Accuracy: 83.1%\n",
            "Val Error: \n",
            " Accuracy: 82.7%, Avg loss: 0.518913 \n",
            "\n",
            "Epoch 73\n",
            "-------------------------------\n",
            "loss: 0.676389  [    0/20380]\n",
            "Train Error: Accuracy: 82.1%\n",
            "loss: 0.340280  [ 6400/20380]\n",
            "Train Error: Accuracy: 81.8%\n",
            "loss: 0.439736  [12800/20380]\n",
            "Train Error: Accuracy: 82.4%\n",
            "loss: 0.494559  [19200/20380]\n",
            "Train Error: Accuracy: 83.3%\n",
            "Val Error: \n",
            " Accuracy: 82.9%, Avg loss: 0.516822 \n",
            "\n",
            "Epoch 74\n",
            "-------------------------------\n",
            "loss: 0.673538  [    0/20380]\n",
            "Train Error: Accuracy: 82.2%\n",
            "loss: 0.335777  [ 6400/20380]\n",
            "Train Error: Accuracy: 81.9%\n",
            "loss: 0.436677  [12800/20380]\n",
            "Train Error: Accuracy: 82.4%\n",
            "loss: 0.489685  [19200/20380]\n",
            "Train Error: Accuracy: 83.4%\n",
            "Val Error: \n",
            " Accuracy: 83.0%, Avg loss: 0.514849 \n",
            "\n",
            "Epoch 75\n",
            "-------------------------------\n",
            "loss: 0.670734  [    0/20380]\n",
            "Train Error: Accuracy: 82.3%\n",
            "loss: 0.331439  [ 6400/20380]\n",
            "Train Error: Accuracy: 82.0%\n",
            "loss: 0.433710  [12800/20380]\n",
            "Train Error: Accuracy: 82.5%\n",
            "loss: 0.484906  [19200/20380]\n",
            "Train Error: Accuracy: 83.5%\n",
            "Val Error: \n",
            " Accuracy: 83.0%, Avg loss: 0.513001 \n",
            "\n",
            "Epoch 76\n",
            "-------------------------------\n",
            "loss: 0.668002  [    0/20380]\n",
            "Train Error: Accuracy: 82.4%\n",
            "loss: 0.327257  [ 6400/20380]\n",
            "Train Error: Accuracy: 82.1%\n",
            "loss: 0.430823  [12800/20380]\n",
            "Train Error: Accuracy: 82.6%\n",
            "loss: 0.480210  [19200/20380]\n",
            "Train Error: Accuracy: 83.6%\n",
            "Val Error: \n",
            " Accuracy: 83.0%, Avg loss: 0.511253 \n",
            "\n",
            "Epoch 77\n",
            "-------------------------------\n",
            "loss: 0.665291  [    0/20380]\n",
            "Train Error: Accuracy: 82.5%\n",
            "loss: 0.323206  [ 6400/20380]\n",
            "Train Error: Accuracy: 82.2%\n",
            "loss: 0.427998  [12800/20380]\n",
            "Train Error: Accuracy: 82.8%\n",
            "loss: 0.475593  [19200/20380]\n",
            "Train Error: Accuracy: 83.7%\n",
            "Val Error: \n",
            " Accuracy: 83.0%, Avg loss: 0.509566 \n",
            "\n",
            "Epoch 78\n",
            "-------------------------------\n",
            "loss: 0.662547  [    0/20380]\n",
            "Train Error: Accuracy: 82.6%\n",
            "loss: 0.319282  [ 6400/20380]\n",
            "Train Error: Accuracy: 82.3%\n",
            "loss: 0.425226  [12800/20380]\n",
            "Train Error: Accuracy: 82.8%\n",
            "loss: 0.471047  [19200/20380]\n",
            "Train Error: Accuracy: 83.7%\n",
            "Val Error: \n",
            " Accuracy: 83.1%, Avg loss: 0.507946 \n",
            "\n",
            "Epoch 79\n",
            "-------------------------------\n",
            "loss: 0.659817  [    0/20380]\n",
            "Train Error: Accuracy: 82.6%\n",
            "loss: 0.315479  [ 6400/20380]\n",
            "Train Error: Accuracy: 82.3%\n",
            "loss: 0.422512  [12800/20380]\n",
            "Train Error: Accuracy: 82.9%\n",
            "loss: 0.466592  [19200/20380]\n",
            "Train Error: Accuracy: 83.8%\n",
            "Val Error: \n",
            " Accuracy: 83.2%, Avg loss: 0.506379 \n",
            "\n",
            "Epoch 80\n",
            "-------------------------------\n",
            "loss: 0.657129  [    0/20380]\n",
            "Train Error: Accuracy: 82.7%\n",
            "loss: 0.311824  [ 6400/20380]\n",
            "Train Error: Accuracy: 82.4%\n",
            "loss: 0.419846  [12800/20380]\n",
            "Train Error: Accuracy: 83.0%\n",
            "loss: 0.462255  [19200/20380]\n",
            "Train Error: Accuracy: 83.9%\n",
            "Val Error: \n",
            " Accuracy: 83.2%, Avg loss: 0.504834 \n",
            "\n",
            "Epoch 81\n",
            "-------------------------------\n",
            "loss: 0.654481  [    0/20380]\n",
            "Train Error: Accuracy: 82.7%\n",
            "loss: 0.308310  [ 6400/20380]\n",
            "Train Error: Accuracy: 82.5%\n",
            "loss: 0.417215  [12800/20380]\n",
            "Train Error: Accuracy: 83.1%\n",
            "loss: 0.458052  [19200/20380]\n",
            "Train Error: Accuracy: 83.9%\n",
            "Val Error: \n",
            " Accuracy: 83.3%, Avg loss: 0.503306 \n",
            "\n",
            "Epoch 82\n",
            "-------------------------------\n",
            "loss: 0.651853  [    0/20380]\n",
            "Train Error: Accuracy: 82.8%\n",
            "loss: 0.304952  [ 6400/20380]\n",
            "Train Error: Accuracy: 82.6%\n",
            "loss: 0.414616  [12800/20380]\n",
            "Train Error: Accuracy: 83.1%\n",
            "loss: 0.453972  [19200/20380]\n",
            "Train Error: Accuracy: 84.0%\n",
            "Val Error: \n",
            " Accuracy: 83.3%, Avg loss: 0.501805 \n",
            "\n",
            "Epoch 83\n",
            "-------------------------------\n",
            "loss: 0.649236  [    0/20380]\n",
            "Train Error: Accuracy: 82.9%\n",
            "loss: 0.301710  [ 6400/20380]\n",
            "Train Error: Accuracy: 82.7%\n",
            "loss: 0.412051  [12800/20380]\n",
            "Train Error: Accuracy: 83.3%\n",
            "loss: 0.449940  [19200/20380]\n",
            "Train Error: Accuracy: 84.1%\n",
            "Val Error: \n",
            " Accuracy: 83.4%, Avg loss: 0.500323 \n",
            "\n",
            "Epoch 84\n",
            "-------------------------------\n",
            "loss: 0.646603  [    0/20380]\n",
            "Train Error: Accuracy: 82.9%\n",
            "loss: 0.298543  [ 6400/20380]\n",
            "Train Error: Accuracy: 82.7%\n",
            "loss: 0.409517  [12800/20380]\n",
            "Train Error: Accuracy: 83.3%\n",
            "loss: 0.445940  [19200/20380]\n",
            "Train Error: Accuracy: 84.2%\n",
            "Val Error: \n",
            " Accuracy: 83.5%, Avg loss: 0.498839 \n",
            "\n",
            "Epoch 85\n",
            "-------------------------------\n",
            "loss: 0.643936  [    0/20380]\n",
            "Train Error: Accuracy: 82.9%\n",
            "loss: 0.295464  [ 6400/20380]\n",
            "Train Error: Accuracy: 82.8%\n",
            "loss: 0.407032  [12800/20380]\n",
            "Train Error: Accuracy: 83.3%\n",
            "loss: 0.442011  [19200/20380]\n",
            "Train Error: Accuracy: 84.2%\n",
            "Val Error: \n",
            " Accuracy: 83.5%, Avg loss: 0.497360 \n",
            "\n",
            "Epoch 86\n",
            "-------------------------------\n",
            "loss: 0.641325  [    0/20380]\n",
            "Train Error: Accuracy: 83.0%\n",
            "loss: 0.292483  [ 6400/20380]\n",
            "Train Error: Accuracy: 83.0%\n",
            "loss: 0.404525  [12800/20380]\n",
            "Train Error: Accuracy: 83.4%\n",
            "loss: 0.438231  [19200/20380]\n",
            "Train Error: Accuracy: 84.3%\n",
            "Val Error: \n",
            " Accuracy: 83.5%, Avg loss: 0.495932 \n",
            "\n",
            "Epoch 87\n",
            "-------------------------------\n",
            "loss: 0.638903  [    0/20380]\n",
            "Train Error: Accuracy: 83.0%\n",
            "loss: 0.289590  [ 6400/20380]\n",
            "Train Error: Accuracy: 83.0%\n",
            "loss: 0.401887  [12800/20380]\n",
            "Train Error: Accuracy: 83.5%\n",
            "loss: 0.434562  [19200/20380]\n",
            "Train Error: Accuracy: 84.4%\n",
            "Val Error: \n",
            " Accuracy: 83.6%, Avg loss: 0.494592 \n",
            "\n",
            "Epoch 88\n",
            "-------------------------------\n",
            "loss: 0.636591  [    0/20380]\n",
            "Train Error: Accuracy: 83.1%\n",
            "loss: 0.286769  [ 6400/20380]\n",
            "Train Error: Accuracy: 83.1%\n",
            "loss: 0.399141  [12800/20380]\n",
            "Train Error: Accuracy: 83.6%\n",
            "loss: 0.430912  [19200/20380]\n",
            "Train Error: Accuracy: 84.5%\n",
            "Val Error: \n",
            " Accuracy: 83.6%, Avg loss: 0.493340 \n",
            "\n",
            "Epoch 89\n",
            "-------------------------------\n",
            "loss: 0.634350  [    0/20380]\n",
            "Train Error: Accuracy: 83.1%\n",
            "loss: 0.284040  [ 6400/20380]\n",
            "Train Error: Accuracy: 83.2%\n",
            "loss: 0.396365  [12800/20380]\n",
            "Train Error: Accuracy: 83.7%\n",
            "loss: 0.427251  [19200/20380]\n",
            "Train Error: Accuracy: 84.6%\n",
            "Val Error: \n",
            " Accuracy: 83.7%, Avg loss: 0.492146 \n",
            "\n",
            "Epoch 90\n",
            "-------------------------------\n",
            "loss: 0.632124  [    0/20380]\n",
            "Train Error: Accuracy: 83.1%\n",
            "loss: 0.281405  [ 6400/20380]\n",
            "Train Error: Accuracy: 83.3%\n",
            "loss: 0.393598  [12800/20380]\n",
            "Train Error: Accuracy: 83.8%\n",
            "loss: 0.423623  [19200/20380]\n",
            "Train Error: Accuracy: 84.6%\n",
            "Val Error: \n",
            " Accuracy: 83.8%, Avg loss: 0.490998 \n",
            "\n",
            "Epoch 91\n",
            "-------------------------------\n",
            "loss: 0.629938  [    0/20380]\n",
            "Train Error: Accuracy: 83.2%\n",
            "loss: 0.278877  [ 6400/20380]\n",
            "Train Error: Accuracy: 83.3%\n",
            "loss: 0.390841  [12800/20380]\n",
            "Train Error: Accuracy: 83.8%\n",
            "loss: 0.420030  [19200/20380]\n",
            "Train Error: Accuracy: 84.7%\n",
            "Val Error: \n",
            " Accuracy: 83.9%, Avg loss: 0.489892 \n",
            "\n",
            "Epoch 92\n",
            "-------------------------------\n",
            "loss: 0.627824  [    0/20380]\n",
            "Train Error: Accuracy: 83.3%\n",
            "loss: 0.276430  [ 6400/20380]\n",
            "Train Error: Accuracy: 83.4%\n",
            "loss: 0.388099  [12800/20380]\n",
            "Train Error: Accuracy: 83.9%\n",
            "loss: 0.416533  [19200/20380]\n",
            "Train Error: Accuracy: 84.8%\n",
            "Val Error: \n",
            " Accuracy: 83.9%, Avg loss: 0.488817 \n",
            "\n",
            "Epoch 93\n",
            "-------------------------------\n",
            "loss: 0.625785  [    0/20380]\n",
            "Train Error: Accuracy: 83.4%\n",
            "loss: 0.274048  [ 6400/20380]\n",
            "Train Error: Accuracy: 83.5%\n",
            "loss: 0.385378  [12800/20380]\n",
            "Train Error: Accuracy: 83.9%\n",
            "loss: 0.413127  [19200/20380]\n",
            "Train Error: Accuracy: 84.8%\n",
            "Val Error: \n",
            " Accuracy: 83.9%, Avg loss: 0.487774 \n",
            "\n",
            "Epoch 94\n",
            "-------------------------------\n",
            "loss: 0.623813  [    0/20380]\n",
            "Train Error: Accuracy: 83.5%\n",
            "loss: 0.271753  [ 6400/20380]\n",
            "Train Error: Accuracy: 83.6%\n",
            "loss: 0.382679  [12800/20380]\n",
            "Train Error: Accuracy: 84.0%\n",
            "loss: 0.409805  [19200/20380]\n",
            "Train Error: Accuracy: 84.9%\n",
            "Val Error: \n",
            " Accuracy: 84.0%, Avg loss: 0.486759 \n",
            "\n",
            "Epoch 95\n",
            "-------------------------------\n",
            "loss: 0.621881  [    0/20380]\n",
            "Train Error: Accuracy: 83.5%\n",
            "loss: 0.269565  [ 6400/20380]\n",
            "Train Error: Accuracy: 83.6%\n",
            "loss: 0.380038  [12800/20380]\n",
            "Train Error: Accuracy: 84.1%\n",
            "loss: 0.406539  [19200/20380]\n",
            "Train Error: Accuracy: 84.9%\n",
            "Val Error: \n",
            " Accuracy: 84.1%, Avg loss: 0.485777 \n",
            "\n",
            "Epoch 96\n",
            "-------------------------------\n",
            "loss: 0.620024  [    0/20380]\n",
            "Train Error: Accuracy: 83.6%\n",
            "loss: 0.267495  [ 6400/20380]\n",
            "Train Error: Accuracy: 83.6%\n",
            "loss: 0.377465  [12800/20380]\n",
            "Train Error: Accuracy: 84.1%\n",
            "loss: 0.403330  [19200/20380]\n",
            "Train Error: Accuracy: 84.9%\n",
            "Val Error: \n",
            " Accuracy: 84.1%, Avg loss: 0.484832 \n",
            "\n",
            "Epoch 97\n",
            "-------------------------------\n",
            "loss: 0.618252  [    0/20380]\n",
            "Train Error: Accuracy: 83.7%\n",
            "loss: 0.265553  [ 6400/20380]\n",
            "Train Error: Accuracy: 83.7%\n",
            "loss: 0.374944  [12800/20380]\n",
            "Train Error: Accuracy: 84.2%\n",
            "loss: 0.400162  [19200/20380]\n",
            "Train Error: Accuracy: 84.9%\n",
            "Val Error: \n",
            " Accuracy: 84.2%, Avg loss: 0.483924 \n",
            "\n",
            "Epoch 98\n",
            "-------------------------------\n",
            "loss: 0.616547  [    0/20380]\n",
            "Train Error: Accuracy: 83.7%\n",
            "loss: 0.263720  [ 6400/20380]\n",
            "Train Error: Accuracy: 83.8%\n",
            "loss: 0.372480  [12800/20380]\n",
            "Train Error: Accuracy: 84.3%\n",
            "loss: 0.397053  [19200/20380]\n",
            "Train Error: Accuracy: 84.9%\n",
            "Val Error: \n",
            " Accuracy: 84.3%, Avg loss: 0.483047 \n",
            "\n",
            "Epoch 99\n",
            "-------------------------------\n",
            "loss: 0.614917  [    0/20380]\n",
            "Train Error: Accuracy: 83.8%\n",
            "loss: 0.261970  [ 6400/20380]\n",
            "Train Error: Accuracy: 83.8%\n",
            "loss: 0.370075  [12800/20380]\n",
            "Train Error: Accuracy: 84.3%\n",
            "loss: 0.394013  [19200/20380]\n",
            "Train Error: Accuracy: 85.0%\n",
            "Val Error: \n",
            " Accuracy: 84.3%, Avg loss: 0.482213 \n",
            "\n",
            "Epoch 100\n",
            "-------------------------------\n",
            "loss: 0.613342  [    0/20380]\n",
            "Train Error: Accuracy: 83.8%\n",
            "loss: 0.260290  [ 6400/20380]\n",
            "Train Error: Accuracy: 83.8%\n",
            "loss: 0.367718  [12800/20380]\n",
            "Train Error: Accuracy: 84.4%\n",
            "loss: 0.391031  [19200/20380]\n",
            "Train Error: Accuracy: 85.0%\n",
            "Val Error: \n",
            " Accuracy: 84.3%, Avg loss: 0.481412 \n",
            "\n",
            "Max number of epochs hit or validation stop reached. Done!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Test Set Presentation\n",
        "6. Present the results using the test set."
      ],
      "metadata": {
        "id": "MnXew6heNzuz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test(test_dataloader, model, loss_fn)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TC0Zml51GriX",
        "outputId": "3341a62f-6550-499c-d155-4d6657cf3652"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Error: \n",
            " Accuracy: 81.7%, Avg loss: 0.487211 \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Using a Baseline Model - Descision Tree (Task 4)\n",
        "\n",
        "We'll use a descision tree with a limited capacity to split branches and observe the results."
      ],
      "metadata": {
        "id": "hmkVOEmaJypY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import Normalizer\n",
        "from sklearn import tree\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import precision_score\n",
        "from sklearn.metrics import recall_score\n",
        "import time\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "import math\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.model_selection import RepeatedStratifiedKFold\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from numpy import mean\n",
        "from numpy import std"
      ],
      "metadata": {
        "id": "k2jOwuPXzx_c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#try limited max_depth\n",
        "clf = tree.DecisionTreeClassifier(max_depth=5, random_state=86)\n",
        "start = time.time()\n",
        "clf = clf.fit(train_x.values, train_y.values)\n",
        "end = time.time()\n",
        "print(\"Time elapsed for Descision Tree fit: \" + str(end - start))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wGHk-8Lgy9B3",
        "outputId": "7a95e574-1ac7-46a3-cf7c-284032f887f7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Time elapsed for Descision Tree fit: 0.0995035171508789\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tree.plot_tree(clf)\n",
        "#plt.show to prevent excess chunks of text code\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 248
        },
        "id": "CkZMwhvm5twv",
        "outputId": "7bebb2e5-d312-473a-f9ff-ba90aec5472f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(clf.tree_.max_depth)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W4gKnHxyeq9I",
        "outputId": "da304c0e-9cbb-4840-8a21-12f345226b32"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_DesTree_train =clf.predict(train_x.values)\n",
        "y_DesTree_train = y_DesTree_train.reshape((y_DesTree_train.shape[0], 1))"
      ],
      "metadata": {
        "id": "x4Wd9IxR5wun"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "acc_s = str(accuracy_score(train_y.values, y_DesTree_train))\n",
        "\n",
        "print(\"Train Accuracy: \" + acc_s )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VKaKbmNHo0B8",
        "outputId": "b2cf897b-af86-467c-a502-7900571220ef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.6501472031403337\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#We can confirm that the initial descision tree overfit the train/test set this way\n",
        "y_DesTree_test =clf.predict(test_x.values)\n",
        "y_DesTree_test = y_DesTree_test.reshape((y_DesTree_test.shape[0], 1))"
      ],
      "metadata": {
        "id": "fUIOjBt26BC6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "acc_s = str(accuracy_score(test_y.values, y_DesTree_test))\n",
        "print(\"Test Accuracy: \" + acc_s )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F_-rJkC4ovzS",
        "outputId": "a8b981c1-7071-4b7b-8c81-fa05f3597ae8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy: 0.6435861202700581\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Closing Thoughts (Task 3 and Task 4)\n",
        "\n",
        "\n",
        "Our Neural Network peformed better than our decision tree in solving the proposed problem. This is likely because our dataset has complex enough features that a descision tree that has been restricted to be general enough to not overfit on the dataset cannot learn it as well as a neural network that is able to more carefully lean the nonlinearity features of the problem without overfitting.\n",
        "\n",
        "Based on our accuracy statistics, using 256 nodes, ELU, and 2 layers was the best configuration."
      ],
      "metadata": {
        "id": "YkVUygBj5cy0"
      }
    }
  ]
}