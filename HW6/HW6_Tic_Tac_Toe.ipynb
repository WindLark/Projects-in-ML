{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "zGBmjkuc-_KP",
        "U6am91IOThiz"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "[Collab Notebook Link](https://colab.research.google.com/drive/1kntdWPZ0uxYSIqCV4yCB-Wb_Qpyy7rYi?usp=sharing)"
      ],
      "metadata": {
        "id": "UA7Noe3Afepd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##\"Classical\" Tabular Q-Learning Implementation of Tic Tac Toe\n",
        "\n",
        "**Introduction**\n",
        "\n",
        "In this homework, I implemeented a table based Q-learning Agent that would learn to play tic tac toe. Given a table with all possible game states and actions that could be peformed in a game, the algorithm plays games and attempts to learn an optimal strategy based on Îµ-greedy action selection which starts if off by selecting random actions but gradually causes the algorithm to focus on a fixed strategy.\n",
        "\n",
        "While the tic tac toe board is typically thought of as a 3x3 board, it can be flattened to a 1 dimensional array of length 9 and still be \"played\" properly. I take advantage of this to work primarily with 1D arrays for the purposes of this assignment as it simplifies tasks like making a straightfoward lookup table for game states and the indexes they correspond to on a 2d Qtable represented by a 2d numpy array.\n",
        "\n",
        "To evaluate the success of our Q Learning Agent, I tested it in the following scenarios - playing games vs a random agent (both in the player 1 and player 2 seats), and having 2 Q Agents play versus each other. The agents are evaluated based on their performance as Player 1, Player 2, and the number of draws they incurred in a match, as well as a more qualitative metric of whether they perform more \"human\" tic tac toe strategies. Some of the games from each episodes are presented as part of their observations, and can be found under each training situation's output.\n",
        "\n",
        "Player 1 is represented by X (1), and Player 2 is represented by O (2). Empty spots are marked by E (zero). \n",
        " \n",
        "**Resources**\n",
        "\n",
        "**Base Q Learning Resources**\n",
        "\n",
        "https://towardsdatascience.com/practical-reinforcement-learning-02-getting-started-with-q-learning-582f63e4acd9\n",
        "\n",
        "https://github.com/Uzmamushtaque/CSCI4962-Projects-ML-AI/blob/main/Lecture_18.ipynb\n",
        "\n",
        "These were the base Q Learning resources provided at the start of the assignemnt. These were used to gain a basic understanding of how to implement Q-learning for a problem, specifically using the provided article's pseudo-code as a starting point.\n",
        "\n",
        "**Studying Q Learning Via the Frozen Lake Problem**\n",
        "\n",
        "https://www.freecodecamp.org/news/diving-deeper-into-reinforcement-learning-with-q-learning-c18d0db58efe/\n",
        "\n",
        "https://github.com/simoninithomas/Deep_reinforcement_learning_Course/blob/master/Q%20learning/FrozenLake/Q%20Learning%20with%20FrozenLake.ipynb\n",
        "\n",
        "I then tried a basic implementation using the Frozen Lake Problem availalbe in openAI gym to get a better understanding of what a Q Learning Agent would look like when learning to play a game.\n",
        "\n",
        "**Understanding the Specifics of the Tic Tac Toe Problem**\n",
        "\n",
        "https://medium.com/@carsten.friedrich/part-1-computer-tic-tac-toe-basics-35964f92fa03\n",
        "\n",
        "https://medium.com/@carsten.friedrich/part-3-tabular-q-learning-a-tic-tac-toe-player-that-gets-better-and-better-fa4da4b0892a\n",
        "\n",
        "https://nestedsoftware.com/2019/07/25/tic-tac-toe-with-tabular-q-learning-1kdn.139811.html\n",
        "\n",
        "After initial research and experimenting with openAI gym, I implemented the Tic Tac Toe game with the interest of making it possible to create multiple different agents that could play the game against each other as part of the training process. This ended up being a slightly ambitious demand for the time available, so I scaled the problem down to implementing Tic Tac Toe with a Random Agent first to ensure I was properly setting up a simulated game that could support modular agents  prior to implementing a Q Agent.\n",
        "\n",
        "Additional attribution is provided where appropriate."
      ],
      "metadata": {
        "id": "4Ysvjh3KujS1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Base Game\n",
        "Here we develop the base tictactoe game and an agent playing random moves to accompany it."
      ],
      "metadata": {
        "id": "zGBmjkuc-_KP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from enum import Enum\n",
        "from abc import ABC, abstractmethod\n",
        "import random"
      ],
      "metadata": {
        "id": "wliEHOnk-9i_"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MARKERS(Enum):\n",
        "    E = 0 \n",
        "    X = 1\n",
        "    O = 2 \n",
        "\n",
        "class GAMESTATE(Enum):\n",
        "    ONGOING = 0 \n",
        "    PLAYER_1 = 1\n",
        "    PLAYER_2 = 2\n",
        "    DRAW = 3 \n",
        "    ERROR = 4 "
      ],
      "metadata": {
        "id": "MazCJhMx_ep-"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(MARKERS.E.value)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cZPaly3Y_7P-",
        "outputId": "c664a720-d200-4785-b09b-52b1ef57932c"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "id": "ipFi1puN-wbQ"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "A tic tac toe board looks like a 3x3 grid:\n",
        "0 1 2 \n",
        "3 4 5\n",
        "6 7 8\n",
        "\n",
        "But we can reinterpret it as a 1 dimensional array of size 9.\n",
        "0 1 2 3 4 5 6 7 8\n",
        "\n",
        "If we use 1 for X and 2 for O, we can reinterpret winning positions accordingly:\n",
        "column: That column, offset by row#*3\n",
        "altenratively, 0=3=6 OR 1=4=7 OR 2=5=8\n",
        "1 0 0 \n",
        "1 0 0\n",
        "1 0 0\n",
        "\n",
        "1 0 0 1 0 0 1 0 0\n",
        "\n",
        "row: Indexes 0=1=2 OR 3=4=5 OR 6=7=8\n",
        "1 1 1\n",
        "0 0 0\n",
        "0 0 0\n",
        "\n",
        "1 1 1 0 0 0 0 0 0\n",
        "\n",
        "diagonal: 0=4=8 or 2=4=6\n",
        "1 0 0\n",
        "0 1 0\n",
        "0 0 1\n",
        "1 0 0 0 1 0 0 0 1\n",
        "0 0 1 0 1 0 1 0 0\n",
        "\n",
        "We could do more graceful calculations for these\n",
        "but for simplicity's  sake we will do simple index checks\n",
        "\"\"\"\n",
        "\n",
        "class TicTacToe:\n",
        "    def __init__(self, base_state=np.zeros(shape=(9), dtype=int)):\n",
        "            self.state = base_state.copy()\n",
        "\n",
        "    def reset_board(self):\n",
        "      self.state = np.zeros(shape=(9), dtype=int)\n",
        "    def printstate(self):\n",
        "      print(self.state[0:3]) #[start index:end index+1]\n",
        "      print(self.state[3:6])\n",
        "      print(self.state[6:9])\n",
        "\n",
        "    #print as xs and os instead\n",
        "    def printstate_xo(self):\n",
        "      for i in self.state[0:3]:\n",
        "        print(MARKERS(i).name, end=' ')\n",
        "      print(' ')\n",
        "      for i in self.state[3:6]:\n",
        "        print(MARKERS(i).name, end=' ')\n",
        "      print(' ')\n",
        "      for i in self.state[6:9]:\n",
        "        print(MARKERS(i).name, end=' ')\n",
        "      print(' ')\n",
        "\n",
        "    def getval(self, index):\n",
        "      return self.state[index]\n",
        "\n",
        "    #checks for a winner. If there is a winner,\n",
        "    #return the winner's number.\n",
        "    #otherwise, return 0 to indicate no winner found.\n",
        "    def check_row(self):\n",
        "      if(self.state[0] == self.state[1] == self.state[2]):\n",
        "        return self.state[0]\n",
        "      elif(self.state[3] == self.state[4] == self.state[5]):\n",
        "        return self.state[3]\n",
        "      elif(self.state[6] == self.state[7] == self.state[8]):\n",
        "        return self.state[6]\n",
        "      return 0\n",
        "\n",
        "    def check_col(self):\n",
        "      if(self.state[0] == self.state[3] == self.state[6]):\n",
        "        return self.state[0]\n",
        "      elif(self.state[1] == self.state[4] == self.state[7]):\n",
        "        return self.state[1]\n",
        "      elif(self.state[2] == self.state[5] == self.state[8]):\n",
        "        return self.state[2]\n",
        "      return 0\n",
        "\n",
        "    def check_diag(self):\n",
        "      if(self.state[0] == self.state[4] == self.state[8]):\n",
        "        return self.state[0]\n",
        "      elif(self.state[2] == self.state[4] == self.state[6]):\n",
        "        return self.state[2]\n",
        "      return 0\n",
        "\n",
        "    #the game should end if we pass a nonzero result.\n",
        "    def victory_check(self):\n",
        "      row=self.check_row()\n",
        "      col=self.check_col()\n",
        "      diag=self.check_diag()\n",
        "      if row !=0:\n",
        "        return row\n",
        "      elif col !=0:\n",
        "        return col\n",
        "      elif diag !=0:\n",
        "        return diag\n",
        "      elif(np.all(self.state)): #np.all will return false if a 0 is found in the array\n",
        "        return 3 #this is a special condition to indicate a draw\n",
        "      return 0 #0 indicates the game is still ongoing\n",
        "      \n",
        "    #converts (column, row) coordinates passed as a tuple to a 1D coord\n",
        "    #assumes 0,0 is the top left corner\n",
        "    #formula is row*3 + col\n",
        "    #e.g. \n",
        "    #0,0 = 0\n",
        "    #1,2 = 7\n",
        "    def twod_to_oned(self, coord):\n",
        "      return (coord[1] * 3) + coord[0]\n",
        "\n",
        "    #return 1 if placement was successful \n",
        "    #else return -1 if already occupied slot\n",
        "    def place1d(self, value, index):\n",
        "      if(self.state[index]==0):\n",
        "        self.state[index] = value\n",
        "        return 1\n",
        "      return -1\n",
        "    def place2d(self, value, coord):\n",
        "      return self.place1d(value, self.twod_to_oned(coord))\n",
        "\n",
        "    #get all empty indices\n",
        "    def getempty(self):\n",
        "      temp_empty = []\n",
        "      for i in range(0,9):\n",
        "        if self.state[i] == 0:\n",
        "          temp_empty.append(i)\n",
        "      return temp_empty\n",
        "\n",
        "\n",
        "\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "a = TicTacToe()\n",
        "#print(a.state.size)\n",
        "a.printstate()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pIl5XUnlDI_t",
        "outputId": "5394fd4c-cbe1-4ada-e105-8e28394e5bc7"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0 0 0]\n",
            "[0 0 0]\n",
            "[0 0 0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "a.place1d(1,4)\n",
        "a.place2d(2,(1,2))\n",
        "a.place1d(42,4) #should fail\n",
        "a.printstate()\n",
        "a.printstate_xo()\n",
        "print(\"reset test\")\n",
        "a.reset_board()\n",
        "a.printstate()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MJqxN6WsQTZ7",
        "outputId": "9262bec6-b1e2-457f-887d-e8aa47fe8248"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0 0 0]\n",
            "[0 1 0]\n",
            "[0 2 0]\n",
            "E E E  \n",
            "E X E  \n",
            "E O E  \n",
            "reset test\n",
            "[0 0 0]\n",
            "[0 0 0]\n",
            "[0 0 0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#test victory\n",
        "a.place2d(2,(0,0))\n",
        "a.place2d(2,(1,1))\n",
        "a.place2d(2,(2,2))\n",
        "a.printstate()\n",
        "print(a.victory_check())\n",
        "a.reset_board()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GtQCAKbiTGLL",
        "outputId": "82014c98-2b40-4b82-a967-18a67146a718"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2 0 0]\n",
            "[0 2 0]\n",
            "[0 0 2]\n",
            "2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Random Moves Agent\n",
        "As a preparatory step to making a Q-Learning Agent we first make an agent\n",
        "that only knows how to pick random moves."
      ],
      "metadata": {
        "id": "U6am91IOThiz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#abstract class that will define all future agents\n",
        "class Agent(ABC):\n",
        "    @abstractmethod\n",
        "    def move(self):\n",
        "      pass\n",
        "\n",
        "class RandomAgent(Agent):\n",
        "  def __init__(self, side):\n",
        "    self.side = side\n",
        "  def move(self, tictactoe):\n",
        "    empty = tictactoe.getempty()\n",
        "    tictactoe.place1d(self.side, random.choice(empty))"
      ],
      "metadata": {
        "id": "EbNcpXYLTr0j"
      },
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "randage1 = RandomAgent(1)\n",
        "randage2 = RandomAgent(2)"
      ],
      "metadata": {
        "id": "AWvD4KBPWh8t"
      },
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "a.reset_board()\n",
        "randage1.move(a)\n",
        "randage2.move(a)\n",
        "randage1.move(a)\n",
        "randage2.move(a)\n",
        "a.printstate()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z3vh6V_qWqVq",
        "outputId": "7e324c28-e1a0-40dc-de47-980abd0fb8a9"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2 1 2]\n",
            "[0 0 1]\n",
            "[0 0 0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#let's try and make a sample game\n",
        "gameend = False\n",
        "result = 4 \n",
        "a.reset_board()\n",
        "while (gameend == False):\n",
        "  randage1.move(a)\n",
        "  vict = a.victory_check() \n",
        "  if vict !=0:\n",
        "    result = vict\n",
        "    gameend = True\n",
        "    break\n",
        "  randage2.move(a)\n",
        "  vict = a.victory_check() \n",
        "  if vict !=0:\n",
        "    result = vict\n",
        "    gameend = True\n",
        "    break\n",
        "\n",
        "a.printstate()\n",
        "a.printstate_xo()\n",
        "print(\"Game is over. Resulting state: \", GAMESTATE(result).name)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d3Mbgzb4XGCE",
        "outputId": "795d818f-a10c-49b1-8c66-9c3f1f2ddad4"
      },
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1 0 2]\n",
            "[2 1 2]\n",
            "[1 0 1]\n",
            "X E O  \n",
            "O X O  \n",
            "X E X  \n",
            "Game is over. Resulting state:  PLAYER_1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Random Agent Games\n",
        "\n",
        "In this context, we have 2 random agents play vs each other."
      ],
      "metadata": {
        "id": "DTGomok6TrIO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#let's try 200 games \n",
        "#player1wins,player2wins,drawwins, errors\n",
        "results_array = [0,0,0,0]\n",
        "\n",
        "for i in range (0,201):\n",
        "\n",
        "  gameend = False\n",
        "  result = 4 \n",
        "  a.reset_board()\n",
        "  if(i%100 == 0):\n",
        "    print(\"Game of Episode \", i)\n",
        "  while (gameend == False):\n",
        "    randage1.move(a)\n",
        "    vict = a.victory_check() \n",
        "    if(i%100 == 0):\n",
        "      print(\"P1\")\n",
        "      a.printstate_xo()\n",
        "    if vict !=0:\n",
        "      result = vict\n",
        "      gameend = True\n",
        "      break\n",
        "    randage2.move(a)\n",
        "    if(i%100 == 0):\n",
        "      print(\"P2\")\n",
        "      a.printstate_xo()\n",
        "    vict = a.victory_check() \n",
        "    if vict !=0:\n",
        "      result = vict\n",
        "      gameend = True\n",
        "      break\n",
        "  results_array[result-1]+=1\n",
        "\n",
        "print(\"Game Statistics: P1 Win, P2 Win, Draw, Error\")  \n",
        "print(results_array)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lte9U-1cZzJZ",
        "outputId": "e738b05b-a110-4b32-a35f-df58ddea5666"
      },
      "execution_count": 148,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Game of Episode  0\n",
            "P1\n",
            "X E E  \n",
            "E E E  \n",
            "E E E  \n",
            "P2\n",
            "X E E  \n",
            "E E E  \n",
            "E E O  \n",
            "P1\n",
            "X E E  \n",
            "X E E  \n",
            "E E O  \n",
            "P2\n",
            "X E E  \n",
            "X E O  \n",
            "E E O  \n",
            "P1\n",
            "X E X  \n",
            "X E O  \n",
            "E E O  \n",
            "P2\n",
            "X O X  \n",
            "X E O  \n",
            "E E O  \n",
            "P1\n",
            "X O X  \n",
            "X E O  \n",
            "E X O  \n",
            "P2\n",
            "X O X  \n",
            "X E O  \n",
            "O X O  \n",
            "P1\n",
            "X O X  \n",
            "X X O  \n",
            "O X O  \n",
            "Game of Episode  100\n",
            "P1\n",
            "X E E  \n",
            "E E E  \n",
            "E E E  \n",
            "P2\n",
            "X E E  \n",
            "E E E  \n",
            "E O E  \n",
            "P1\n",
            "X E E  \n",
            "E E E  \n",
            "X O E  \n",
            "P2\n",
            "X E E  \n",
            "E E O  \n",
            "X O E  \n",
            "P1\n",
            "X E E  \n",
            "E X O  \n",
            "X O E  \n",
            "P2\n",
            "X E E  \n",
            "E X O  \n",
            "X O O  \n",
            "P1\n",
            "X E E  \n",
            "X X O  \n",
            "X O O  \n",
            "Game of Episode  200\n",
            "P1\n",
            "E E X  \n",
            "E E E  \n",
            "E E E  \n",
            "P2\n",
            "O E X  \n",
            "E E E  \n",
            "E E E  \n",
            "P1\n",
            "O X X  \n",
            "E E E  \n",
            "E E E  \n",
            "P2\n",
            "O X X  \n",
            "E E E  \n",
            "O E E  \n",
            "P1\n",
            "O X X  \n",
            "E X E  \n",
            "O E E  \n",
            "P2\n",
            "O X X  \n",
            "E X O  \n",
            "O E E  \n",
            "P1\n",
            "O X X  \n",
            "E X O  \n",
            "O X E  \n",
            "Game Statistics: P1 Win, P2 Win, Draw, Error\n",
            "[125, 50, 26, 0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Q Learning Agent\n",
        "\n",
        "We will now build a Q Learning agent whose goal is to construct a Q table that it can use to play the game. \n",
        "\n",
        "\n",
        "https://stackoverflow.com/questions/7466429/generate-a-list-of-all-unique-tic-tac-toe-boards\n",
        "\n",
        "It is worth noting that there are 19683 possible states in a tic tac toe game."
      ],
      "metadata": {
        "id": "k3dk-nr8ZGzT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "https://towardsdatascience.com/practical-reinforcement-learning-02-getting-started-with-q-learning-582f63e4acd9\n",
        "\n",
        "PseudoCode of Q Learning Algorithm\n",
        "\n",
        "Define Gamma, Epilson, Alpha\n",
        "Initiate q-table with zeroes\n",
        "observe initial state s\n",
        "repeat:\n",
        "  select action a\n",
        "    with probability epsilon select random action a\n",
        "    else selection action a = argmax(Q(s,a'))\n",
        "  carry out action a\n",
        "  observe next state s' and reward r\n",
        "  calculate Q_target = r+gamma*max[Q(s',A)]\n",
        "  calculate Q_delta = Q_target - Q(s,a)\n",
        "  add alpha*Q_delta to Q(s,a)\n",
        "  s=s'\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "qLALw38Ilf2G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Game state generator\n",
        "#Attribution: https://stackoverflow.com/questions/7466429/generate-a-list-of-all-unique-tic-tac-toe-boards\n",
        "#Quick generation of all possible tic tac toe gamestates \n",
        "#(regardless of legality)\n",
        "#for the purposes of having the \"state\" indexes \n",
        "#of the q table correspond to these game states.\n",
        "def generateStates():\n",
        "  StatesMatrix = np.zeros((3**9,9))\n",
        "  for i in range(3**9):\n",
        "      c = i\n",
        "      for j in range(9):\n",
        "        StatesMatrix[i][j] = c % 3\n",
        "        c //= 3\n",
        "  return StatesMatrix"
      ],
      "metadata": {
        "id": "AzYeufAHn4DX"
      },
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def searchStates(statesmat, target):\n",
        "  for i in range(0, statesmat.shape[0]):\n",
        "    if np.array_equal(statesmat[i], target):\n",
        "      return i"
      ],
      "metadata": {
        "id": "66ZGoSTMfr8t"
      },
      "execution_count": 167,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "statesm = generateStates()\n",
        "print(statesm)\n",
        "print(statesm.shape)\n",
        "print(statesm.shape[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g-15NOs7oFZG",
        "outputId": "8d1e35d4-0a7b-4ed2-9f43-17ffb63e5d7b"
      },
      "execution_count": 166,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0. 0. 0. ... 0. 0. 0.]\n",
            " [1. 0. 0. ... 0. 0. 0.]\n",
            " [2. 0. 0. ... 0. 0. 0.]\n",
            " ...\n",
            " [0. 2. 2. ... 2. 2. 2.]\n",
            " [1. 2. 2. ... 2. 2. 2.]\n",
            " [2. 2. 2. ... 2. 2. 2.]]\n",
            "(19683, 9)\n",
            "19683\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(searchStates(statesm,np.array([0., 0., 0., 0., 0., 0., 0., 0., 0.])))\n",
        "print(searchStates(statesm,np.array([0, 1, 2, 0, 1, 1, 0, 0, 0])))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "915HcucnoOgH",
        "outputId": "89883c2b-4f65-4cb4-c422-5194b3f994e7"
      },
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n",
            "345\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\"as a reminder, our board setup looks like:\n",
        "A tic tac toe board looks like a 3x3 grid:\n",
        "0 1 2 \n",
        "3 4 5\n",
        "6 7 8\n",
        "But we can reinterpret it as a 1 dimensional array of size 9.\n",
        "0 1 2 3 4 5 6 7 8\n",
        "\"\"\"\n",
        "#Initiate q-table with zeroes\n",
        "qtable = np.zeros((19683, 9))\n",
        "print(qtable)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EGlHvs5_onVO",
        "outputId": "d8548fd1-39e5-4fd8-9b8b-75724448601e"
      },
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " ...\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class QAgent(Agent):\n",
        "  def __init__(self, side):\n",
        "    self.side = side\n",
        "    self.qtable = np.zeros((19683, 9)) #q-table - we'll update this whenever a win or loss is reached.\n",
        "    #+5 to reward wins, -5 to indicate losses\n",
        "    self.qlookup = generateStates()\n",
        "    #Define Gamma, Epilson, Alpha   \n",
        "    self.gamma = 0.95 \n",
        "    self.start_epsilon = 1.0 \n",
        "    self.epsilon = 1.0 \n",
        "    self.min_epsilon = 0.01 \n",
        "    self.decay_rate = 0.05 \n",
        "    self.alpha = 0.8                \n",
        "\n",
        "  def reset_epsilon(self):\n",
        "     self.epsilon = self.start_epsilon\n",
        "\n",
        "  def decay_epsilon(self):\n",
        "    if(self.epsilon >= self.min_epsilon and not (self.epsilon - self.decay_rate <self.min_epsilon)):\n",
        "      self.epsilon = self.epsilon - self.decay_rate\n",
        "  #reset ALL modifiable properties.\n",
        "  def reset_mod_properties(self):\n",
        "    self.qtable = np.zeros((19683, 9)) #q-table - we'll update this whenever a win or loss is reached.=\n",
        "    self.epsilon = self.start_epsilon  # Current exploration rate\n",
        "    \n",
        "  def move(self, tictactoe):\n",
        "    #our move action will have to be a bit more complicated than just picking random valid moves.\n",
        "    #mode 1 - randomly select a valid move    \n",
        "    \"\"\"\n",
        "    select action a\n",
        "      with probability epsilon select random action a\n",
        "    \"\"\"\n",
        "    if(random.uniform(0, 1)<=self.epsilon): #random check falls within range of 0,epsilon, select randomly\n",
        "    #if(1==2): #DEBUG\n",
        "      empty = tictactoe.getempty()\n",
        "      tictactoe.place1d(self.side, random.choice(empty))\n",
        "    else: #else peform a q-learn action\n",
        "    #mode 2 - get all valid moves, select a move then find the corresponding Q-table value,\n",
        "    #and update it.\n",
        "      \"\"\"\n",
        "        else selection action a = argmax(Q(s,a'))\n",
        "      \"\"\"\n",
        "      empty = tictactoe.getempty() #get all valid actions\n",
        "      argument_values = []\n",
        "      for desired_space in empty:\n",
        "        hypo_state = tictactoe.state.copy()\n",
        "        hypo_state[desired_space] = self.side #check what the next action would be for each state\n",
        "        targetindex = searchStates(self.qlookup, hypo_state)\n",
        "        argument_values.append(self.qtable[targetindex][desired_space]) #get the qtable's current value and add it to argument values\n",
        "      best_action_index = np.argmax(argument_values)\n",
        "      \"\"\"\n",
        "        carry out action a\n",
        "        We don't actually execute the action yet on the main tictactoe board\n",
        "        because we still want to observe s and s'\n",
        "      \"\"\"\n",
        "      best_action = empty[best_action_index]\n",
        "\n",
        "      #instead we create a new hypothetical, and observe how this one would go.\n",
        "      action_hypo = tictactoe.state.copy()\n",
        "      action_hypo[best_action] = self.side\n",
        "\n",
        "      \"\"\"\n",
        "      class GAMESTATE(Enum):\n",
        "          ONGOING = 0 \n",
        "          PLAYER_1 = 1\n",
        "          PLAYER_2 = 2\n",
        "          DRAW = 3 \n",
        "          ERROR = 4 (shouldn't be possible.)\n",
        "      \"\"\"\n",
        "\n",
        "      \"\"\"\n",
        "      observe next state s' and reward r\n",
        "\n",
        "      here we check if s' as a result of our action\n",
        "      is a victory or defeat.\n",
        "\n",
        "      r value table: \n",
        "      If victory, +5, \n",
        "      if ongoing, +0,\n",
        "      if draw, -1 (we want to avoid draws but they're not the end of the world.)\n",
        "      if defeat or any other value (error), -5 (we want to avoid this as much as possible)\n",
        "      \"\"\"\n",
        "      vict = tictactoe.victory_check() \n",
        "      reward = 0\n",
        "      possiblerewards = []\n",
        "      #we need to do this check manually because we need to account for what side the player's on\n",
        "      if vict == 0: #indicates the game is still ongoing, we need to do one more check for successive states\n",
        "        reward = 0\n",
        "        tictactoe2 = TicTacToe(action_hypo) #create a hypothetical tictactoe\n",
        "        empty2 = tictactoe2.getempty() #get all valid actions\n",
        "        if(len(empty2) == 0): #if the board's full, don't look-ahead in s', instead check victory state\n",
        "          vict2 = tictactoe.victory_check()\n",
        "          if vict2 == 3: #Draw - game over\n",
        "            possiblerewards.append(-1)\n",
        "          elif vict2 == self.side: #Win - game over\n",
        "            possiblerewards.append(5)\n",
        "          else: #a defeat or an error state - game over\n",
        "            possiblerewards.append(-5)\n",
        "        else:\n",
        "          #print(\"DEBUG\", empty2)\n",
        "          #print(\"DEBUG\", tictactoe2.printstate_xo())\n",
        "          for desired_space2 in empty2:\n",
        "            copy3 = tictactoe2.state.copy()\n",
        "            tictactoe3 = TicTacToe(copy3)\n",
        "            tictactoe3.place1d(self.side, desired_space2)\n",
        "            vict3 = tictactoe3.victory_check() \n",
        "            if vict3 == 0: #the game is still ongoing 2 actions in, so we'll just note that it's still going.\n",
        "              possiblerewards.append(0)\n",
        "            elif vict == 3:\n",
        "              possiblerewards.append(-1)\n",
        "            elif vict == self.side: #a win\n",
        "              possiblerewards.append(5)\n",
        "            else: #a defeat or another error-ish value\n",
        "              possiblerewards.append(-5)\n",
        "      elif vict == 3: #Draw - game over\n",
        "        reward = -1\n",
        "        possiblerewards.append(0)\n",
        "      elif vict == self.side: #Win - game over\n",
        "        reward = 5\n",
        "        possiblerewards.append(0)\n",
        "      else: #a defeat or an error state - game over\n",
        "        reward = -5\n",
        "        possiblerewards.append(0)\n",
        "\n",
        "      \"\"\"\n",
        "      calculate Q_target = r+gamma*max[Q(s',A)]\n",
        "      \"\"\"\n",
        "      Q_target = reward + (self.gamma * max(possiblerewards))\n",
        "      \"\"\"\n",
        "      calculate Q_delta = Q_target - Q(s,a)\n",
        "      add alpha*Q_delta to Q(s,a)\n",
        "      modify Q(s,a)\n",
        "      \"\"\"\n",
        "      qsa_index = searchStates(self.qlookup, action_hypo)\n",
        "\n",
        "      Q_delta = Q_target - self.qtable[qsa_index][best_action]\n",
        "      self.qtable[qsa_index][best_action] = self.qtable[qsa_index][best_action] + (self.alpha * Q_delta) \n",
        "      \n",
        "      #print(\"DEBUG\", self.qtable[qsa_index])\n",
        "      \"\"\"\n",
        "      s=s'\n",
        "      advance the state.\n",
        "      \"\"\"\n",
        "      tictactoe.place1d(self.side, best_action)\n",
        "    self.decay_epsilon()\n",
        "  "
      ],
      "metadata": {
        "id": "hJ7Nh4M7JunE"
      },
      "execution_count": 157,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "QAgent1 = QAgent(1)\n",
        "QAgent2 = QAgent(2)"
      ],
      "metadata": {
        "id": "9OVDoGxBsOXM"
      },
      "execution_count": 123,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "a.reset_board()\n",
        "QAgent1.move(a)\n",
        "a.printstate()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gK5dbiEcsSYk",
        "outputId": "eb4ba4a7-dea4-44d2-c534-7bdd93aeea0c"
      },
      "execution_count": 124,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0 0 0]\n",
            "[0 0 0]\n",
            "[1 0 0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "a.reset_board()\n",
        "QAgent1.move(a)\n",
        "QAgent2.move(a)\n",
        "QAgent1.move(a)\n",
        "QAgent2.move(a)\n",
        "a.printstate()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "92sJckTossH1",
        "outputId": "fffd385f-6a54-4ef8-e3e0-432f5f84f02c"
      },
      "execution_count": 125,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2 0 1]\n",
            "[0 0 2]\n",
            "[1 0 0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#QAgent Games\n",
        "\n",
        "Games are formed as P1 vs P2"
      ],
      "metadata": {
        "id": "O53CrvOETnMI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##QAgent vs Random Player"
      ],
      "metadata": {
        "id": "op8eEcnuew1c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "episodes = 201       # Total episodes - i.e. the number of games we'll play to completion\n",
        "#38s for 100 episodes\n",
        "#~6min for 1k episodes\n",
        "\n",
        "#reset the agents\n",
        "QAgent1 = QAgent(1)\n",
        "QAgent2 = QAgent(2)\n",
        "\n",
        "randage1 = RandomAgent(1)\n",
        "randage2 = RandomAgent(2)\n",
        "\n",
        "#player1wins,player2wins,drawwins, errors\n",
        "results_array = [0,0,0,0]\n",
        "\n",
        "#make a new tictactoe game\n",
        "tictactournament = TicTacToe()\n",
        "for i in range(0, episodes):\n",
        "  gameend = False\n",
        "  result = 4 \n",
        "  tictactournament.reset_board()\n",
        "  if(i%5 == 0):\n",
        "    print(\"Episode \", i)\n",
        "  if(i%100 == 0):\n",
        "    print(\"Game of Episode \", i)\n",
        "  while (gameend == False):\n",
        "    QAgent1.move(tictactournament)\n",
        "    if(i%100 == 0):\n",
        "      print(\"P1\")\n",
        "      tictactournament.printstate_xo()\n",
        "    vict = tictactournament.victory_check() \n",
        "    if vict !=0:\n",
        "      result = vict\n",
        "      gameend = True\n",
        "      break\n",
        "    randage2.move(tictactournament)\n",
        "    if(i%100 == 0):\n",
        "      print(\"P2\")\n",
        "      tictactournament.printstate_xo()\n",
        "    vict = tictactournament.victory_check() \n",
        "    if vict !=0:\n",
        "      result = vict\n",
        "      gameend = True\n",
        "      break\n",
        "  results_array[result-1]+=1\n",
        "\n",
        "print(\"Game Statistics: P1 Win, P2 Win, Draw, Error\")  \n",
        "print(results_array)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oGCQiFNTYG6A",
        "outputId": "d01f6bb6-d377-479d-b176-da2b916ab58e"
      },
      "execution_count": 162,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode  0\n",
            "Game of Episode  0\n",
            "P1\n",
            "E E E  \n",
            "E E E  \n",
            "E X E  \n",
            "P2\n",
            "E O E  \n",
            "E E E  \n",
            "E X E  \n",
            "P1\n",
            "E O E  \n",
            "E X E  \n",
            "E X E  \n",
            "P2\n",
            "O O E  \n",
            "E X E  \n",
            "E X E  \n",
            "P1\n",
            "O O X  \n",
            "E X E  \n",
            "E X E  \n",
            "P2\n",
            "O O X  \n",
            "E X E  \n",
            "O X E  \n",
            "P1\n",
            "O O X  \n",
            "X X E  \n",
            "O X E  \n",
            "P2\n",
            "O O X  \n",
            "X X O  \n",
            "O X E  \n",
            "P1\n",
            "O O X  \n",
            "X X O  \n",
            "O X X  \n",
            "Episode  5\n",
            "Episode  10\n",
            "Episode  15\n",
            "Episode  20\n",
            "Episode  25\n",
            "Episode  30\n",
            "Episode  35\n",
            "Episode  40\n",
            "Episode  45\n",
            "Episode  50\n",
            "Episode  55\n",
            "Episode  60\n",
            "Episode  65\n",
            "Episode  70\n",
            "Episode  75\n",
            "Episode  80\n",
            "Episode  85\n",
            "Episode  90\n",
            "Episode  95\n",
            "Episode  100\n",
            "Game of Episode  100\n",
            "P1\n",
            "X E E  \n",
            "E E E  \n",
            "E E E  \n",
            "P2\n",
            "X E E  \n",
            "E O E  \n",
            "E E E  \n",
            "P1\n",
            "X X E  \n",
            "E O E  \n",
            "E E E  \n",
            "P2\n",
            "X X E  \n",
            "O O E  \n",
            "E E E  \n",
            "P1\n",
            "X X X  \n",
            "O O E  \n",
            "E E E  \n",
            "Episode  105\n",
            "Episode  110\n",
            "Episode  115\n",
            "Episode  120\n",
            "Episode  125\n",
            "Episode  130\n",
            "Episode  135\n",
            "Episode  140\n",
            "Episode  145\n",
            "Episode  150\n",
            "Episode  155\n",
            "Episode  160\n",
            "Episode  165\n",
            "Episode  170\n",
            "Episode  175\n",
            "Episode  180\n",
            "Episode  185\n",
            "Episode  190\n",
            "Episode  195\n",
            "Episode  200\n",
            "Game of Episode  200\n",
            "P1\n",
            "X E E  \n",
            "E E E  \n",
            "E E E  \n",
            "P2\n",
            "X E E  \n",
            "E O E  \n",
            "E E E  \n",
            "P1\n",
            "X E E  \n",
            "E O E  \n",
            "E E X  \n",
            "P2\n",
            "X E O  \n",
            "E O E  \n",
            "E E X  \n",
            "P1\n",
            "X X O  \n",
            "E O E  \n",
            "E E X  \n",
            "P2\n",
            "X X O  \n",
            "E O O  \n",
            "E E X  \n",
            "P1\n",
            "X X O  \n",
            "X O O  \n",
            "E E X  \n",
            "P2\n",
            "X X O  \n",
            "X O O  \n",
            "O E X  \n",
            "Game Statistics: P1 Win, P2 Win, Draw, Error\n",
            "[110, 72, 19, 0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#check to make sure epsilon actually decreased\n",
        "print(QAgent1.epsilon)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H_IQkNORMoMQ",
        "outputId": "8640c5a8-e088-4ab5-b1f2-d781be36736f"
      },
      "execution_count": 163,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.049999999999999684\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Random Player vs QAgent"
      ],
      "metadata": {
        "id": "1o_ZDmUje2rg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#we'll swap the starting player to see if has any effect\n",
        "#player1wins,player2wins,drawwins, errors\n",
        "results_array = [0,0,0,0]\n",
        "\n",
        "#make a new tictactoe game\n",
        "tictactournament = TicTacToe()\n",
        "for i in range(0, episodes):\n",
        "  gameend = False\n",
        "  result = 4 \n",
        "  tictactournament.reset_board()\n",
        "  if(i%5 == 0):\n",
        "    print(\"Episode \", i)\n",
        "  if(i%100 == 0):\n",
        "    print(\"Game of Episode \", i)\n",
        "  while (gameend == False):\n",
        "    randage1.move(tictactournament)\n",
        "    if(i%100 == 0):\n",
        "      print(\"P1\")\n",
        "      tictactournament.printstate_xo()\n",
        "    vict = tictactournament.victory_check() \n",
        "    if vict !=0:\n",
        "      result = vict\n",
        "      gameend = True\n",
        "      break\n",
        "    QAgent2.move(tictactournament)\n",
        "    if(i%100 == 0):\n",
        "      print(\"P2\")\n",
        "      tictactournament.printstate_xo()\n",
        "    vict = tictactournament.victory_check() \n",
        "    if vict !=0:\n",
        "      result = vict\n",
        "      gameend = True\n",
        "      break\n",
        "  results_array[result-1]+=1\n",
        "\n",
        "print(\"Game Statistics: P1 Win, P2 Win, Draw, Error\")  \n",
        "print(results_array)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TNhfvvjQTLJ-",
        "outputId": "a543a145-fed4-4e32-8761-9219c042fe81"
      },
      "execution_count": 164,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode  0\n",
            "Game of Episode  0\n",
            "P1\n",
            "E E E  \n",
            "E E E  \n",
            "E E X  \n",
            "P2\n",
            "E E E  \n",
            "O E E  \n",
            "E E X  \n",
            "P1\n",
            "E X E  \n",
            "O E E  \n",
            "E E X  \n",
            "P2\n",
            "O X E  \n",
            "O E E  \n",
            "E E X  \n",
            "P1\n",
            "O X E  \n",
            "O E E  \n",
            "X E X  \n",
            "P2\n",
            "O X E  \n",
            "O E E  \n",
            "X O X  \n",
            "P1\n",
            "O X E  \n",
            "O E X  \n",
            "X O X  \n",
            "P2\n",
            "O X E  \n",
            "O O X  \n",
            "X O X  \n",
            "P1\n",
            "O X X  \n",
            "O O X  \n",
            "X O X  \n",
            "Episode  5\n",
            "Episode  10\n",
            "Episode  15\n",
            "Episode  20\n",
            "Episode  25\n",
            "Episode  30\n",
            "Episode  35\n",
            "Episode  40\n",
            "Episode  45\n",
            "Episode  50\n",
            "Episode  55\n",
            "Episode  60\n",
            "Episode  65\n",
            "Episode  70\n",
            "Episode  75\n",
            "Episode  80\n",
            "Episode  85\n",
            "Episode  90\n",
            "Episode  95\n",
            "Episode  100\n",
            "Game of Episode  100\n",
            "P1\n",
            "E E E  \n",
            "E E E  \n",
            "E E X  \n",
            "P2\n",
            "O E E  \n",
            "E E E  \n",
            "E E X  \n",
            "P1\n",
            "O X E  \n",
            "E E E  \n",
            "E E X  \n",
            "P2\n",
            "O X O  \n",
            "E E E  \n",
            "E E X  \n",
            "P1\n",
            "O X O  \n",
            "E E E  \n",
            "X E X  \n",
            "P2\n",
            "O X O  \n",
            "O E E  \n",
            "X E X  \n",
            "P1\n",
            "O X O  \n",
            "O X E  \n",
            "X E X  \n",
            "P2\n",
            "O X O  \n",
            "O X E  \n",
            "X O X  \n",
            "P1\n",
            "O X O  \n",
            "O X X  \n",
            "X O X  \n",
            "Episode  105\n",
            "Episode  110\n",
            "Episode  115\n",
            "Episode  120\n",
            "Episode  125\n",
            "Episode  130\n",
            "Episode  135\n",
            "Episode  140\n",
            "Episode  145\n",
            "Episode  150\n",
            "Episode  155\n",
            "Episode  160\n",
            "Episode  165\n",
            "Episode  170\n",
            "Episode  175\n",
            "Episode  180\n",
            "Episode  185\n",
            "Episode  190\n",
            "Episode  195\n",
            "Episode  200\n",
            "Game of Episode  200\n",
            "P1\n",
            "E E E  \n",
            "E E X  \n",
            "E E E  \n",
            "P2\n",
            "O E E  \n",
            "E E X  \n",
            "E E E  \n",
            "P1\n",
            "O E X  \n",
            "E E X  \n",
            "E E E  \n",
            "P2\n",
            "O O X  \n",
            "E E X  \n",
            "E E E  \n",
            "P1\n",
            "O O X  \n",
            "E X X  \n",
            "E E E  \n",
            "P2\n",
            "O O X  \n",
            "O X X  \n",
            "E E E  \n",
            "P1\n",
            "O O X  \n",
            "O X X  \n",
            "E X E  \n",
            "P2\n",
            "O O X  \n",
            "O X X  \n",
            "E X O  \n",
            "P1\n",
            "O O X  \n",
            "O X X  \n",
            "X X O  \n",
            "Game Statistics: P1 Win, P2 Win, Draw, Error\n",
            "[116, 66, 19, 0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##QAgent vs QAgent"
      ],
      "metadata": {
        "id": "EvTxjYWUe7fl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Lastly we'll have 2 QAgents play vs each other\n",
        "\n",
        "QAgent3 = QAgent(1)\n",
        "QAgent4 = QAgent(2)\n",
        "#we'll swap the starting player to see if has any effect\n",
        "#player1wins,player2wins,drawwins, errors\n",
        "results_array = [0,0,0,0]\n",
        "\n",
        "#make a new tictactoe game\n",
        "tictactournament = TicTacToe()\n",
        "for i in range(0, episodes):\n",
        "  gameend = False\n",
        "  result = 4 \n",
        "  tictactournament.reset_board()\n",
        "  if(i%5 == 0):\n",
        "    print(\"Episode \", i)\n",
        "  if(i%100 == 0):\n",
        "    print(\"Game of Episode \", i)\n",
        "  while (gameend == False):\n",
        "    QAgent3.move(tictactournament)\n",
        "    if(i%100 == 0):\n",
        "      print(\"P1\")\n",
        "      tictactournament.printstate_xo()\n",
        "    vict = tictactournament.victory_check() \n",
        "    if vict !=0:\n",
        "      result = vict\n",
        "      gameend = True\n",
        "      break\n",
        "    QAgent4.move(tictactournament)\n",
        "    if(i%100 == 0):\n",
        "      print(\"P2\")\n",
        "      tictactournament.printstate_xo()\n",
        "    vict = tictactournament.victory_check() \n",
        "    if vict !=0:\n",
        "      result = vict\n",
        "      gameend = True\n",
        "      break\n",
        "  results_array[result-1]+=1\n",
        "\n",
        "print(\"Game Statistics: P1 Win, P2 Win, Draw, Error\")  \n",
        "print(results_array)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bw3tMN_nTbrG",
        "outputId": "1066303f-759a-413e-ad48-496244854567"
      },
      "execution_count": 165,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode  0\n",
            "Game of Episode  0\n",
            "P1\n",
            "E E E  \n",
            "E X E  \n",
            "E E E  \n",
            "P2\n",
            "E E O  \n",
            "E X E  \n",
            "E E E  \n",
            "P1\n",
            "E E O  \n",
            "E X E  \n",
            "E X E  \n",
            "P2\n",
            "E E O  \n",
            "O X E  \n",
            "E X E  \n",
            "P1\n",
            "E E O  \n",
            "O X E  \n",
            "E X X  \n",
            "P2\n",
            "E O O  \n",
            "O X E  \n",
            "E X X  \n",
            "P1\n",
            "X O O  \n",
            "O X E  \n",
            "E X X  \n",
            "Episode  5\n",
            "Episode  10\n",
            "Episode  15\n",
            "Episode  20\n",
            "Episode  25\n",
            "Episode  30\n",
            "Episode  35\n",
            "Episode  40\n",
            "Episode  45\n",
            "Episode  50\n",
            "Episode  55\n",
            "Episode  60\n",
            "Episode  65\n",
            "Episode  70\n",
            "Episode  75\n",
            "Episode  80\n",
            "Episode  85\n",
            "Episode  90\n",
            "Episode  95\n",
            "Episode  100\n",
            "Game of Episode  100\n",
            "P1\n",
            "X E E  \n",
            "E E E  \n",
            "E E E  \n",
            "P2\n",
            "X O E  \n",
            "E E E  \n",
            "E E E  \n",
            "P1\n",
            "X O X  \n",
            "E E E  \n",
            "E E E  \n",
            "P2\n",
            "X O X  \n",
            "O E E  \n",
            "E E E  \n",
            "P1\n",
            "X O X  \n",
            "O X E  \n",
            "E E E  \n",
            "P2\n",
            "X O X  \n",
            "O X O  \n",
            "E E E  \n",
            "P1\n",
            "X O X  \n",
            "O X O  \n",
            "X E E  \n",
            "Episode  105\n",
            "Episode  110\n",
            "Episode  115\n",
            "Episode  120\n",
            "Episode  125\n",
            "Episode  130\n",
            "Episode  135\n",
            "Episode  140\n",
            "Episode  145\n",
            "Episode  150\n",
            "Episode  155\n",
            "Episode  160\n",
            "Episode  165\n",
            "Episode  170\n",
            "Episode  175\n",
            "Episode  180\n",
            "Episode  185\n",
            "Episode  190\n",
            "Episode  195\n",
            "Episode  200\n",
            "Game of Episode  200\n",
            "P1\n",
            "X E E  \n",
            "E E E  \n",
            "E E E  \n",
            "P2\n",
            "X O E  \n",
            "E E E  \n",
            "E E E  \n",
            "P1\n",
            "X O X  \n",
            "E E E  \n",
            "E E E  \n",
            "P2\n",
            "X O X  \n",
            "O E E  \n",
            "E E E  \n",
            "P1\n",
            "X O X  \n",
            "O X E  \n",
            "E E E  \n",
            "P2\n",
            "X O X  \n",
            "O X O  \n",
            "E E E  \n",
            "P1\n",
            "X O X  \n",
            "O X O  \n",
            "X E E  \n",
            "Game Statistics: P1 Win, P2 Win, Draw, Error\n",
            "[175, 17, 9, 0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Conclusion\n",
        "\n",
        "When playing vs random agents, our Q-learning agents performed slightly worse in the player 1 seat, but better in the player 2 seat, and were able to draw less often on the whole. We speculate that the struggle to win more in the player 1 seat is likely due to the relatively short number of episodes we had to train the Q-learning agent in due to time/usage constraints and that further adjustments could be made to the hyperparameters to help it more quickly learn the game. \n",
        "\n",
        "https://en.wikipedia.org/wiki/First-player_and_second-player_win\n",
        "\n",
        "If both players were playing perfectly, Tic-Tac-Toe should always end in a draw. However, our agents are not perfect - the random agent picks randomly, and the Q-learning agent is trying to learn the game from scratch. \n",
        "\n",
        "https://momath.org/wp-content/uploads/2021/08/Alyssa-Choi-Tic-Tac-Toe.pdf\n",
        "\n",
        "https://www.quora.com/How-can-we-win-in-Tic-Tac-Toe-when-we-go-second\n",
        "\n",
        "In the event that players are playing imperfectly, an advantage can be gained by the first player by taking as many the four corners of the board as possible (this is demonstrated by Choi et al as the \"fork strategy\"). We observe that the Q Agent when played in the \"1st player seat\" learned this advantage over the course of its training and always attempted to claim a corner in its first move once trained, and attempted to make moves to claim the other corners especially when playing vs the other Q Agent.\n",
        "\n",
        "In the event that the first player is trying the forking strategy, it is generally in player 2's best interest to claim the center of the board. It appears that our Q Agent in the player 2 seat did generally not learn this strategy - against the random agent, which was less likely to employ that strategy, it instead attempted to claim the corners as much as possible instead (possibly to try and replicate the forking strategy), and when facing a Q Agent in the player 1 seat that had learned the forking strategy, it went for more short-sighted attempts to block any three-way connections. This is likely also another situation where more training episodes and more time to experiment with different hyperparameters could lead to interesting results, particularly in seeing if a Player 2 Q Agent in a Q agent-only game could learn the strategy of takign the center space to block Player 1 attempting to fork."
      ],
      "metadata": {
        "id": "BpIPkZewZ3lu"
      }
    }
  ]
}